{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "classification_BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "NRg8_j-37E4u",
        "rgMinvO2Bl07",
        "PEtQtrQvDoEs",
        "-_DhUcajFTgM",
        "exkNVP06BB8h",
        "cArnq2Kgx5ZP",
        "oGUtyPrnvTy6",
        "gB_S_hdYvTy6",
        "0KoO20NRN8mj",
        "ZDPT4S6qN8ml",
        "b5Ig0FuWN8mm",
        "1a06W1hEN8mq",
        "QDKx4xc9eWcT",
        "TI7gV6uDeWcX",
        "ZfBa3a5AiDe2",
        "q7h5Ng4Dq9R-",
        "6kOo-CPxajDX",
        "Ecv7S7U7ajDb",
        "CxpNii0TcJ5T",
        "DtU_7qvkgBgM",
        "FiSXIASXgBgR"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BERT-base-uncased\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DtU_7qvkgBgM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparation"
      ],
      "metadata": {
        "id": "8j36jjUcZUD8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8db5c805-b7f5-4830-e334-fe6c22afffb1",
        "id": "ry_Bxbj0gBgM"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytorch_pretrained_bert in /usr/local/lib/python3.7/dist-packages (0.6.2)\n",
            "Requirement already satisfied: pytorch-nlp in /usr/local/lib/python3.7/dist-packages (0.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.21.6)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.23.10)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (4.64.0)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.11.0+cu113)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (4.2.0)\n",
            "Requirement already satisfied: botocore<1.27.0,>=1.26.10 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_pretrained_bert) (1.26.10)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_pretrained_bert) (1.0.0)\n",
            "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_pretrained_bert) (0.5.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.27.0,>=1.26.10->boto3->pytorch_pretrained_bert) (1.25.11)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.27.0,>=1.26.10->boto3->pytorch_pretrained_bert) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.27.0,>=1.26.10->boto3->pytorch_pretrained_bert) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch_pretrained_bert pytorch-nlp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import random as rn\n",
        "import torch\n",
        "from pytorch_pretrained_bert import BertModel\n",
        "from torch import nn\n",
        "from pytorch_pretrained_bert import BertTokenizer\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.optim import Adam\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from IPython.display import clear_output"
      ],
      "metadata": {
        "id": "gPagmLwKgBgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_val = 42\n",
        "rn.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed(seed_val)"
      ],
      "metadata": {
        "id": "EQdkk27DgBgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "metadata": {
        "id": "5Vr86LRkgBgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1753c323-8c22-4832-819b-338419c8b1dd",
        "id": "E524L-_RgBgN"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pytorch_pretrained_bert.tokenization.BertTokenizer at 0x7f59df2ef950>"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train = pd.read_csv(\"/content/drive/MyDrive/Keypoints/inter-text_from_T5-small_temp_3_full_train_indomain.csv\")\n",
        "dev = pd.read_csv(\"/content/drive/MyDrive/Keypoints/inter-text_from_T5-small_temp_3_full_dev_indomain.csv\")\n",
        "test = pd.read_csv(\"/content/drive/MyDrive/Keypoints/inter-text_from_T5-small_temp_3_full_test_indomain.csv\")\n",
        "for split in [train,test]:\n",
        "  for i in split.index:\n",
        "    arg = split['argument'][i]\n",
        "    key = split['key_point'][i]\n",
        "    in_text = split['intermediary_text'][i]\n",
        "    if arg[-1] != '.':\n",
        "      pair =  arg + '. ' + in_text + '. '+ key + '.'\n",
        "      split.at[i, 'pair'] = pair\n",
        "    else:\n",
        "      pair =  arg + ' ' + in_text + '. '+ key + '.'\n",
        "      split.at[i, 'pair'] = pair"
      ],
      "metadata": {
        "id": "yM1cVy5hgBgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['pair'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "7a7e5a91-7a1b-4e2e-84bb-e9cad162ddf4",
        "id": "oWSpF5_8gBgN"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"prostitution isnt going anywhere. at least if it's legalized it would take the profession out of the hands of criminals and be safer for the women. Legalizing sex work will allow to regulate those in the profession. Legalizing sex work boosts the economy.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pairs_train = train.pair.values\n",
        "labels_train = train.label.values\n",
        "\n",
        "pairs_dev = dev.pair.values\n",
        "labels_dev = dev.label.values\n",
        "\n",
        "pairs_test = test.pair.values\n",
        "labels_test = test.label.values"
      ],
      "metadata": {
        "id": "GdmhHaChgBgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:85] + ['[SEP]'], pairs_train))\n",
        "dev_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:85] + ['[SEP]'], pairs_dev))\n",
        "test_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:85] + ['[SEP]'], pairs_test))\n",
        "\n",
        "train_tokens_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, train_tokens)), maxlen=87, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
        "dev_tokens_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, dev_tokens)), maxlen=87, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
        "test_tokens_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, test_tokens)), maxlen=87, truncating=\"post\", padding=\"post\", dtype=\"int\")"
      ],
      "metadata": {
        "id": "6-vRFg8XgBgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_masks = [[float(i > 0) for i in ii] for ii in train_tokens_ids]\n",
        "dev_masks = [[float(i > 0) for i in ii] for ii in dev_tokens_ids]\n",
        "test_masks = [[float(i > 0) for i in ii] for ii in test_tokens_ids]"
      ],
      "metadata": {
        "id": "WZ2x8ykjgBgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERT model"
      ],
      "metadata": {
        "id": "heWH0KgegBgO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BertBinaryClassifier(nn.Module):\n",
        "    def __init__(self, dropout=0.1):\n",
        "        super(BertBinaryClassifier, self).__init__()\n",
        "\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear = nn.Linear(768, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "    def forward(self, tokens, masks=None):\n",
        "        _, pooled_output = self.bert(tokens, attention_mask=masks, output_all_encoded_layers=False)\n",
        "        dropout_output = self.dropout(pooled_output)\n",
        "        linear_output = self.linear(dropout_output)\n",
        "        proba = self.sigmoid(linear_output)\n",
        "        return proba"
      ],
      "metadata": {
        "id": "XK51Y4HdgBgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8833c81d-f4f6-4bb1-a5e5-03fbe1586cb4",
        "id": "W1Hbwr9ygBgP"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_clf = BertBinaryClassifier()\n",
        "bert_clf = bert_clf.cuda()"
      ],
      "metadata": {
        "id": "utzrFyssgBgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finetune"
      ],
      "metadata": {
        "id": "aOHHpRvqgBgP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "EPOCHS = 3"
      ],
      "metadata": {
        "id": "qAjAYV54gBgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_tokens_tensor = torch.tensor(train_tokens_ids)\n",
        "train_labels_tensor = torch.tensor(labels_train.reshape(-1, 1)).float()\n",
        "\n",
        "dev_tokens_tensor = torch.tensor(dev_tokens_ids)\n",
        "dev_labels_tensor = torch.tensor(labels_dev.reshape(-1, 1)).float()\n",
        "\n",
        "test_tokens_tensor = torch.tensor(test_tokens_ids)\n",
        "test_labels_tensor = torch.tensor(labels_test.reshape(-1, 1)).float()\n",
        "\n",
        "train_masks_tensor = torch.tensor(train_masks)\n",
        "dev_masks_tensor = torch.tensor(dev_masks)\n",
        "test_masks_tensor = torch.tensor(test_masks)"
      ],
      "metadata": {
        "id": "QfsYR_W6gBgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TensorDataset(train_tokens_tensor, train_masks_tensor, train_labels_tensor)\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "dev_dataset = TensorDataset(dev_tokens_tensor, dev_masks_tensor, dev_labels_tensor)\n",
        "dev_sampler = SequentialSampler(dev_dataset)\n",
        "dev_dataloader = DataLoader(dev_dataset, sampler=dev_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "test_dataset = TensorDataset(test_tokens_tensor, test_masks_tensor, test_labels_tensor)\n",
        "test_sampler = SequentialSampler(test_dataset)\n",
        "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "kbE-ja1bgBgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_optimizer = list(bert_clf.sigmoid.named_parameters()) \n",
        "optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
        "\n",
        "optimizer = Adam(bert_clf.parameters(), lr=2e-5)"
      ],
      "metadata": {
        "id": "IDx2pt0lgBgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "metadata": {
        "id": "14k7zDaSgBgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "JFGySmz8gBgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t0 = time.time()\n",
        "for epoch_num in range(EPOCHS):\n",
        "\n",
        "  bert_clf.train()\n",
        "  train_loss = 0\n",
        "  for step_num, batch_data in enumerate(train_dataloader):\n",
        "    token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n",
        "    print(str(torch.cuda.memory_allocated(device)/1000000 ) + 'M')\n",
        "    \n",
        "    logits = bert_clf(token_ids, masks)\n",
        "    loss_func = nn.BCELoss()\n",
        "\n",
        "    batch_loss = loss_func(logits, labels)\n",
        "    train_loss += batch_loss.item()\n",
        "    \n",
        "    bert_clf.zero_grad()\n",
        "    batch_loss.backward()\n",
        "        \n",
        "\n",
        "    clip_grad_norm_(parameters=bert_clf.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "        \n",
        "    clear_output(wait=True)\n",
        "    print('Epoch: ', epoch_num + 1)\n",
        "    print(\"\\r\" + \"{0}/{1} train loss: {2} \".format(step_num, len(train) / BATCH_SIZE, train_loss / (step_num + 1)))\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-t0)))"
      ],
      "metadata": {
        "id": "mFzOdnqlgBgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "YrIiBh-qgBgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_clf.eval()\n",
        "\n",
        "all_logits = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for step_num, batch_data in enumerate(test_dataloader):\n",
        "\n",
        "        token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n",
        "\n",
        "        logits = bert_clf(token_ids, masks)\n",
        "        loss_func = nn.BCELoss()\n",
        "        loss = loss_func(logits, labels)\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        numpy_logits = logits.cpu().detach().numpy()\n",
        "        all_logits += list(numpy_logits[:, 0]) \n",
        " \n",
        "\n",
        "        "
      ],
      "metadata": {
        "id": "WQp3UlaUgBgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# F1 of test set\n",
        "test[\"prob\"] = all_logits\n",
        "threshold = 0.5\n",
        "for i in test.index:\n",
        "  if test[\"prob\"][i] > threshold:\n",
        "    test.at[i,\"prediction\"] = 1\n",
        "  else:\n",
        "    test.at[i,\"prediction\"] = 0\n",
        "\n",
        "from sklearn.metrics import classification_report,accuracy_score,f1_score\n",
        "true = test[\"label\"]\n",
        "prediction = test[\"prediction\"]\n",
        "print(classification_report(true, prediction, digits=3))"
      ],
      "metadata": {
        "id": "0cFyn8h2gBgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save and load"
      ],
      "metadata": {
        "id": "FiSXIASXgBgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save({\n",
        "            'epoch': epoch_num,\n",
        "            'model_state_dict': bert_clf.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': batch_loss,\n",
        "            }, \"/content/drive/MyDrive/Keypoints/bert.pth\")"
      ],
      "metadata": {
        "id": "IxErHovqgBgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_clf = BertBinaryClassifier() \n",
        "bert_clf.load_state_dict(torch.load(\"/content/drive/MyDrive/Keypoints/bert.pth\")['model_state_dict'],strict=False) \n",
        "bert_clf.to(device) \n",
        "bert_clf.eval()"
      ],
      "metadata": {
        "id": "gNMeXv4AgBgR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}