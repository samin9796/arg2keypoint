{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BERT_baseline.ipynb","provenance":[],"mount_file_id":"1CGuO82I9AxxVNWYqBoXFro7CKNTLE-qh","authorship_tag":"ABX9TyPnGCqHrVqAGOCueUevtZaY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# BERT-base/large-uncased"],"metadata":{"id":"8xNAAFu-uICJ"}},{"cell_type":"markdown","source":["## Preparation"],"metadata":{"id":"eLiEg5BRcv9M"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"tb6EZ9SauICK"},"outputs":[],"source":["!pip install pytorch_pretrained_bert pytorch-nlp"]},{"cell_type":"code","source":["import sys\n","import numpy as np\n","import random as rn\n","import torch\n","from pytorch_pretrained_bert import BertModel\n","from torch import nn\n","from pytorch_pretrained_bert import BertTokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from torch.optim import Adam\n","from torch.nn.utils import clip_grad_norm_\n","from IPython.display import clear_output"],"metadata":{"id":"2fLsF3rquICL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["seed_val = 42\n","rn.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed(seed_val)"],"metadata":{"id":"yubqJV0SuICM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)#bert-large-uncased"],"metadata":{"id":"mq_HwWqVuICN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","train = pd.read_csv(\"/content/drive/MyDrive/Keypoints/train.csv\")\n","dev = pd.read_csv(\"/content/drive/MyDrive/Keypoints/dev.csv\")\n","test = pd.read_csv(\"/content/drive/MyDrive/Keypoints/test.csv\")\n","for split in [train,test]:\n","  for i in split.index:\n","    arg = split['argument'][i]\n","    key = split['key_point'][i]\n","    if arg[-1] != '.':\n","      pair = arg + '. ' + key + '.'\n","      split.at[i, 'pair'] = pair\n","    else:\n","      pair = arg + ' ' + key + '.'\n","      split.at[i, 'pair'] = pair"],"metadata":{"id":"E5p36LdFuICO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pairs_train = train.pair.values\n","labels_train = train.label.values\n","\n","pairs_dev = dev.pair.values\n","labels_dev = dev.label.values\n","\n","pairs_test = test.pair.values\n","labels_test = test.label.values"],"metadata":{"id":"OsyHx5rmuICP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pairs_train[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"id":"TYR5j9mX-xgK","executionInfo":{"status":"ok","timestamp":1652891007566,"user_tz":-120,"elapsed":223,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"1f1169f7-21e0-4003-a1db-3216ddcc9990"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'a person created through cloning could potentially have developmental problems caused by imperfections in the cloning process. Cloning is not understood enough yet.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["train_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:71] + ['[SEP]'], pairs_train))\n","dev_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:70] + ['[SEP]'], pairs_dev))\n","test_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:71] + ['[SEP]'], pairs_test))\n","\n","train_tokens_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, train_tokens)), maxlen=73, truncating=\"post\", padding=\"post\", dtype=\"int\")\n","dev_tokens_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, dev_tokens)), maxlen=73, truncating=\"post\", padding=\"post\", dtype=\"int\")\n","test_tokens_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, test_tokens)), maxlen=73, truncating=\"post\", padding=\"post\", dtype=\"int\")\n","\n"],"metadata":{"id":"RhgmpCdluICP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_masks = [[float(i > 0) for i in ii] for ii in train_tokens_ids]\n","dev_masks = [[float(i > 0) for i in ii] for ii in dev_tokens_ids]\n","test_masks = [[float(i > 0) for i in ii] for ii in test_tokens_ids]"],"metadata":{"id":"f12cYK0-uICQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Maximum sequence size for BERT is 512, but the maximum text length in the dataset is 73, so we’ll truncate any review that is longer than this.\n","\n","We need to pad our input so it will have the same size of 73. It means that for any review that is shorter than 73 tokens, we’ll add zeros to reach 73 tokens"],"metadata":{"id":"NblxeIajuICQ"}},{"cell_type":"markdown","source":["### BERT model"],"metadata":{"id":"h5JHI9cJuICS"}},{"cell_type":"code","source":["class BertBinaryClassifier(nn.Module):\n","    def __init__(self, dropout=0.1):\n","        super(BertBinaryClassifier, self).__init__()\n","\n","        self.bert = BertModel.from_pretrained('bert-base-uncased') #bert-large-uncased\n","\n","        self.dropout = nn.Dropout(dropout)\n","        self.linear = nn.Linear(1024, 1)\n","        self.sigmoid = nn.Sigmoid()\n","    \n","    def forward(self, tokens, masks=None):\n","        _, pooled_output = self.bert(tokens, attention_mask=masks, output_all_encoded_layers=False)\n","        dropout_output = self.dropout(pooled_output)\n","        linear_output = self.linear(dropout_output)\n","        proba = self.sigmoid(linear_output)\n","        return proba"],"metadata":{"id":"Rvr8dsfduICS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Every model in PyTorch is a `nn.Module` object. It means that every model we built must provide 2 methods. The `__init__` method declares all the different parts the model will use. In our case, we create the BERT model that we’ll fine-tune, the Linear layer, and the Sigmoid activation. The `forward` method is the actual code that runs during the forward pass (like the predict method in sklearn or keras). Here we take the tokens input and pass it to the BERT model. The output of BERT is 2 variables, as we have seen before, we use only the second one (the `_` name is used to emphasize that this variable is not used). We take the pooled output and pass it to the linear layer. Finally, we use the Sigmoid activation to provide the actual probability."],"metadata":{"id":"vw8GyhZKuICT"}},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652891031053,"user_tz":-120,"elapsed":330,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"e02af787-285b-4dfc-ff23-fb6ebe3169b3","id":"53Kh5SiJuICT"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["str(torch.cuda.memory_allocated(device)/1000000 ) + 'M'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1652658180488,"user_tz":-120,"elapsed":219,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"cd1c61d6-0b4f-4882-8a3c-ab9c5097eeb7","id":"av5b6SXEuICU"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'439.065088M'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":64}]},{"cell_type":"code","source":["bert_clf = BertBinaryClassifier()\n","bert_clf = bert_clf.cuda()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1Pp4pYmruICV","executionInfo":{"status":"ok","timestamp":1652891098436,"user_tz":-120,"elapsed":67384,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"9e9c400b-e343-4626-af2d-8d60152a7a75"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1248501532/1248501532 [00:27<00:00, 44663645.14B/s]\n"]}]},{"cell_type":"code","source":["str(torch.cuda.memory_allocated(device)/1000000 ) + 'M'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1652891098439,"user_tz":-120,"elapsed":11,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"e733adc0-d998-449e-c675-62c2bc90bdcc","id":"1OFFp0eZuICW"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'1341.383168M'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["x = torch.tensor(train_tokens_ids[:3]).to(device)\n","y, pooled = bert_clf.bert(x, output_all_encoded_layers=False)\n","x.shape, y.shape, pooled.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652891098770,"user_tz":-120,"elapsed":339,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"ba373cd9-d25e-48c7-a541-72c7ff18ba9d","id":"KD4ge2FruICX"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([3, 73]), torch.Size([3, 73, 1024]), torch.Size([3, 1024]))"]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","source":["x is of size (3, 512) , we took only 3 reviews, 512 tokens each. y is of size (3, 512, 1024), this is the BERTs final layer output for each token. We could use `output_all_encoded_layer=True` to get the output of all the 12 layers. Each token in each review is represented using a vector of size 1024. pooled is of size (3, 1024) this is the output of our `[CLS]` token, the first token in our sequence."],"metadata":{"id":"TLLXr6BZuICR"}},{"cell_type":"code","source":["y = bert_clf(x)\n","y.cpu().detach().numpy()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652891098771,"user_tz":-120,"elapsed":10,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"ab2933d6-7292-4333-b646-5a06b502da0d","id":"rF0ol-yyuICY"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.6919196 ],\n","       [0.44563645],\n","       [0.560727  ]], dtype=float32)"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["str(torch.cuda.memory_allocated(device)/1000000 ) + 'M'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1652891098774,"user_tz":-120,"elapsed":10,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"0d8d6ac9-ade7-4399-af9c-db20f9944a2f","id":"E-8KUYRjuICZ"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'2686.828544M'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["y, x, pooled = None, None, None\n","torch.cuda.empty_cache()\n","str(torch.cuda.memory_allocated(device)/1000000 ) + 'M'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1652891098995,"user_tz":-120,"elapsed":9,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"7f0407fb-354b-4130-da9e-422c803ba6cf","id":"pHSxcyW-uICa"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'1341.385216M'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","source":["## Finetune"],"metadata":{"id":"4EFh4UxcuICb"}},{"cell_type":"code","source":["BATCH_SIZE = 32\n","EPOCHS = 3"],"metadata":{"id":"JtN0Is9NuICb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_tokens_tensor = torch.tensor(train_tokens_ids)\n","train_labels_tensor = torch.tensor(labels_train.reshape(-1, 1)).float()\n","\n","dev_tokens_tensor = torch.tensor(dev_tokens_ids)\n","dev_labels_tensor = torch.tensor(labels_dev.reshape(-1, 1)).float()\n","\n","test_tokens_tensor = torch.tensor(test_tokens_ids)\n","test_labels_tensor = torch.tensor(labels_test.reshape(-1, 1)).float()\n","\n","train_masks_tensor = torch.tensor(train_masks)\n","dev_masks_tensor = torch.tensor(dev_masks)\n","test_masks_tensor = torch.tensor(test_masks)\n","\n","str(torch.cuda.memory_allocated(device)/1000000 ) + 'M'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1652891107263,"user_tz":-120,"elapsed":222,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"839366d3-3020-4a07-8bc8-a9a9d6595275","id":"27FnKEvjuICc"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'1341.383168M'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["train_dataset = TensorDataset(train_tokens_tensor, train_masks_tensor, train_labels_tensor)\n","train_sampler = RandomSampler(train_dataset)\n","train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)\n","\n","dev_dataset = TensorDataset(dev_tokens_tensor, dev_masks_tensor, dev_labels_tensor)\n","dev_sampler = SequentialSampler(dev_dataset)\n","dev_dataloader = DataLoader(dev_dataset, sampler=dev_sampler, batch_size=BATCH_SIZE)\n","\n","test_dataset = TensorDataset(test_tokens_tensor, test_masks_tensor, test_labels_tensor)\n","test_sampler = SequentialSampler(test_dataset)\n","test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)"],"metadata":{"id":"hEBU1fu3uICd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["param_optimizer = list(bert_clf.sigmoid.named_parameters()) \n","optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n","optimizer = Adam(bert_clf.parameters(), lr=2e-5)"],"metadata":{"id":"vq-r2rqsuICe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import time\n","import datetime\n","\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"],"metadata":{"id":"W4Fd27Jv9oXn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.cuda.empty_cache()"],"metadata":{"id":"B_REvVWyuICg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["t0 = time.time()\n","for epoch_num in range(EPOCHS):\n","\n","  bert_clf.train()\n","  train_loss = 0\n","  for step_num, batch_data in enumerate(train_dataloader):\n","    token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n","    print(str(torch.cuda.memory_allocated(device)/1000000 ) + 'M')\n","    \n","    logits = bert_clf(token_ids, masks)\n","    loss_func = nn.BCELoss()\n","\n","    batch_loss = loss_func(logits, labels)\n","    train_loss += batch_loss.item()\n","    \n","    bert_clf.zero_grad()\n","    batch_loss.backward()\n","        \n","\n","    clip_grad_norm_(parameters=bert_clf.parameters(), max_norm=1.0)\n","    optimizer.step()\n","        \n","    clear_output(wait=True)\n","    print('Epoch: ', epoch_num + 1)\n","    print(\"\\r\" + \"{0}/{1} train loss: {2} \".format(step_num, len(train) / BATCH_SIZE, train_loss / (step_num + 1)))\n","\n","print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-t0)))"],"metadata":{"id":"T-MTIOGDuICg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bert_clf.eval()\n","bert_predicted = []\n","all_logits = []\n","true_labels = []\n","with torch.no_grad():\n","    for step_num, batch_data in enumerate(test_dataloader):\n","\n","        token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n","\n","        logits = bert_clf(token_ids, masks)\n","        loss_func = nn.BCELoss()\n","        loss = loss_func(logits, labels)\n","        numpy_logits = logits.cpu().detach().numpy()\n","        label_ids = labels.to('cpu').numpy()\n","        \n","        bert_predicted += list(numpy_logits[:, 0] > 0.5)\n","        all_logits += list(numpy_logits[:, 0])\n","        true_labels.append(label_ids)"],"metadata":{"id":"GhLb4kqquICh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import classification_report,accuracy_score\n","flat_true_labels = np.concatenate(true_labels, axis=0)\n","print(accuracy_score(flat_true_labels, bert_predicted))\n","print(classification_report(flat_true_labels, bert_predicted, digits=3))"],"metadata":{"id":"tu4eKkJXuICk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Save"],"metadata":{"id":"QoSyw9sTdF2P"}},{"cell_type":"code","source":["torch.save({\n","            'epoch': epoch_num,\n","            'model_state_dict': bert_clf.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'loss': batch_loss,\n","            }, \"/content/drive/MyDrive/Keypoints/BERT-base.pth\")"],"metadata":{"id":"Oh2WcjuBxC3B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bert_clf = BertBinaryClassifier() \n","bert_clf.load_state_dict(torch.load(\"/content/drive/MyDrive/Keypoints/BERT-base.pth\")['model_state_dict'],strict=False) \n","bert_clf.to(device) \n","bert_clf.eval()"],"metadata":{"id":"t7y9Am9iyQCR"},"execution_count":null,"outputs":[]}]}