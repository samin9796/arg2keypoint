{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#OpenPrompt Installation\n",
        "OpenPrompt is a modular and flexible platform to develop a prompt-learning pipeline"
      ],
      "metadata": {
        "id": "-Yvycc1F2fJN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "no0X6k_mtXaO"
      },
      "outputs": [],
      "source": [
        "!pip install openprompt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Pre-Processing"
      ],
      "metadata": {
        "id": "_CpiFGWN3cOi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing Data from separate csv files.\n",
        "The datasets are available on https://github.com/samin9796/arg2keypoint.\n",
        "\n",
        "Note: There are different versions of trainning datasets, for whole training set, for few-shot and for subset experiments "
      ],
      "metadata": {
        "id": "qFWJa9Kv30kp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XPaeFuOwbuF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "train_df = pd.read_csv(\"train.csv\")\n",
        "dev_df = pd.read_csv(\"dev.csv\")\n",
        "test_df = pd.read_csv(\"test.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For **fewshot** and **subset** experiments the following line helps to filter out some extra columns:"
      ],
      "metadata": {
        "id": "g-N1NR3K4LVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train_df = train_df.filter(['Unnamed: 0', 'topic', 'argument', 'key_point','stance', 'label'], axis=1)"
      ],
      "metadata": {
        "id": "Hy_KOLdWVu3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuTS-Yrxwgz9"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_df.columns=['idx', 'topic', 'argument', 'key_point', 'stance', 'label']\n",
        "dev_df.columns=['idx', 'topic', 'argument', 'key_point', 'stance', 'label']\n",
        "test_df.columns=['idx', 'topic', 'argument', 'key_point', 'stance', 'label']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To merge train, dev, and test dataframes into one dataset:"
      ],
      "metadata": {
        "id": "Pzu-DWVI4GEp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDQqlLI8ses7"
      },
      "outputs": [],
      "source": [
        "from openprompt.data_utils import InputExample\n",
        "dataset = {}\n",
        "\n",
        "dataset['train'] = []\n",
        "dataset['dev'] = []\n",
        "dataset['test'] = []\n",
        "for index, data in train_df.iterrows():\n",
        "  input_example = InputExample(text_a = data['argument'], text_b = data['key_point'], label=int(data['label']), guid=data['idx'])\n",
        "  dataset['train'].append(input_example)\n",
        "for index, data in dev_df.iterrows():\n",
        "  input_example = InputExample(text_a = data['argument'], text_b = data['key_point'], label=int(data['label']), guid=data['idx'])\n",
        "  dataset['dev'].append(input_example)\n",
        "for index, data in test_df.iterrows():\n",
        "  input_example = InputExample(text_a = data['argument'], text_b = data['key_point'], label=int(data['label']), guid=data['idx'])\n",
        "  dataset['test'].append(input_example)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Obtain a Pre-trained Language Model"
      ],
      "metadata": {
        "id": "Zc_wQSEn3vzg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVg_RJgsLmOc"
      },
      "outputs": [],
      "source": [
        "from openprompt.plms import load_plm\n",
        "plm, tokenizer, model_config, WrapperClass = load_plm(\"t5\", \"t5-base\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define a Template\n",
        "There are five different templates in the first approach. In each experiment we use one of them. You need to uncomment each of them that you want to use."
      ],
      "metadata": {
        "id": "Eyh5M0Zc31nb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Manual Templates\n"
      ],
      "metadata": {
        "id": "BA5rjTHn4vtg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Template 1: 'The argument: [X1] and the keypoint [X2] are [Z].'\n"
      ],
      "metadata": {
        "id": "wAKS7mRqsjep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openprompt.prompts import ManualTemplate\n",
        "template_text = 'The argument: {\"placeholder\": \"text_a\"}, and the keypoint: {\"placeholder\": \"text_b\"} are {\"mask\"}'\n",
        "mytemplate = ManualTemplate(tokenizer=tokenizer, text=template_text)"
      ],
      "metadata": {
        "id": "mdDKN-NIsc_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Template 2: 'The argument: [X1] is [Z] with the keypoint: [X2]'\n"
      ],
      "metadata": {
        "id": "jk-gipybs7DQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4iEYRRS6LmMD"
      },
      "outputs": [],
      "source": [
        "# template_text = 'The argument: {\"placeholder\": \"text_a\"} is {\"mask\"} with the keypoint: {\"placeholder\": \"text_b\"}'\n",
        "# mytemplate = ManualTemplate(tokenizer=tokenizer, text=template_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Template 3: 'Does the argument: [X1] comprise the fact that [X2]? [Z]'"
      ],
      "metadata": {
        "id": "yolIHkn7tSVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# template_text = 'Does the argument: {\"placeholder\": \"text_a\"}, comprise the fact that: {\"placeholder\": \"text_b\"}? {\"mask\"}'\n",
        "# mytemplate = ManualTemplate(tokenizer=tokenizer, text=template_text)"
      ],
      "metadata": {
        "id": "irBF-bxnsQ_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Template 4: 'A keypoint is a summarization of the corresponding argument. In other words, an argument comprises a keypoint. Does the argument: [X1], comprise the keypoint [X2]? [Z]'\n"
      ],
      "metadata": {
        "id": "YUVDzBaZtkUW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# template_text = 'A keypoint is a summarization of the corresponding argument. In other words, an argument comprises a keypoint. Does the argument: {\"placeholder\": \"text_a\"}, comprise the the keypoint: {\"placeholder\": \"text_b\"}? {\"mask\"}'\n",
        "# mytemplate = ManualTemplate(tokenizer=tokenizer, text=template_text)"
      ],
      "metadata": {
        "id": "sf1MrtN_tpS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Mixed Template\n",
        "In MixedTemplate, you can use {\"soft\"} to denote a tunable template."
      ],
      "metadata": {
        "id": "LuCovjr04z_Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Template 5: 'Argument: [X1] Keypoint: [X2] {\"soft\": \"Does\"} {\"soft\": \"the\", \"soft_id\": 1} argument matches {\"soft_id\": 1} keypoint? [Z]'"
      ],
      "metadata": {
        "id": "uUDoomkct44W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnIoBvR5t6Hy"
      },
      "outputs": [],
      "source": [
        "# from openprompt.prompts import MixedTemplate\n",
        "\n",
        "# mytemplate = MixedTemplate(model=plm, tokenizer=tokenizer, text='Argument: {\"placeholder\": \"text_a\"} Keypiont: {\"placeholder\": \"text_b\"} {\"soft\": \"Does\"} {\"soft\": \"the\", \"soft_id\": 1} argument matches {\"soft_id\": 1} keypoint? {\"mask\"}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBW1d1S1LmJk"
      },
      "outputs": [],
      "source": [
        "wrapped_example = mytemplate.wrap_one_example(dataset['train'][0])\n",
        "print(wrapped_example)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tokenization"
      ],
      "metadata": {
        "id": "fGC2kTGi6L6e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlSzMxneOA0N"
      },
      "outputs": [],
      "source": [
        "wrapped_t5tokenizer = WrapperClass(max_seq_length=128, decoder_max_length=3, tokenizer=tokenizer,truncate_method=\"head\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcY_5-9xOAxn"
      },
      "outputs": [],
      "source": [
        "tokenized_example = wrapped_t5tokenizer.tokenize_one_example(wrapped_example, teacher_forcing=False)\n",
        "print(tokenized_example)\n",
        "print(tokenizer.convert_ids_to_tokens(tokenized_example['input_ids']))\n",
        "print(tokenizer.convert_ids_to_tokens(tokenized_example['decoder_input_ids']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wIUOnlxOFOn"
      },
      "outputs": [],
      "source": [
        "model_inputs = {}\n",
        "for split in ['train', 'dev', 'test']:\n",
        "    model_inputs[split] = []\n",
        "    for sample in dataset[split]:\n",
        "        tokenized_example = wrapped_t5tokenizer.tokenize_one_example(mytemplate.wrap_one_example(sample), teacher_forcing=False)\n",
        "        model_inputs[split].append(tokenized_example)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PamNuSeGOFMJ"
      },
      "outputs": [],
      "source": [
        "from openprompt import PromptDataLoader\n",
        "\n",
        "train_dataloader = PromptDataLoader(dataset=dataset[\"train\"], template=mytemplate, tokenizer=tokenizer,\n",
        "    tokenizer_wrapper_class=WrapperClass, max_seq_length=256, decoder_max_length=3,\n",
        "    batch_size=4,shuffle=True, teacher_forcing=False, predict_eos_token=False,\n",
        "    truncate_method=\"head\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define a Verbalizer\n",
        " In classification, you need to define your verbalizer, which is a mapping from logits on the vocabulary to the final label probability."
      ],
      "metadata": {
        "id": "AHrXhidb53Od"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example the verbalizer contains multiple label words in each class. We have two different vocabulary as final labels:\n",
        "\n",
        "For **template 1** and **template 2** use the following verbalizer:"
      ],
      "metadata": {
        "id": "A54iLV_bz1tl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIryCsJ7OAuh"
      },
      "outputs": [],
      "source": [
        "from openprompt.prompts import ManualVerbalizer\n",
        "import torch\n",
        "\n",
        "myverbalizer = ManualVerbalizer(tokenizer, num_classes=2,\n",
        "                        label_words=[[\"matched\"], [\"not matched\"]])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For **template 3** , **template 4**, and **template 5** uncomment and use the following verbalizer:"
      ],
      "metadata": {
        "id": "Rwd9AT0Lzy0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from openprompt.prompts import ManualVerbalizer\n",
        "# import torch\n",
        "\n",
        "# myverbalizer = ManualVerbalizer(tokenizer, num_classes=2,\n",
        "#                         label_words=[[\"Yes\"], [\"No\"]])"
      ],
      "metadata": {
        "id": "9teKI6Ke0Ht9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a pseudo output from the plm, and see what the verbalizer do:"
      ],
      "metadata": {
        "id": "sSaUuWuo1Cb3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(myverbalizer.label_words_ids)\n",
        "logits = torch.randn(2,len(tokenizer)) \n",
        "print(myverbalizer.process_logits(logits)) "
      ],
      "metadata": {
        "id": "DLehG0ad0jKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxVwY-kJOAnJ"
      },
      "outputs": [],
      "source": [
        "from openprompt import PromptForClassification\n",
        "\n",
        "use_cuda = True\n",
        "prompt_model = PromptForClassification(plm=plm,template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\n",
        "if use_cuda:\n",
        "    prompt_model=  prompt_model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFPIHOXJROTl"
      },
      "source": [
        "##Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_A5DbatekKg3"
      },
      "outputs": [],
      "source": [
        "from transformers import  AdamW, get_linear_schedule_with_warmup\n",
        "loss_func = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "\n",
        "# it's always good practice to set no decay to biase and LayerNorm parameters\n",
        "optimizer_grouped_parameters1 = [\n",
        "    {'params': [p for n, p in prompt_model.plm.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in prompt_model.plm.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "\n",
        "# Using different optimizer for prompt parameters and model parameters\n",
        "optimizer_grouped_parameters2 = [\n",
        "    {'params': [p for n,p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n",
        "]\n",
        "\n",
        "optimizer1 = AdamW(optimizer_grouped_parameters1, lr=1e-4)\n",
        "optimizer2 = AdamW(optimizer_grouped_parameters2, lr=1e-3)\n",
        "\n",
        "for epoch in range(3):\n",
        "    tot_loss = 0\n",
        "    for step, inputs in enumerate(train_dataloader):\n",
        "        if use_cuda:\n",
        "            inputs = inputs.cuda()\n",
        "        logits = prompt_model(inputs)\n",
        "        labels = inputs['label']\n",
        "        loss = loss_func(logits, labels)\n",
        "        loss.backward()\n",
        "        tot_loss += loss.item()\n",
        "        optimizer1.step()\n",
        "        optimizer1.zero_grad()\n",
        "        optimizer2.step()\n",
        "        optimizer2.zero_grad()\n",
        "        # print(tot_loss/(step+1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYxm_zt9UArp"
      },
      "source": [
        "#Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55rwzLO1QC42"
      },
      "outputs": [],
      "source": [
        "\n",
        "validation_dataloader = PromptDataLoader(dataset=dataset[\"test\"], template=mytemplate, tokenizer=tokenizer,\n",
        "    tokenizer_wrapper_class=WrapperClass, max_seq_length=256, decoder_max_length=3,\n",
        "    batch_size=4,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
        "    truncate_method=\"head\")\n",
        "\n",
        "allpreds = []\n",
        "alllabels = []\n",
        "for step, inputs in enumerate(validation_dataloader):\n",
        "    if use_cuda:\n",
        "        inputs = inputs.cuda()\n",
        "    logits = prompt_model(inputs)\n",
        "    labels = inputs['label']\n",
        "    alllabels.extend(labels.cpu().tolist())\n",
        "    allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
        "\n",
        "acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
        "print(acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uK_-BFuwQCtX"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(alllabels, allpreds, digits=3))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Approach1-T5-base-OpenPrompt.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOMOMTtZ9HL7+5LIkHtuciJ"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}