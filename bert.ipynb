{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"bert.ipynb","provenance":[],"collapsed_sections":["4zU8ZdsKfuT3","bwttor08QPQt"],"mount_file_id":"1SinDJigMmuN7NwdMZ4KQWX51QGFlbi6b","authorship_tag":"ABX9TyMro9xfLSmMlaMj5fotsqvO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"e36fc1536fc24a1ab00470486c6b5b6f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_844fb2cf32df46edb24d21b48db91c8e","IPY_MODEL_d75164d4f0504ba4ade40f3977878650","IPY_MODEL_9abdf0769fe847d2a0260c986da07c36"],"layout":"IPY_MODEL_2a5ed82d9c964763a41ec07fda5e33c5"}},"844fb2cf32df46edb24d21b48db91c8e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_790d83c4a7574770baff2a3c0b8e6c6e","placeholder":"​","style":"IPY_MODEL_2979f7a12be4496fb6a32f01148a32da","value":"Downloading: 100%"}},"d75164d4f0504ba4ade40f3977878650":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e99a974b027e4cd9bf37d25e565281a5","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7331b239d20041f7b1c94242c0dd5ea4","value":231508}},"9abdf0769fe847d2a0260c986da07c36":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c57e5ed127454557a5e9f0c091ccbacd","placeholder":"​","style":"IPY_MODEL_593b289ed95c44abbbd20db6fad94058","value":" 226k/226k [00:00&lt;00:00, 814kB/s]"}},"2a5ed82d9c964763a41ec07fda5e33c5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"790d83c4a7574770baff2a3c0b8e6c6e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2979f7a12be4496fb6a32f01148a32da":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e99a974b027e4cd9bf37d25e565281a5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7331b239d20041f7b1c94242c0dd5ea4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c57e5ed127454557a5e9f0c091ccbacd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"593b289ed95c44abbbd20db6fad94058":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4b7372ff655f4d8785930d21c71304a1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_135bad7cc5bd45c088384520c7eece0e","IPY_MODEL_5e31328b930840a1bd7d1564b444790f","IPY_MODEL_e522938ea40b4f1081db355934138ce6"],"layout":"IPY_MODEL_f88f5add71254e7e814169d841c3af96"}},"135bad7cc5bd45c088384520c7eece0e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b3da8df5e1e04fc0802e5c2364680b9a","placeholder":"​","style":"IPY_MODEL_f9511e59404241478922c3d25708fc47","value":"Downloading: 100%"}},"5e31328b930840a1bd7d1564b444790f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5fd44bfc2076483bba3863c6984db814","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_044e83f30d7249b0bed72222b2738f6e","value":28}},"e522938ea40b4f1081db355934138ce6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5d762ebef81943e0b95a44f4496f416d","placeholder":"​","style":"IPY_MODEL_4e75da6989b64eb7b1e577f46c773de3","value":" 28.0/28.0 [00:00&lt;00:00, 335B/s]"}},"f88f5add71254e7e814169d841c3af96":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b3da8df5e1e04fc0802e5c2364680b9a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9511e59404241478922c3d25708fc47":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5fd44bfc2076483bba3863c6984db814":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"044e83f30d7249b0bed72222b2738f6e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5d762ebef81943e0b95a44f4496f416d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e75da6989b64eb7b1e577f46c773de3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"de38784ae17f40f0a28ff83cb265318e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e6e74718eb05491493a03082ddbe04a2","IPY_MODEL_f7e5b51a3f394c438043255fb89195db","IPY_MODEL_3370a92d69b54d418406c10367af722b"],"layout":"IPY_MODEL_d8e438eaa0f441b89dbd1a9fb17a723f"}},"e6e74718eb05491493a03082ddbe04a2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_005298c2200845708b2521a6f156dd43","placeholder":"​","style":"IPY_MODEL_7adc132608e14d0e8460f680806ab0a2","value":"Downloading: 100%"}},"f7e5b51a3f394c438043255fb89195db":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9920845e184e41859218436c2e21f17b","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_80a7b85a8a8e48ed9f44d461bf24a719","value":570}},"3370a92d69b54d418406c10367af722b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0fe985231d30463eb293c20278a15e7f","placeholder":"​","style":"IPY_MODEL_16face4b13f645da91968645c25746d9","value":" 570/570 [00:00&lt;00:00, 2.14kB/s]"}},"d8e438eaa0f441b89dbd1a9fb17a723f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"005298c2200845708b2521a6f156dd43":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7adc132608e14d0e8460f680806ab0a2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9920845e184e41859218436c2e21f17b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"80a7b85a8a8e48ed9f44d461bf24a719":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0fe985231d30463eb293c20278a15e7f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"16face4b13f645da91968645c25746d9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"70c6c71f1bc14fb2b1304411b09583cc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8ace0fb45ef848fd958953bca447cace","IPY_MODEL_0fe3999f40944148bafde526b6b87638","IPY_MODEL_d52c503d32434554bcdf610bea7b3b9d"],"layout":"IPY_MODEL_0cdc61083289408d846ed2eb6b07ce3c"}},"8ace0fb45ef848fd958953bca447cace":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3caae2e6b1844315a9c4d87a739e84c8","placeholder":"​","style":"IPY_MODEL_8a4075e031c8431b8875d8096a45965b","value":"Downloading: 100%"}},"0fe3999f40944148bafde526b6b87638":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f678f30e794347d5ae3600c963efec6e","max":440473133,"min":0,"orientation":"horizontal","style":"IPY_MODEL_92d4ecefd68c451e8e2148081ae3fb14","value":440473133}},"d52c503d32434554bcdf610bea7b3b9d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_022738c085214e74a0e3d68f4584772b","placeholder":"​","style":"IPY_MODEL_a41d96b3c04043e2926a7f6f99720f84","value":" 420M/420M [00:13&lt;00:00, 51.4MB/s]"}},"0cdc61083289408d846ed2eb6b07ce3c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3caae2e6b1844315a9c4d87a739e84c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a4075e031c8431b8875d8096a45965b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f678f30e794347d5ae3600c963efec6e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"92d4ecefd68c451e8e2148081ae3fb14":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"022738c085214e74a0e3d68f4584772b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a41d96b3c04043e2926a7f6f99720f84":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a0d79ca33b8940cea6748e0757032691":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9d8b45faeffe4956862fbb528b761de4","IPY_MODEL_544c55993fb24bdd8e2d0029def933a3","IPY_MODEL_71cf8329b3d442b08537e1a4cdff6b04"],"layout":"IPY_MODEL_e369a531e7f54fa2bdf6b983d6d85127"}},"9d8b45faeffe4956862fbb528b761de4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bc3d53040b024f96907ace59fc7234e7","placeholder":"​","style":"IPY_MODEL_a44cdbc37d2345a7b52c10006a2a0cb6","value":"Downloading: 100%"}},"544c55993fb24bdd8e2d0029def933a3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cd400c9cf96d4cb9952172503add70fb","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7e16e2444d6a4d4f84f4297769a7ed0d","value":231508}},"71cf8329b3d442b08537e1a4cdff6b04":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_312a81607ac04adb8ecb12a0abe3ff98","placeholder":"​","style":"IPY_MODEL_cd8b95bd05374e4d9862142d15a50c4e","value":" 226k/226k [00:00&lt;00:00, 2.54MB/s]"}},"e369a531e7f54fa2bdf6b983d6d85127":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc3d53040b024f96907ace59fc7234e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a44cdbc37d2345a7b52c10006a2a0cb6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cd400c9cf96d4cb9952172503add70fb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e16e2444d6a4d4f84f4297769a7ed0d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"312a81607ac04adb8ecb12a0abe3ff98":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd8b95bd05374e4d9862142d15a50c4e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4ff7415fb5b24e56b66d49a419626590":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6cd4bad8b6114f5d83937b2196250e58","IPY_MODEL_35909b2e50b7415383c2898a8564f33a","IPY_MODEL_c4b72b41d4e246b9b06392a7042d0674"],"layout":"IPY_MODEL_2b519cab4d7e41a09b9b318302ebb6a5"}},"6cd4bad8b6114f5d83937b2196250e58":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0b866b28b8494938bcd24352de0b83a7","placeholder":"​","style":"IPY_MODEL_cc259b588fb4486a87e6d8b8bdc40d15","value":"Downloading: 100%"}},"35909b2e50b7415383c2898a8564f33a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_114dbc4ecfd14b77870ce95c79676c62","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_15b891cf237341cd9ccf997fec3c2cbb","value":28}},"c4b72b41d4e246b9b06392a7042d0674":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7745a628d33942af9022a395be069e04","placeholder":"​","style":"IPY_MODEL_d780337e432348979ce7bcd6312af3ec","value":" 28.0/28.0 [00:00&lt;00:00, 425B/s]"}},"2b519cab4d7e41a09b9b318302ebb6a5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0b866b28b8494938bcd24352de0b83a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc259b588fb4486a87e6d8b8bdc40d15":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"114dbc4ecfd14b77870ce95c79676c62":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15b891cf237341cd9ccf997fec3c2cbb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7745a628d33942af9022a395be069e04":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d780337e432348979ce7bcd6312af3ec":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"495188cfe29843d08d5b0f3251a4e04f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8ddf098f9fdc419fb8b269ac3444fe2a","IPY_MODEL_bd6fd48a6a43471e9331ea2ddf057459","IPY_MODEL_d57d873a14614ff392bac18317c5222d"],"layout":"IPY_MODEL_de08b3c7a0ea4553b08a068ea6efca6f"}},"8ddf098f9fdc419fb8b269ac3444fe2a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b782544f70964ae783d994dfe7f84747","placeholder":"​","style":"IPY_MODEL_0f9e47d90410437a9e1f519916646d34","value":"Downloading: 100%"}},"bd6fd48a6a43471e9331ea2ddf057459":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1a1e363d1d8e4962bb9d89bc8c81f279","max":571,"min":0,"orientation":"horizontal","style":"IPY_MODEL_149d06df74bf417eba33d0386db0eb93","value":571}},"d57d873a14614ff392bac18317c5222d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_076cc52243e84940a310a831b4be55e9","placeholder":"​","style":"IPY_MODEL_364254a6517d48898d7453ad127dc119","value":" 571/571 [00:00&lt;00:00, 10.9kB/s]"}},"de08b3c7a0ea4553b08a068ea6efca6f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b782544f70964ae783d994dfe7f84747":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f9e47d90410437a9e1f519916646d34":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1a1e363d1d8e4962bb9d89bc8c81f279":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"149d06df74bf417eba33d0386db0eb93":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"076cc52243e84940a310a831b4be55e9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"364254a6517d48898d7453ad127dc119":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3fe739e0666a484bbc2a1a22c5c00f89":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a6a21e7213c946a8bf1ca5be2df02a52","IPY_MODEL_a5c12f8555224ec28582783de97debf4","IPY_MODEL_d79eea5cdec9408493e7f5189a7aaa78"],"layout":"IPY_MODEL_8ac31770e97d455488374e8ae1f07706"}},"a6a21e7213c946a8bf1ca5be2df02a52":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_671fbad3bfdf48c9a4535f910f701eaf","placeholder":"​","style":"IPY_MODEL_68aeea6e038c4e1aba083bdc3221292f","value":"Downloading: 100%"}},"a5c12f8555224ec28582783de97debf4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_088a856a699944abbb5a83f141c2a414","max":1344997306,"min":0,"orientation":"horizontal","style":"IPY_MODEL_41f1058b54854c1eaff1e767dcdec694","value":1344997306}},"d79eea5cdec9408493e7f5189a7aaa78":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9402fa4a8fbc44d7a310fb00ef23f030","placeholder":"​","style":"IPY_MODEL_033f4457d5394ee9bcbc4c4c4b8563aa","value":" 1.25G/1.25G [00:31&lt;00:00, 51.3MB/s]"}},"8ac31770e97d455488374e8ae1f07706":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"671fbad3bfdf48c9a4535f910f701eaf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"68aeea6e038c4e1aba083bdc3221292f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"088a856a699944abbb5a83f141c2a414":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"41f1058b54854c1eaff1e767dcdec694":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9402fa4a8fbc44d7a310fb00ef23f030":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"033f4457d5394ee9bcbc4c4c4b8563aa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1SinDJigMmuN7NwdMZ4KQWX51QGFlbi6b?usp=sharing)"],"metadata":{"id":"158WIT4BN0Zs"}},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"id":"XfKEXkyNv1iy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## base(baseline)"],"metadata":{"id":"4zU8ZdsKfuT3"}},{"cell_type":"code","source":["import tensorflow as tf\n","\n","# Get the GPU device name.\n","device_name = tf.test.gpu_device_name()\n","\n","# The device name should look like the following:\n","if device_name == '/device:GPU:0':\n","    print('Found GPU at: {}'.format(device_name))\n","else:\n","    raise SystemError('GPU device not found')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652635382661,"user_tz":-120,"elapsed":16357,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"ca44c2a3-0e67-4c5c-a42a-534e279b5f4d","id":"cn9HeShXfuT-"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Found GPU at: /device:GPU:0\n"]}]},{"cell_type":"code","source":["import torch\n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652635410076,"user_tz":-120,"elapsed":13792,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"b328a341-0b07-4349-b227-930b456ed1fd","id":"PRE9BfK9fuT_"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla T4\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","train = pd.read_csv(\"/content/drive/MyDrive/Keypoints/train.csv\")\n","dev = pd.read_csv(\"/content/drive/MyDrive/Keypoints/dev.csv\")\n","test = pd.read_csv(\"/content/drive/MyDrive/Keypoints/test.csv\")"],"metadata":{"executionInfo":{"status":"ok","timestamp":1652635413728,"user_tz":-120,"elapsed":1125,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"id":"1w0-gaH6fuT_"},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["for split in [train,dev,test]:\n","  for i in split.index:\n","    arg = split['argument'][i]\n","    key = split['key_point'][i]\n","    if arg[-1] != '.':\n","      pair = arg + '. ' + key + '.'\n","      split.at[i, 'pair'] = pair\n","    else:\n","      pair = arg + ' ' + key + '.'\n","      split.at[i, 'pair'] = pair"],"metadata":{"executionInfo":{"status":"ok","timestamp":1652635415593,"user_tz":-120,"elapsed":816,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"id":"AMSvcN5RfuT_"},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["print(train['pair'][0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652635418057,"user_tz":-120,"elapsed":217,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"3538aa94-1bfd-4d86-9f7f-4c5fc88d6fc4","id":"2utMmFAofuT_"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["a person created through cloning could potentially have developmental problems caused by imperfections in the cloning process. Cloning is not understood enough yet.\n"]}]},{"cell_type":"code","source":["pairs_train = train.pair.values\n","labels_train = train.label.values\n","\n","pairs_dev = dev.pair.values\n","labels_dev = dev.label.values\n","\n","pairs_test = test.pair.values\n","labels_test = test.label.values"],"metadata":{"executionInfo":{"status":"ok","timestamp":1652635420203,"user_tz":-120,"elapsed":2,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"id":"VvE8qTNxfuT_"},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# base\n","from transformers import BertTokenizer\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":112,"referenced_widgets":["e36fc1536fc24a1ab00470486c6b5b6f","844fb2cf32df46edb24d21b48db91c8e","d75164d4f0504ba4ade40f3977878650","9abdf0769fe847d2a0260c986da07c36","2a5ed82d9c964763a41ec07fda5e33c5","790d83c4a7574770baff2a3c0b8e6c6e","2979f7a12be4496fb6a32f01148a32da","e99a974b027e4cd9bf37d25e565281a5","7331b239d20041f7b1c94242c0dd5ea4","c57e5ed127454557a5e9f0c091ccbacd","593b289ed95c44abbbd20db6fad94058","4b7372ff655f4d8785930d21c71304a1","135bad7cc5bd45c088384520c7eece0e","5e31328b930840a1bd7d1564b444790f","e522938ea40b4f1081db355934138ce6","f88f5add71254e7e814169d841c3af96","b3da8df5e1e04fc0802e5c2364680b9a","f9511e59404241478922c3d25708fc47","5fd44bfc2076483bba3863c6984db814","044e83f30d7249b0bed72222b2738f6e","5d762ebef81943e0b95a44f4496f416d","4e75da6989b64eb7b1e577f46c773de3","de38784ae17f40f0a28ff83cb265318e","e6e74718eb05491493a03082ddbe04a2","f7e5b51a3f394c438043255fb89195db","3370a92d69b54d418406c10367af722b","d8e438eaa0f441b89dbd1a9fb17a723f","005298c2200845708b2521a6f156dd43","7adc132608e14d0e8460f680806ab0a2","9920845e184e41859218436c2e21f17b","80a7b85a8a8e48ed9f44d461bf24a719","0fe985231d30463eb293c20278a15e7f","16face4b13f645da91968645c25746d9"]},"executionInfo":{"status":"ok","timestamp":1652635459732,"user_tz":-120,"elapsed":2287,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"eef72d44-02ba-4569-f649-e712f43859f5","id":"gubFfDH9fuUA"},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e36fc1536fc24a1ab00470486c6b5b6f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b7372ff655f4d8785930d21c71304a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de38784ae17f40f0a28ff83cb265318e"}},"metadata":{}}]},{"cell_type":"code","source":["max_len = 0\n","\n","for split in [pairs_train,pairs_dev,pairs_test]:\n","  for pair in split:\n","\n","      # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n","      input_ids = tokenizer.encode(pair, add_special_tokens=True)\n","\n","      # Update the maximum sentence length.\n","      max_len = max(max_len, len(input_ids))\n","\n","print('Max sentence length: ', max_len)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652635488871,"user_tz":-120,"elapsed":27225,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"699c369f-0841-4bfa-8922-62cc97596882","id":"eBiw1nYHfuUA"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Max sentence length:  73\n"]}]},{"cell_type":"code","source":["# Tokenize all of the sentences and map the tokens to thier word IDs.\n","input_ids_train = []\n","attention_masks_train = []\n","\n","# For every sentence...\n","for pair in pairs_train:\n","    encoded_dict = tokenizer.encode_plus(\n","                        pair,     # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = 73,   # Pad & truncate all sentences.\n","                        pad_to_max_length = True,\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","    \n","    # Add the encoded sentence to the list.    \n","    input_ids_train.append(encoded_dict['input_ids'])\n","    \n","    # And its attention mask (simply differentiates padding from non-padding).\n","    attention_masks_train.append(encoded_dict['attention_mask'])\n","\n","# Convert the lists into tensors.\n","input_ids_train = torch.cat(input_ids_train, dim=0)\n","attention_masks_train = torch.cat(attention_masks_train, dim=0)\n","labels_train = torch.tensor(labels_train)\n","\n","# Print sentence 0, now as a list of IDs.\n","print('Original: ', pairs_train[0])\n","print('Token IDs:', input_ids_train[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652635583347,"user_tz":-120,"elapsed":22479,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"d5529c5d-2ce0-4350-a8a4-e2141edb46af","id":"zFZkWDujfuUA"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Original:  a person created through cloning could potentially have developmental problems caused by imperfections in the cloning process. Cloning is not understood enough yet.\n","Token IDs: tensor([  101,  1037,  2711,  2580,  2083, 18856, 13369,  2071,  9280,  2031,\n","        13908,  3471,  3303,  2011, 29238,  8496,  1999,  1996, 18856, 13369,\n","         2832,  1012, 18856, 13369,  2003,  2025,  5319,  2438,  2664,  1012,\n","          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0])\n"]}]},{"cell_type":"code","source":["# Tokenize all of the sentences and map the tokens to thier word IDs.\n","input_ids_dev = []\n","attention_masks_dev = []\n","\n","for pair in pairs_dev:\n","    encoded_dict = tokenizer.encode_plus(\n","                        pair,     # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = 73,   # Pad & truncate all sentences.\n","                        pad_to_max_length = True,\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","        \n","    input_ids_dev.append(encoded_dict['input_ids'])\n","    \n","\n","    attention_masks_dev.append(encoded_dict['attention_mask'])\n","\n","\n","input_ids_dev = torch.cat(input_ids_dev, dim=0)\n","attention_masks_dev = torch.cat(attention_masks_dev, dim=0)\n","labels_dev = torch.tensor(labels_dev)\n","\n","\n","print('Original: ', pairs_dev[0])\n","print('Token IDs:', input_ids_dev[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652635606430,"user_tz":-120,"elapsed":3259,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"2039f78e-40c2-4802-8f08-3d5737d1c930","id":"b0BrYkbsfuUB"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Original:  a man or woman has the right to do what they wish with their body, and if they choose to sell it for sex, the government should not interfere. Legalizing sex work boosts the economy.\n","Token IDs: tensor([  101,  1037,  2158,  2030,  2450,  2038,  1996,  2157,  2000,  2079,\n","         2054,  2027,  4299,  2007,  2037,  2303,  1010,  1998,  2065,  2027,\n","         5454,  2000,  5271,  2009,  2005,  3348,  1010,  1996,  2231,  2323,\n","         2025, 15115,  1012,  3423,  6026,  3348,  2147, 12992,  2015,  1996,\n","         4610,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0])\n"]}]},{"cell_type":"code","source":["from torch.utils.data import TensorDataset\n","train_dataset = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n","dev_dataset = TensorDataset(input_ids_dev, attention_masks_dev, labels_dev)"],"metadata":{"executionInfo":{"status":"ok","timestamp":1652635607314,"user_tz":-120,"elapsed":3,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"id":"I9Af2VxxfuUB"},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","batch_size = 32\n","\n","\n","train_dataloader = DataLoader(\n","            train_dataset,  # The training samples.\n","            sampler = RandomSampler(train_dataset), # Select batches randomly\n","            batch_size = batch_size # Trains with this batch size.\n","        )\n","\n","dev_dataloader = DataLoader(\n","            dev_dataset, # The validation samples.\n","            sampler = SequentialSampler(dev_dataset), # Pull out batches sequentially.\n","            batch_size = batch_size # Evaluate with this batch size.\n","        )"],"metadata":{"executionInfo":{"status":"ok","timestamp":1652635608400,"user_tz":-120,"elapsed":204,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"id":"TIELWZ0JfuUB"},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["from transformers import BertForSequenceClassification, AdamW, BertConfig\n","\n","\n","model = BertForSequenceClassification.from_pretrained(\n","    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n","    num_labels = 2, # The number of output labels--2 for binary classification.\n","                    # You can increase this for multi-class tasks.   \n","    output_attentions = False, # Whether the model returns attentions weights.\n","    output_hidden_states = False, # Whether the model returns all hidden-states.\n",")\n","\n","\n","model.cuda()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["70c6c71f1bc14fb2b1304411b09583cc","8ace0fb45ef848fd958953bca447cace","0fe3999f40944148bafde526b6b87638","d52c503d32434554bcdf610bea7b3b9d","0cdc61083289408d846ed2eb6b07ce3c","3caae2e6b1844315a9c4d87a739e84c8","8a4075e031c8431b8875d8096a45965b","f678f30e794347d5ae3600c963efec6e","92d4ecefd68c451e8e2148081ae3fb14","022738c085214e74a0e3d68f4584772b","a41d96b3c04043e2926a7f6f99720f84"]},"executionInfo":{"status":"ok","timestamp":1652635636172,"user_tz":-120,"elapsed":15605,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"94da7c89-0439-4072-8ca2-bb509e22e949","id":"4PzghSIyfuUB"},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70c6c71f1bc14fb2b1304411b09583cc"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n","# I believe the 'W' stands for 'Weight Decay fix\"\n","optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n","                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n","                )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652635639211,"user_tz":-120,"elapsed":224,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"094b78b6-1966-482e-aea1-959c0f7177cc","id":"-9NvWux1fuUC"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n"]}]},{"cell_type":"code","source":["from transformers import get_linear_schedule_with_warmup\n","\n","# Number of training epochs. The BERT authors recommend between 2 and 4. \n","\n","epochs = 3\n","\n","total_steps = len(train_dataloader) * epochs\n","\n","# Create the learning rate scheduler.\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0, # Default value in run_glue.py\n","                                            num_training_steps = total_steps)"],"metadata":{"executionInfo":{"status":"ok","timestamp":1652635641557,"user_tz":-120,"elapsed":208,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"id":"lZ6pg47tfuUC"},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"],"metadata":{"executionInfo":{"status":"ok","timestamp":1652635642713,"user_tz":-120,"elapsed":2,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"id":"Rap-bocJfuUC"},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["import time\n","import datetime\n","\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"],"metadata":{"executionInfo":{"status":"ok","timestamp":1652635644061,"user_tz":-120,"elapsed":200,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"id":"3iBfOm88fuUC"},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["import random\n","import numpy as np\n","\n","seed_val = 42\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","\n","training_stats = []\n","\n","# Measure the total training time for the whole run.\n","total_t0 = time.time()\n","\n","# For each epoch...\n","for epoch_i in range(0, epochs):\n","    \n","\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    # Measure how long the training epoch takes.\n","    t0 = time.time()\n","\n","    # Reset the total loss for this epoch.\n","    total_train_loss = 0\n","\n","    model.train()\n","\n","    # For each batch of training data...\n","    for step, batch in enumerate(train_dataloader):\n","\n","        # Progress update every 40 batches.\n","        if step % 40 == 0 and not step == 0:\n","            # Calculate elapsed time in minutes.\n","            elapsed = format_time(time.time() - t0)\n","            \n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","\n","        model.zero_grad()        \n","\n","        result = model(b_input_ids, \n","                       token_type_ids=None, \n","                       attention_mask=b_input_mask, \n","                       labels=b_labels,\n","                       return_dict=True)\n","\n","        loss = result.loss\n","        logits = result.logits\n","\n","        total_train_loss += loss.item()\n","\n","\n","        loss.backward()\n","\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        optimizer.step()\n","\n","        # Update the learning rate.\n","        scheduler.step()\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_train_loss = total_train_loss / len(train_dataloader)            \n","    \n","    # Measure how long this epoch took.\n","    training_time = format_time(time.time() - t0)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epcoh took: {:}\".format(training_time))\n","        \n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    t0 = time.time()\n","\n","\n","    model.eval()\n","\n","    # Tracking variables \n","    total_eval_accuracy = 0\n","    total_eval_loss = 0\n","    nb_eval_steps = 0\n","\n","    for batch in dev_dataloader:\n","        \n","\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","        \n","\n","        with torch.no_grad():        \n","\n","\n","            result = model(b_input_ids, \n","                           token_type_ids=None, \n","                           attention_mask=b_input_mask,\n","                           labels=b_labels,\n","                           return_dict=True)\n","\n","\n","        loss = result.loss\n","        logits = result.logits\n","            \n","\n","        total_eval_loss += loss.item()\n","\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","\n","\n","        total_eval_accuracy += flat_accuracy(logits, label_ids)\n","        \n","\n","    # Report the final accuracy for this validation run.\n","    avg_val_accuracy = total_eval_accuracy / len(dev_dataloader)\n","    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_val_loss = total_eval_loss / len(dev_dataloader)\n","    \n","    # Measure how long the validation run took.\n","    validation_time = format_time(time.time() - t0)\n","    \n","    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n","    print(\"  Validation took: {:}\".format(validation_time))\n","\n","    # Record all statistics from this epoch.\n","    training_stats.append(\n","        {\n","            'epoch': epoch_i + 1,\n","            'Training Loss': avg_train_loss,\n","            'Valid. Loss': avg_val_loss,\n","            'Valid. Accur.': avg_val_accuracy,\n","            'Training Time': training_time,\n","            'Validation Time': validation_time\n","        }\n","    )\n","\n","print(\"\")\n","print(\"Training complete!\")\n","\n","print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"5db70798-fa58-491b-feed-63625abb7ed2","id":"vGzpCJtgfuUC","executionInfo":{"status":"ok","timestamp":1652636349679,"user_tz":-120,"elapsed":704351,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}}},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======== Epoch 1 / 3 ========\n","Training...\n","  Batch    40  of    532.    Elapsed: 0:00:15.\n","  Batch    80  of    532.    Elapsed: 0:00:30.\n","  Batch   120  of    532.    Elapsed: 0:00:46.\n","  Batch   160  of    532.    Elapsed: 0:01:01.\n","  Batch   200  of    532.    Elapsed: 0:01:17.\n","  Batch   240  of    532.    Elapsed: 0:01:33.\n","  Batch   280  of    532.    Elapsed: 0:01:49.\n","  Batch   320  of    532.    Elapsed: 0:02:05.\n","  Batch   360  of    532.    Elapsed: 0:02:22.\n","  Batch   400  of    532.    Elapsed: 0:02:38.\n","  Batch   440  of    532.    Elapsed: 0:02:54.\n","  Batch   480  of    532.    Elapsed: 0:03:11.\n","  Batch   520  of    532.    Elapsed: 0:03:28.\n","\n","  Average training loss: 0.35\n","  Training epcoh took: 0:03:33\n","\n","Running Validation...\n","  Accuracy: 0.83\n","  Validation Loss: 0.42\n","  Validation took: 0:00:14\n","\n","======== Epoch 2 / 3 ========\n","Training...\n","  Batch    40  of    532.    Elapsed: 0:00:17.\n","  Batch    80  of    532.    Elapsed: 0:00:33.\n","  Batch   120  of    532.    Elapsed: 0:00:50.\n","  Batch   160  of    532.    Elapsed: 0:01:07.\n","  Batch   200  of    532.    Elapsed: 0:01:24.\n","  Batch   240  of    532.    Elapsed: 0:01:41.\n","  Batch   280  of    532.    Elapsed: 0:01:58.\n","  Batch   320  of    532.    Elapsed: 0:02:15.\n","  Batch   360  of    532.    Elapsed: 0:02:32.\n","  Batch   400  of    532.    Elapsed: 0:02:49.\n","  Batch   440  of    532.    Elapsed: 0:03:06.\n","  Batch   480  of    532.    Elapsed: 0:03:23.\n","  Batch   520  of    532.    Elapsed: 0:03:39.\n","\n","  Average training loss: 0.21\n","  Training epcoh took: 0:03:45\n","\n","Running Validation...\n","  Accuracy: 0.81\n","  Validation Loss: 0.46\n","  Validation took: 0:00:14\n","\n","======== Epoch 3 / 3 ========\n","Training...\n","  Batch    40  of    532.    Elapsed: 0:00:17.\n","  Batch    80  of    532.    Elapsed: 0:00:34.\n","  Batch   120  of    532.    Elapsed: 0:00:51.\n","  Batch   160  of    532.    Elapsed: 0:01:08.\n","  Batch   200  of    532.    Elapsed: 0:01:25.\n","  Batch   240  of    532.    Elapsed: 0:01:42.\n","  Batch   280  of    532.    Elapsed: 0:01:58.\n","  Batch   320  of    532.    Elapsed: 0:02:15.\n","  Batch   360  of    532.    Elapsed: 0:02:32.\n","  Batch   400  of    532.    Elapsed: 0:02:49.\n","  Batch   440  of    532.    Elapsed: 0:03:06.\n","  Batch   480  of    532.    Elapsed: 0:03:23.\n","  Batch   520  of    532.    Elapsed: 0:03:40.\n","\n","  Average training loss: 0.15\n","  Training epcoh took: 0:03:45\n","\n","Running Validation...\n","  Accuracy: 0.82\n","  Validation Loss: 0.52\n","  Validation took: 0:00:14\n","\n","Training complete!\n","Total training took 0:11:44 (h:mm:ss)\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","\n","# Display floats with two decimal places.\n","pd.set_option('precision', 2)\n","\n","# Create a DataFrame from our training statistics.\n","df_stats = pd.DataFrame(data=training_stats)\n","\n","# Use the 'epoch' as the row index.\n","df_stats = df_stats.set_index('epoch')\n","\n","# A hack to force the column headers to wrap.\n","#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n","\n","# Display the table.\n","df_stats"],"metadata":{"id":"MhezqjYGfuUD","colab":{"base_uri":"https://localhost:8080/","height":175},"executionInfo":{"status":"ok","timestamp":1652636356042,"user_tz":-120,"elapsed":237,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"16a6c8a5-72f3-4719-dbd2-203d7f92a93d"},"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n","epoch                                                                         \n","1               0.35         0.42           0.83       0:03:33         0:00:14\n","2               0.21         0.46           0.81       0:03:45         0:00:14\n","3               0.15         0.52           0.82       0:03:45         0:00:14"],"text/html":["\n","  <div id=\"df-71bd9d83-b350-4652-a2cb-b29e9833ee24\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Training Loss</th>\n","      <th>Valid. Loss</th>\n","      <th>Valid. Accur.</th>\n","      <th>Training Time</th>\n","      <th>Validation Time</th>\n","    </tr>\n","    <tr>\n","      <th>epoch</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>0.35</td>\n","      <td>0.42</td>\n","      <td>0.83</td>\n","      <td>0:03:33</td>\n","      <td>0:00:14</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.21</td>\n","      <td>0.46</td>\n","      <td>0.81</td>\n","      <td>0:03:45</td>\n","      <td>0:00:14</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.15</td>\n","      <td>0.52</td>\n","      <td>0.82</td>\n","      <td>0:03:45</td>\n","      <td>0:00:14</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-71bd9d83-b350-4652-a2cb-b29e9833ee24')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-71bd9d83-b350-4652-a2cb-b29e9833ee24 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-71bd9d83-b350-4652-a2cb-b29e9833ee24');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","% matplotlib inline\n","\n","import seaborn as sns\n","\n","# Use plot styling from seaborn.\n","sns.set(style='darkgrid')\n","\n","# Increase the plot size and font size.\n","sns.set(font_scale=1.5)\n","plt.rcParams[\"figure.figsize\"] = (12,6)\n","\n","# Plot the learning curve.\n","plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n","plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n","\n","# Label the plot.\n","plt.title(\"Training & Validation Loss\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.xticks([1, 2, 3, 4])\n","\n","plt.show()"],"metadata":{"id":"TEtunka8fuUD","colab":{"base_uri":"https://localhost:8080/","height":427},"executionInfo":{"status":"ok","timestamp":1652636371587,"user_tz":-120,"elapsed":742,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"95bb757b-37cb-4283-cf59-6452a108f7df"},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 864x432 with 1 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAvUAAAGaCAYAAACPCLyfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeVhTV/4/8HcCgRBWhQRQVrGARURwr7TWDVFxLS7VEWsdq61Lx36dqmMX6/wcOy5Vq9UZbe1CXVFUrNYNtR1bq2PtaK2orQuCIESWhD0Jub8/kGgMCFEwRN+v5+kzk5N7zz03cvWdw+eeKxIEQQAREREREVktsaUHQEREREREj4ahnoiIiIjIyjHUExERERFZOYZ6IiIiIiIrx1BPRERERGTlGOqJiIiIiKwcQz0RPfUyMzMREhKCVatWPXQfc+bMQUhISAOO6slV2+cdEhKCOXPm1KuPVatWISQkBJmZmQ0+vuTkZISEhODkyZMN3jcRUWOxtfQAiIjuZ044Tk1NhY+PTyOOxvqUlpbiX//6F/bt24fc3Fw0b94cHTp0wBtvvIGgoKB69TFjxgwcOHAAu3btQps2bWrcRhAE9O7dG2q1GsePH4dUKm3I02hUJ0+exKlTpzB+/Hi4uLhYejgmMjMz0bt3b4wdOxbvvfeepYdDRFaAoZ6ImpzFixcbvf7555+xdetWjBo1Ch06dDB6r3nz5o98vJYtW+LcuXOwsbF56D7+/ve/44MPPnjksTSEd955B3v37kVcXBw6d+4MpVKJI0eO4OzZs/UO9fHx8Thw4AB27NiBd955p8ZtfvrpJ9y8eROjRo1qkEB/7tw5iMWP5xfIp06dwurVqzFs2DCTUD9kyBAMHDgQEonksYyFiKghMNQTUZMzZMgQo9eVlZXYunUr2rdvb/Le/YqLi+Hk5GTW8UQiEezt7c0e572aSgAsKyvD/v37ER0djWXLlhnap02bBo1GU+9+oqOj4e3tjT179uDtt9+GnZ2dyTbJyckAqr4ANIRH/TNoKDY2No/0BY+IyBJYU09EVqtXr14YN24cLly4gIkTJ6JDhw4YPHgwgKpwv3z5cowYMQJdunRB27Zt0bdvXyxduhRlZWVG/dRU431v29GjR/HSSy8hPDwc0dHR+Oc//wmdTmfUR0019dVtRUVFeP/999GtWzeEh4dj9OjROHv2rMn5FBQUYO7cuejSpQsiIyORkJCACxcuYNy4cejVq1e9PhORSASRSFTjl4yagnltxGIxhg0bhsLCQhw5csTk/eLiYhw8eBDBwcFo166dWZ93bWqqqdfr9fj3v/+NXr16ITw8HHFxcUhJSalx/ytXrmD+/PkYOHAgIiMjERERgeHDhyMpKclouzlz5mD16tUAgN69eyMkJMToz7+2mvr8/Hx88MEH6NGjB9q2bYsePXrggw8+QEFBgdF21fufOHECn332Gfr06YO2bduiX79+2LlzZ70+C3NcvHgRU6dORZcuXRAeHo4BAwZg/fr1qKysNNouOzsbc+fORc+ePdG2bVt069YNo0ePNhqTXq/HF198gUGDBiEyMhJRUVHo168f/va3v0Gr1Tb42Imo4XCmnoisWlZWFsaPH4/Y2FjExMSgtLQUAJCTk4Pt27cjJiYGcXFxsLW1xalTp/Dpp58iLS0Nn332Wb36/+6777Bp0yaMHj0aL730ElJTU7Fhwwa4urpiypQp9epj4sSJaN68OaZOnYrCwkJ8/vnneO2115Cammr4rYJGo8GECROQlpaG4cOHIzw8HJcuXcKECRPg6upa789DKpVi6NCh2LFjB7755hvExcXVe9/7DR8+HGvXrkVycjJiY2ON3tu7dy/Ky8vx0ksvAWi4z/t+ixYtwldffYVOnTrhlVdeQV5eHhYsWABfX1+TbU+dOoXTp0/jxRdfhI+Pj+G3Fu+88w7y8/MxefJkAMCoUaNQXFyMQ4cOYe7cuWjWrBmAB9/LUVRUhJdffhnp6el46aWX8OyzzyItLQ2bN2/GTz/9hKSkJJPfEC1fvhzl5eUYNWoU7OzssHnzZsyZMwd+fn4mZWQP69dff8W4ceNga2uLsWPHwsPDA0ePHsXSpUtx8eJFw29rdDodJkyYgJycHIwZMwYBAQEoLi7GpUuXcPr0aQwbNgwAsHbtWnz88cfo2bMnRo8eDRsbG2RmZuLIkSPQaDRN5jdSRFQDgYioiduxY4cQHBws7Nixw6i9Z8+eQnBwsLBt2zaTfSoqKgSNRmPSvnz5ciE4OFg4e/asoS0jI0MIDg4WPv74Y5O2iIgIISMjw9Cu1+uFgQMHCt27dzfqd/bs2UJwcHCNbe+//75R+759+4Tg4GBh8+bNhravv/5aCA4OFtasWWO0bXV7z549Tc6lJkVFRcKkSZOEtm3bCs8++6ywd+/eeu1Xm4SEBKFNmzZCTk6OUfvIkSOFsLAwIS8vTxCER/+8BUEQgoODhdmzZxteX7lyRQgJCRESEhIEnU5naD9//rwQEhIiBAcHG/3ZlJSUmBy/srJS+NOf/iRERUUZje/jjz822b9a9c/bTz/9ZGj76KOPhODgYOHrr7822rb6z2f58uUm+w8ZMkSoqKgwtN+6dUsICwsTZs6caXLM+1V/Rh988MEDtxs1apTQpk0bIS0tzdCm1+uFGTNmCMHBwcKPP/4oCIIgpKWlCcHBwcK6dese2N/QoUOF/v371zk+Imp6WH5DRFbNzc0Nw4cPN2m3s7MzzCrqdDqoVCrk5+fjueeeA4Aay19q0rt3b6PVdUQiEbp06QKlUomSkpJ69fHKK68Yve7atSsAID093dB29OhR2NjYICEhwWjbESNGwNnZuV7H0ev1ePPNN3Hx4kV8++23eOGFFzBr1izs2bPHaLt3330XYWFh9aqxj4+PR2VlJXbt2mVou3LlCv73v/+hV69ehhuVG+rzvldqaioEQcCECROMatzDwsLQvXt3k+1lMpnh/1dUVKCgoACFhYXo3r07iouLcfXqVbPHUO3QoUNo3rw5Ro0aZdQ+atQoNG/eHIcPHzbZZ8yYMUYlT56enggMDMT169cfehz3ysvLwy+//IJevXohNDTU0C4SifD6668bxg3A8DN08uRJ5OXl1dqnk5MTcnJycPr06QYZIxE9Piy/ISKr5uvrW+tNjRs3bsSWLVvwxx9/QK/XG72nUqnq3f/93NzcAACFhYVwdHQ0u4/qco/CwkJDW2ZmJhQKhUl/dnZ28PHxgVqtrvM4qampOH78OJYsWQIfHx+sXLkS06ZNw9tvvw2dTmcosbh06RLCw8PrVWMfExMDFxcXJCcn47XXXgMA7NixAwAMpTfVGuLzvldGRgYAoFWrVibvBQUF4fjx40ZtJSUlWL16Nb799ltkZ2eb7FOfz7A2mZmZaNu2LWxtjf/ZtLW1RUBAAC5cuGCyT20/Ozdv3nzocdw/JgBo3bq1yXutWrWCWCw2fIYtW7bElClTsG7dOkRHR6NNmzbo2rUrYmNj0a5dO8N+b731FqZOnYqxY8dCoVCgc+fOePHFF9GvXz+z7skgosePoZ6IrJqDg0ON7Z9//jk+/PBDREdHIyEhAQqFAhKJBDk5OZgzZw4EQahX/w9aBeVR+6jv/vVVfWNnp06dAFR9IVi9ejVef/11zJ07FzqdDqGhoTh79iwWLlxYrz7t7e0RFxeHTZs24cyZM4iIiEBKSgq8vLzw/PPPG7ZrqM/7Ufzf//0fjh07hpEjR6JTp05wc3ODjY0NvvvuO3zxxRcmXzQa2+NanrO+Zs6cifj4eBw7dgynT5/G9u3b8dlnn+HPf/4z/vrXvwIAIiMjcejQIRw/fhwnT57EyZMn8c0332Dt2rXYtGmT4QstETU9DPVE9ETavXs3WrZsifXr1xuFq++//96Co6pdy5YtceLECZSUlBjN1mu1WmRmZtbrAUnV53nz5k14e3sDqAr2a9aswZQpU/Duu++iZcuWCA4OxtChQ+s9tvj4eGzatAnJyclQqVRQKpWYMmWK0efaGJ939Uz31atX4efnZ/TelStXjF6r1WocO3YMQ4YMwYIFC4ze+/HHH036FolEZo/l2rVr0Ol0RrP1Op0O169fr3FWvrFVl4X98ccfJu9dvXoVer3eZFy+vr4YN24cxo0bh4qKCkycOBGffvopXn31Vbi7uwMAHB0d0a9fP/Tr1w9A1W9gFixYgO3bt+PPf/5zI58VET2spjWNQETUQMRiMUQikdEMsU6nw/r16y04qtr16tULlZWV+Oqrr4zat23bhqKionr10aNHDwBVq67cWy9vb2+Pjz76CC4uLsjMzES/fv1MykgeJCwsDG3atMG+ffuwceNGiEQik7XpG+Pz7tWrF0QiET7//HOj5Rl/++03k6Be/UXi/t8I5ObmmixpCdytv69vWVCfPn2Qn59v0te2bduQn5+PPn361KufhuTu7o7IyEgcPXoUly9fNrQLgoB169YBAPr27QugavWe+5ektLe3N5Q2VX8O+fn5JscJCwsz2oaImibO1BPREyk2NhbLli3DpEmT0LdvXxQXF+Obb74xK8w+TiNGjMCWLVuwYsUK3Lhxw7Ck5f79++Hv72+yLn5Nunfvjvj4eGzfvh0DBw7EkCFD4OXlhYyMDOzevRtAVUD75JNPEBQUhP79+9d7fPHx8fj73/+O//znP+jcubPJDHBjfN5BQUEYO3Ysvv76a4wfPx4xMTHIy8vDxo0bERoaalTH7uTkhO7duyMlJQVSqRTh4eG4efMmtm7dCh8fH6P7FwAgIiICALB06VIMGjQI9vb2eOaZZxAcHFzjWP785z9j//79WLBgAS5cuIA2bdogLS0N27dvR2BgYKPNYJ8/fx5r1qwxabe1tcVrr72GefPmYdy4cRg7dizGjBkDuVyOo0eP4vjx44iLi0O3bt0AVJVmvfvuu4iJiUFgYCAcHR1x/vx5bN++HREREYZwP2DAALRv3x7t2rWDQqGAUqnEtm3bIJFIMHDgwEY5RyJqGE3zXzciokc0ceJECIKA7du3Y+HChZDL5ejfvz9eeuklDBgwwNLDM2FnZ4cvv/wSixcvRmpqKr799lu0a9cOX3zxBebNm4fy8vJ69bNw4UJ07twZW7ZswWeffQatVouWLVsiNjYWr776Kuzs7DBq1Cj89a9/hbOzM6Kjo+vV76BBg7B48WJUVFSY3CALNN7nPW/ePHh4eGDbtm1YvHgxAgIC8N577yE9Pd3k5tQlS5Zg2bJlOHLkCHbu3ImAgADMnDkTtra2mDt3rtG2HTp0wKxZs7Blyxa8++670Ol0mDZtWq2h3tnZGZs3b8bHH3+MI0eOIDk5Ge7u7hg9ejSmT59u9lOM6+vs2bM1rhxkZ2eH1157DeHh4diyZQs+/vhjbN68GaWlpfD19cWsWbPw6quvGrYPCQlB3759cerUKezZswd6vR7e3t6YPHmy0XavvvoqvvvuOyQmJqKoqAju7u6IiIjA5MmTjVbYIaKmRyQ8jruXiIjooVRWVqJr165o167dQz/AiYiInnysqSciaiJqmo3fsmUL1Gp1jeuyExERVWP5DRFRE/HOO+9Ao9EgMjISdnZ2+OWXX/DNN9/A398fI0eOtPTwiIioCWP5DRFRE7Fr1y5s3LgR169fR2lpKdzd3dGjRw+8+eab8PDwsPTwiIioCWOoJyIiIiKycqypJyIiIiKycgz1RERERERWjjfKmqmgoAR6fcNWLLm7OyEvr7hB+ySiKry+iBoPry+ixiEWi9CsmaNZ+zDUm0mvFxo81Ff3S0SNg9cXUePh9UXUNLD8hoiIiIjIyjHUExERERFZOYZ6IiIiIiIrx1BPRERERGTlGOqJiIiIiKwcV78hIiIiagBlZSUoLlahslJr6aFQE2ZjI4GTkyscHMxbsrIuDPVEREREj0ir1aCoqABubh6QSOwhEoksPSRqggRBgFZbgcLC27C1lUAisWuwvll+Q0RERPSIiooK4eTkCjs7KQM91UokEsHOTgpHR1cUFxc2aN8M9URERESPSKfTwN7ewdLDICshlTpAq9U0aJ8svyEiIiKznLp1BilX9qOwohBu9m4YHBSLzl5Rlh6WRen1lRCLbSw9DLISYrEN9PrKBu2ToZ6IiIjq7dStM9h0cQe0+qqbQQsqCrHp4g4AeOqDPctuqL4a42eF5TdERERUL5pKDZJ//8YQ6Ktp9VqkXNlvoVEREcCZeiIiIqqBIAjIK8/HVVU6rqtv4JoqHZnF2dAL+hq3L6ho2Jv+6OkxbdprAIDVq9c91n2fNAz1REREhIpKDW6oM3BNdQNX1em4rrqBIm0xAMDOxg4Bzr7o6/cifsg6iWJticn+zezdHveQqZFFR3es13ZJSSnw9m7RyKOhujDUExERPWUEQYCyLM8wA39NlY6bJbcMs/AKmQeedQ9BoKsfAl384e3oCZs7N4F6OSqMauoBQCKWYHBQrEXOhRrPu+8uMHq9bdtm5ORkY/r0t4za3dyaPdJxli//xCL7PmkY6omIiJ5w5boK3CjKwFXVDVxXp+Oa6oZhtt3exg4BLn6I8XsRga7+CHD1g5Ok9iddVt8My9Vvnnz9+g0wen3sWCpUqkKT9vuVl5dDKpXW+zgSieShxveo+z5pGOqJiIieIFWz8LeNymhuFmdDgAAA8JTJ0da9TdUsvGvVLLxYZN66GZ29otDZKwpyuTOUyqLGOA2yEtOmvYbi4mK8/fbfsGrVcly6dBFjxyZg4sTJ+M9/jiElZScuX74EtVoFuVyBAQMGYdy4CbCxsTHqA7hbF3/mzGnMmDEFCxcuxrVrV7Fr1w6o1SqEh0fgr3/9G3x8fBtkXwDYsWMbtmzZiLy82wgKCsK0aTOxfv1aoz6tBUM9ERGRFSvXleO6OuNuKY36Bkq0pQAAqY09Alz8EBvQq2oW3sUPjhKZhUdM9XXit1tI/u4K8tQVcHexx/AeQegW5mXpYZkoLCzA22/PRExMLGJjB8LTs2qM+/Z9AwcHGUaNGguZzAE//3wan376L5SUlGDq1Dfr7PfLLz+DWGyDMWMSUFSkxubNifjgg3ewfv2XDbLvzp3bsXz5YrRvH4VRo15GdnY25s6dBWdnZ8jliof/QCyEoZ6IiMhKCIKA3FIlrqpv4PqdAJ9VfMswC+8lU6CdRxgCXapm4b0cFWbPwlPTcOK3W/jy24vQ6Kruc8hTV+DLby8CQJML9rdvKzFnzruIixti1D5//v+Dvf3dMpyhQ+OxZMk/sHNnEiZNeh12dnYP7Fen02HDhi9ha1sVV11cXLFy5VJcvfoHWrVq/Uj7arVafPrpWoSFhWPFijWG7Vq3fgYLF85nqCciIqKGU6YrR7o6A9dU6biqTke6KgMluqpZeAdbKQJc/BAREIYAV38EuvhCxln4JueHX7Nx/Fy22ftdyVJBVykYtWl0eny+Lw3f/y/L7P6i23mje7i32fvVh1QqRWzsQJP2ewN9aWkJNBotIiIisXt3MtLTr+OZZ4If2O/AgYMNYRsAIiLaAwCysm7WGerr2vfixQtQqVR4441hRtv17RuLjz/+6IF9N1UM9URERE2AXtAjt1SJa6obuHbnZtbskhwIECCCCF6OCkTIwxDo6o9AV394yuSchX+C3R/o62q3JLlcYRSMq129egXr16/FmTP/RUmJ8TKoJSXFdfZbXcZTzdnZBQBQVFT3fRx17XvrVtUXrftr7G1tbeHt3ThffhobQz0REZEFlOnKcF2VYbiZ9Zr6Bsp0ZQAAB1sHBLr4ob0iHK1c/OHv4guZxMHCI6aH0T384WbI/7rmB+SpK0za3V3sMXts01pp6N4Z+WpFRUWYPv01yGROmDhxClq29IGdnR0uX76ItWtXQa+v+SFm9xKLbWpsF4S6v9g8yr7WyqKhXqPRYOXKldi9ezfUajVCQ0Mxc+ZMdOvW7YH7rVq1CqtXrzZp9/DwwA8//GDSnpSUhA0bNiAzMxMtWrRAQkICxo4d22DnQURE9CB6QY+cUqVhTfhr6hu4VZJrmIX3dvRElCIcAS7+aOXqBwVn4Z96w3sEGdXUA4CdrRjDewRZcFT198svP0OlUmHhwiVo3/7ul5DsbPNLhxqDl1fVF63MzAxEREQa2nU6HbKzsxEU9ODynqbIoqF+zpw5OHjwIBISEuDv74+dO3di0qRJSExMRGRkZJ37L1iwwGgd1JrWRN2yZQvef/99xMbGYsKECTh9+jQWLFiAiooKvPrqqw16PkRERABQqi3F9Tu18NfUN3BdfQNlunIAgMzWAQGufuigiECgqz/8XXzgYMtZeDJWfTOsNax+UxOxuOpL6b0z41qtFjt3JllqSEZCQ5+Fq6srUlJ2ol+/AYbyoUOH9qOoSG3h0T0ci4X6c+fOYe/evZg7dy5eeeUVAMDQoUMRFxeHpUuXYuPGjXX20b9/f7i4uNT6fnl5OZYvX47evXtj5cqVAICRI0dCr9dj9erVGDFiBJydnRvkfIiI6OmkF/S4VZJrCPDXVOm4VZoLABBBhBZOXoi6E+BbuVTNwotEIguPmqxBtzAvqwnx9wsPbwdnZxcsXDgf8fGjIBKJcODAPjSV6heJRIJXX30Ny5cvwV/+8gZ69uyN7OxsfPvtHrRs6WOV16jFQv3+/fshkUgwYsQIQ5u9vT3i4+OxfPly5ObmQqF48HJCgiCguLgYjo6ONX74J0+eRGFhIcaMGWPUPnbsWOzZswfff/89Bg40vVubiIioNiXa0rtrwqtu4Lo6A+WVVbPwjhIZAl380MkrEgEufghw8YXUtv5P1iR6Uri6umHx4uVYvXoF1q9fC2dnF8TE9EfHjp3x1lvTLD08AMBLL42CIAjYsmUjPvlkJYKCnsGHH36EFSuWws7O3tLDM5vFQn1aWhoCAwPh6Gj8KOp27dpBEASkpaXVGepffPFFlJaWwtHREf369cPs2bPh5uZmeP/ChQsAgLZt2xrtFxYWBrFYjAsXLjDUExFRrfSCHtklOYYAf02djpxSJYC7s/AdvdqjlYs/Al39IHfwsMoZPqL6WLRomUnbg566Gh4egX//+3OT9uPHTz+wj6iojibbAIC3d4sG3RcA4uNHIz5+tOG1Xq9HdnYWgoNDajijps1ioV6pVMLT09OkXS6XAwByc3Nr3dfFxQXjxo1DREQEJBIJfvrpJ2zduhUXLlxAUlKS4WEGSqUSdnZ2RkEfgKHtQccgIqKnT7G2pGolmjulNOnqDJRXVq1A4iRxRKCrHzp7dUArVz/4OftCamt9s3lEVKWiogL29sbX8P79e6FWqxAZ2cFCo3p4Fgv15eXlkEgkJu3VH25FhekyTtXGjx9v9Do2NhbPPPMMFixYgF27dmHkyJEPPEb1cR50jNq4uzuZvU99yOWs7SdqLLy+qCZ6vR43VFm4nHcVl/Ou4ve8a8guqprsEYvE8HdtiRcCuyDYvRWC3QPh6cRa+Jrw+qqSmyuGrS1XLLImZ86cwyefrETPnr3h6uqKS5cuYs+e3QgKao2+fWMa/c9TLBY36PVjsVAvlUqh1WpN2quD9v3fnOry8ssvY8mSJThx4oQh1EulUmg0mhq3r+nbWX3k5RVDr2/YuzzkcmcolXU/SIGIzMfri6oVa0oMD3W6pkrH9aIMaCqr/o2omoX3R2d5BwS6+sHPxRf2Nvc8wr4cuF1e98Nynja8vu7S6/XQ6epee52aDk9Pb7i7y7Ft2xao1Sq4uLgiNnYgpkyZBpHIptH/PPV6fa3Xj1gsMnsi2WKhXi6X11j+olRW1SrWVU9/P7FYDE9PT6hUKqNjaLVaFBYWGpXgaDQaFBYWmn0MIiKyDpX6SmSV3DJakUZZlgegahbex8kbXb06ItDVD61c/eEubc5ZeKKnTMuWPli8eLmlh9FgLBbqQ0NDkZiYiJKSEqObZc+ePWt43xxarRbZ2dlGN8W2adMGAHD+/HlER0cb2s+fPw+9Xm94n4iIrFuRptgowKerM6DRV/022NnOCa1c/NG9RRcEuPjB38UHdvfOwhMRPQEsFupjY2OxYcMGJCUlGdap12g0SE5ORlRUlOEm2qysLJSVlSEo6O4T1PLz89G8eXOj/j777DNUVFTg+eefN7R17doVbm5u2LRpk1Go37x5M2QyGV544YVGPEMiImoMlfpK3CzONgT4a6p03C7PB1A9C98C3Vp0RisXPwS4+sNd2oyz8ET0xLNYqI+IiEBsbCyWLl0KpVIJPz8/7Ny5E1lZWVi0aJFhu9mzZ+PUqVO4dOmSoa1nz54YMGAAgoODYWdnh5MnT+LAgQPo0KED4uLiDNtJpVLMmDEDCxYswJtvvono6GicPn0aKSkpmDVr1gMfXEVERE2DWlNktKRkujoT2juz8K52zgh09Ud0y64IdPWHn7MP7GxqXiCBiOhJZrFQDwCLFy/GihUrsHv3bqhUKoSEhGDdunXo0OHBywgNGjQIZ86cwf79+6HVatGyZUu88cYbmDx5suExv9XGjh0LiUSCDRs2IDU1Fd7e3pg3bx4SEhIa89SIiOghVOorkVmcZQjw11Q3kHdnFt5GZAMf5xaIbtEFAa5+CHTxR3OpG2fhiYgAiAShqTyw1zpw9Rsi68Lrq2lTVaiNymhuFGVCq9cBAFztXBDoWvVQp0AXf/g6t+QsfBPD6+uuW7fS4eXlb+lhkBV50M+MVa1+Q0RETxedXnd3Fv7OTa355QUAAFuRDXydW1aV0bj4o5WrP9zsXTkLT0RUTwz1RETUKAorVEYBPuOeWXg3e1cEuvqjp093BLj6w9epBSSchSciemgM9URE9Mi0eh0yi27eU0pzAwUVhQCqZ+F98HzLblXlNC5+aCZ1q6NHInrS7Nu3B//4xwdISkqBt3cLAEB8/CBERnbAvHnzzd73UZ05cxozZkzBxx//C1FRHRukT0tiqCciIrMVlBcaBfiM4pvQ3ZmFb2bvhkBXP/RyfR6BLv7wcW4BiZj/3BBZm7ffnokzZ/6LPXsOwcHBocZt3nprGn777VekpByEvb39Yx5h/Rw+fAD5+XkYOXKMpYfSqPi3LBERPZBWr0NG0U2jhzsVVlQ9vdtWbAs/Zx/08HkOgS5VN7W62btaeMRE1BD69u2HHwWUdP4AACAASURBVH/8D44f/w59+8aavF9QkI+ff/4vYmL6P3Sg37RpB8Ri8aMO9YFSUw/i998vm4T69u2jkJr6AySSJ6P0j6GeiIgMBEFAQUWh0ZKSmUU3oRMqAQDNpc0Q5BpgWJXGx6kFbDkLT/REev75F+HgIMPhwwdqDPVHjhxGZWUlYmJM36svOzvLPd1ZLBY32d8uPAz+TUxE9BTTVmpxo+imIcBfU6VDpVEDACR3ZuFf9I021MK72vOhfURPC6lUiuef74GjRw9DrVabPLTz8OEDcHd3h6+vP5Yu/RA//3wKOTk5kEqliIrqiKlT36yz/r2mmvqrV69gxYolOH/+V7i6umLIkOHw8JCb7Puf/xxDSspOXL58CWq1CnK5AgMGDMK4cRNgY2MDAJg27TX8739nAADR0VV1815e3ti+fU+tNfWpqQfx9ddfID39OmQyR3Tv/jxef30G3Nzu3gs0bdprKC4uxnvvLcBHHy1GWtpvcHZ2wYgRozF27HjzPugGwlBPRPSUEAQB+eWFdwJ8VSlNZlEWKu/MwrtLm+OZZq0MZTQ+Ti1gI7ax8KiJnl6nbp1BypX9KKgoRDN7NwwOikVnr6jHOoa+fWNx8OC3OHYsFYMHDzO037qVjfPnzyE+fjTS0n7D+fPn0KdPP8jlCmRnZ2HXrh2YPn0yvv46CVKptN7Hy8u7jRkzpkCv1+NPfxoPqdQBKSk7a5xR37fvGzg4yDBq1FjIZA74+efT+PTTf6GkpARTp74JABg//lWUlZUhJycb06e/BQBwcJDVevzqG3LDwsLx+uszkJubgx07tiIt7TesX/+V0TjUahX+7/9moGfP3ujdOwZHjx7G2rWr0KpVa3Tr1r3e59xQGOqJiJ5QmkotbhRlGgL8dVU6VJqqBwVJxBL4u/igl+/zCHT1R4CLH1ztnS08YiKqdurWGWy6uANavRYAUFBRiE0XdwDAYw32nTp1gZtbMxw+fMAo1B8+fACCIKBv334ICmqNnj37GO3XvfsLmDJlAo4dS0Vs7MB6H2/jxi+hUhXi008TERISCgDo3z8OL788zGTb+fP/H+zt735hGDo0HkuW/AM7dyZh0qTXYWdnh06duiI5OQkqVSH69RvwwGPrdDqsXbsKrVsHY9WqfxtKg0JCQjF//jzs2bMT8fGjDdvn5ubg/ff/n6E0KS5uCOLj47B3726GeiIiejiCICCvvMDoZtbM4izoBT0AwMPBHcHNWhtq4Vs6enMWnugxOJn9M05k/9fs/a6pbkAn6IzatHotNqZtx49Zp8zur5t3J3Tx7mD2fra2tujVqw927dqB27dvw8PDAwBw+PBB+Pj44tln2xptr9PpUFJSDB8fXzg5OePy5YtmhfoTJ35AeHiEIdADQLNmzdC3b3/s3JlktO29gb60tAQajRYREZHYvTsZ6enX8cwzwWad68WLF1BQkG/4QlCtV6+++OSTlfjxxx+MQr2TkxP69OlneC2RSNCmTRiysm6addyGwlBPRGSFNJUapKszcU2djuuqG7iqTkeRphgAYCeWwN/FF338eiDQxQ+Brv5wtjPvceNEZFn3B/q62htT376xSE5OwpEjBzFy5Bhcv34Nf/xxGRMmTAIAVFSUIzHxC+zbtwdKZS4EQTDsW1xcbNaxcnJuITw8wqTdz8/fpO3q1StYv34tzpz5L0pKSozeKykx77hAVUlRTccSi8Xw8fFFTk62UbtC4Wny1GtnZxdcufKH2cduCAz1RERNnCAIuF2Wf/dmVnU6bhZnG2bh5Q7uaNM82FAL38LRi7PwRE1EF+8ODzVD/s4P/zA8wO1ezezd8JeoKQ0xtHoLD4+At3dLHDq0HyNHjsGhQ/sBwFB2snz5EuzbtwcjRryMtm3D4eTkBECE+fP/ZhTwG1JRURGmT38NMpkTJk6cgpYtfWBnZ4fLly9i7dpV0Ov1jXLce4lr+Xu2sc65Lgz1RERNTEWlBunqDMMM/HXVDRRp78zC29ghwNkXff1eRKCrHwJd/OFk52jhERNRQxscFGtUUw9U3QszOOjhl498FH36xCAx8XNkZmYgNfUgQkLaGGa0q+vmp0+fadi+oqLC7Fl6APD09EJmZoZJ+40b6Uavf/nlZ6hUKixcuATt29+9xyA7O6uGXkU1tJny8vI2HOvePgVBQGZmBgIDg+rVj6Uw1BMRWZAgCFCW5RndzHqz5JZhFl4h88Cz7iGGAN/CyQtiUeM+qIWILK/6ZlhLr35TLSamPxITP8fq1cuRmZlhFOBrmrHesWMrKisrzT5Ot27dkZS0BZcuXTTU1RcUFODQoW+Ntqt+YNW9s+Jardak7h4AHBwc6vUFIzT0WTRr1hy7dm1H//5xhodSHT2aCqUyF2PHJph9Po8TQz0R0WNUrqvAjaIMXL2zJvx19Q0Ua6tqQe1t7BDg4ocY/54IdPFDgKsfnCSchSd6WnX2irJYiL9fYGArtG4djOPHv4dYLEbv3ndvEH3uuWgcOLAPjo5OCAgIxG+//YrTp0/B1dX8p0uPGTMeBw7sw1tvTUV8/GjY20uRkrITnp7eKC7+3bBdeHg7ODu7YOHC+YiPHwWRSIQDB/ahpsqXkJBQHDz4LVat+gihoc/CwUGG6OgXTLaztbXF669Pxz/+8QGmT5+MPn1ikJubg+3bt6JVqyAMGmS6Ak9TwlBPRNRIBEFAbtltoxVpsopvQUDVvzqeMjnaurepmoV39Ye3oydn4YmoyYqJicUff1xGZGQHwyo4APDmm7MgFotx6NC3qKjQIDw8AitWfIK33ppu9jE8PDzw8cf/xvLli5GY+IXRw6c+/PDvhu1cXd2wePFyrF69AuvXr4WzswtiYvqjY8fOeOutaUZ9DhnyEi5fvoh9+77B1q2b4OXlXWOoB4ABAwbBzs4OGzd+iU8+WQlHR0f07RuLKVOmN/mnz4oES1XzW6m8vGLo9Q37kcnlzlAqixq0TyKq8jivr3JdOa6rM3BNdQPX1VVBvkRbCgCQ2kgR4OJrCPABLn5wlNT+ABQia8B/v+66dSsdXl6mK7QQ1eZBPzNisQju7uatWsaZeiKihyAIAnJLlbiqvltGc+8svJdMgXYeYYZaeC9HBWfhiYio0TDUExHVQ5muHNfVN4xWpCnVlQEAHGylCHDxQ0RAmGEWXiZxsPCIiYjoacJQT0R0H72gr5qFv+dm1uySHAgQIIIIXo4KtJe3vfN0Vn94yuSchSciIotiqCeip16ptgzp6gxcVaffCfEZKDPMwjsg0MUPkYpwBLr4I8DVFw62nIUnIqKmhaGeiJ5Ip26dQcqV/SisKITbPes76wU9bpXk4vqdWvir6hvIKck1zMJ7O3oi6k6AD3T1h0LmwVl4IiJq8hjqieiJc+rWGaMnMRZUFCIxbRsOpR9DQUUhynTlAABHWxkCXP3QURGBQFd/+Lv4wsFWasmhExERPRSGeiKyenpBj4LyQuSW3kZOmRIpV/YbPVq9epuc0lx08+6EAFd/tHLxg0Imh0hUv8eHExERNWUM9URkFQRBQJG2GLmlt5FbqjT8b07Zbdwuy4NOr6uzj0pBj5dDX3oMoyWip5EgCJwooHppjMdEMdQTUZNSpiu7E9jvhPey24bX5ZXlhu1sRDaQO7hDIZOjrXsoFDIPKBzkUMjkWHJ6FQoqCk36bmbv9jhPhYieIjY2ttBqNbCza9pPHaWmQavVwMamYWO4RUO9RqPBypUrsXv3bqjVaoSGhmLmzJno1q2bWf1MmjQJ33//PRISEjBv3jyj90JCQmrcZ/78+Xj55ZcfeuxE9PC0eh1ul+UZz7iX3kZumRJFmmLDdiKI0FzqBoVMji7eUXdCuwcUMjmaS91qvYF1cFCsUU09AEjEEgwOim30cyOip5OTkxsKC5Vwc5NDIrHjjD3VSBAEaLUaFBYq4ezcrEH7tmionzNnDg4ePIiEhAT4+/tj586dmDRpEhITExEZGVmvPo4dO4bTp08/cJvo6GgMHjzYqC0iIuKhx01EddMLeuSXF94N7mV3A3x+eaHhyasA4GznBIWDHOHubSC/E9oVDh6QO7hDYiMx+9idvaIAoMbVb4iIGoODgyMAQKW6jcrKussB6ellY2MLZ+dmhp+ZhmKxUH/u3Dns3bsXc+fOxSuvvAIAGDp0KOLi4rB06VJs3Lixzj40Gg0WLVqEiRMnYtWqVbVu16pVKwwZMqShhk5EdwiCALWm+E6ZTFVoV5berqpzL70NnVBp2FZqYw+FzAOBrv7o4tWhKrjLPKCQeTTKuu+dvaLQ2SsKcrkzlMqiBu+fiOh+Dg6ODR7UiOrLYqF+//79kEgkGDFihKHN3t4e8fHxWL58OXJzc6FQKB7Yx1dffYXy8vI6Qz0AlJeXQyQSwd6etW5E5qq5zl15p869wrCdrcgGHjIPeDp4INy9jaFURu7gARc7J/46moiIqJFYLNSnpaUhMDAQjo7G32jbtWsHQRCQlpb2wFCvVCqxZs0avPfee3BwePAs3/bt25GYmAhBEBAcHIwZM2agb9++DXIeRE8KbaUWyrI8o8Be/b9F2vvr3JtBIfNAF+8AKBw86lXnTkRERI3HYqFeqVTC09PTpF0ulwMAcnNzH7j/Rx99hMDAwDrLaiIjIzFgwAD4+PggOzsbX331FaZNm4Zly5YhLi7u4U+AyApV1bkXVN2UeiewK8vqqHP3aHNPqYwcHtLmD1XnTkRERI3HYqG+vLwcEolpMKguj6moqDB5r9q5c+ewa9cuJCYm1vnr/C1bthi9HjZsGOLi4rBkyRIMHDjQ7HIAd3cns7avL7ncuVH6paePIAgoLFcjuygH2UW5yC7ORVZRLrKLcpBTfNtoPXcHiRQtnDzRRtEa3s4KeDt7ooWzAl7OCsgkDV/nbim8vogaD68voqbBYqFeKpVCq9WatFeH+dpq3wVBwMKFCxETE4OOHTuafVyZTIbRo0dj2bJluHr1KoKCgszaPy+vGHp9wz4wgDfy0cMo1ZZBWXYbOfeWytyZda+o1Bi2sxXbGtZzf9Yt1OgGVWdJDXXulUBJoQ4leDJ+Jnl9ETUeXl9EjUMsFpk9kWyxUC+Xy2sssVEqlQBQaz39oUOHcO7cOcycOROZmZlG7xUXFyMzMxMeHh6QSqW1Htvb2xsAoFKpHnb4RI+FplJrtJ57zj0rzNxf5+4ubQa5zAOtvAOgkHnA886a7s1Y505ERPTEs1ioDw0NRWJiIkpKSoxulj179qzh/ZpkZWVBr9dj/PjxJu8lJycjOTkZ69evxwsvvFDrsTMyMgAAzZs3f5RTIGoQlfrKqvXcDeu43511L7ivzt3FzhkKmQfCPZ411Lh7yjzg7uAOiZgPiCYiInpaWSwFxMbGYsOGDUhKSjKsU6/RaJCcnIyoqCjDTbRZWVkoKyszlMn06tULPj4+Jv1NnToVPXv2RHx8PMLCwgAA+fn5JsG9oKAAmzZtgo+PDwICAhrvBInuUbWee5HJjHtu6W3cLstDpdF67lJ4yuQIcg2AwvueBzHJPOBgW/tvoIiIiOjpZbFQHxERgdjYWCxduhRKpRJ+fn7YuXMnsrKysGjRIsN2s2fPxqlTp3Dp0iUAgJ+fH/z8/Grs09fXF3369DG83rhxI1JTU/Hiiy+iRYsWyMnJwdatW5Gfn49PPvmkcU+Qnkql2jKjJ6feO+t+f527wsEDXo4KtPN41lDn7imTw0niyPXciYiIyCwW/X394sWLsWLFCuzevRsqlQohISFYt24dOnTo0CD9R0ZG4syZM0hKSoJKpYJMJkP79u0xefLkBjsGPX00ldo7y0BWPz31bngv1pYYtquuc1fI5AhyC7x7g6qDHM2krqxzJyIiogYjEgShYZdyecJx9Zung2md+52ymVIlCitURnXurnbORuu4Vz+MiXXuTQOvL6LGw+uLqHFY1eo3RJYmCAJUGrVxqcydEH+7LN+ozt3BVgqFTI7WboF3w7vMAwoHD0hZ505EREQWxlBPT7xSbendJ6iWVZXM5JYqkVN2G5oa6ty9HT0RIW97Z8a9Kryzzp2IiIiaMoZ6eiLcW+d+/6y7SZ27Q3MoZB5o7dbKaNbdzZ517kRERGSdGOrJalTqK5FXXnDPk1PvBviCikKjbV3tXKCQeVTNuN8pk1HI5PBwaA5b1rkTERHRE4bphpqUu3XuyrslM6W3oSy7DWVZHvSC3rCtg62DYcbdU+ZhmHWXO7izzp2IiIieKgz1ZBEl2lKjNdzvlswY17lLxLaQO3jA29Hrzqx71RNUFQ5yOEpkrHMnIiIiAkM9NSJNpQbKsjzklCpNat1LtKWG7cQisWE992eatTK6QZV17kRERER1Y6inR1JV555vCO05htVlTOvc3exdIXdwR3t5uOHpqQoHD7izzp2IiIjokTBJUZ0EQUBhhcpQHnPvjPvtsnyTOndPw4y7/L46d3sLngURERHRk4uhngyq6tyVRrPuuaVKKEtvQ6PXGraTiCVQyDzQ0tEbkfJ2d4I769yJiIiILIWh/ilTUampKo+5d8b9zs2qtdW5BzcLMsy6e8rkcLV3YZ07ERERURPCUP8EqtRX4nZ5vmGWPeeeNd0LK1RG27rZu0Lh4IFIebjh5lSFTA4PaXPYiG0sdAZEREREZA6GeiulF/RQVaiNnpxaPfN+u9y4zl12p849pFnre2rcPVjnTkRERPSEYKi3oFO3ziDlyn4UVhTCzd4Ng4Ni0dkrymibYm2J8XKQd0plaq1zd/JGpKKdIbwrZB5wkjg+7lMjIiIioseIod5CTt06g00Xd0B7J5gXVBTi67QknFP+BjsbO0OIL9EZ17l7SJtDIfO4O+t+p9adde5ERERETy+GegtJubLfEOirVQqV+EX5a1Wdu0yOSM928LznQUzurHMnIiIiohow1FvI/Q9mutfC7vMe40iIiIiIyNqxXsNCmtm7mdVORERERFQbhnoLGRwUC4lYYtQmEUswOCjWQiMiIiIiImvF8hsLqV7lpq7Vb4iIiIiI6sJQb0GdvaLQ2SsKcrkzlMoiSw+HiIiIiKwUy2+IiIiIiKwcQz0RERERkZVjqCciIiIisnIM9UREREREVs6ioV6j0WDJkiWIjo5Gu3btMHLkSJw4ccLsfiZNmoSQkBAsXLiwxveTkpLQv39/hIeHo1+/fti4ceOjDp2IiIiIqMmwaKifM2cOvvzySwwePBjz5s2DWCzGpEmT8Msvv9S7j2PHjuH06dO1vr9lyxa88847CA4OxrvvvouIiAgsWLAAGzZsaIhTICIiIiKyOIuF+nPnzmHv3r2YNWsW3n77bYwaNQpffvklvL29sXTp0nr1odFosGjRIkycOLHG98vLy7F8+XL07t0bK1euxMiRI7F48WIMGjQIq1evRlERl5EkIiIiIutnsVC/f/9+SCQSjBgxwtBmb2+P+Ph4/Pzzz8jNza2zj6+++grl5eW1hvqTJ0+isLAQY8aMMWofO3YsSkpK8P333z/aSRARERERNQEWC/VpaWkIDAyEo6OjUXu7du0gCALS0tIeuL9SqcSaNWswc+ZMODg41LjNhQsXAABt27Y1ag8LC4NYLDa8T0RERERkzSwW6pVKJRQKhUm7XC4HgDpn6j/66CMEBgZiyJAhDzyGnZ0d3NzcjNqr2+rz2wAiIiIioqbO1lIHLi8vh0QiMWm3t7cHAFRUVNS677lz57Br1y4kJiZCJBKZfYzq4zzoGLVxd3cye5/6kMudG6VfIuL1RdSYeH0RNQ0WC/VSqRRardakvTpoV4f7+wmCgIULFyImJgYdO3as8xgajabG9yoqKmo9xoPk5RVDrxfM3u9B5HJnKJW8aZeoMfD6Imo8vL6IGodYLDJ7Itli5TdyubzG8helUgkANZbmAMChQ4dw7tw5vPzyy8jMzDT8BwDFxcXIzMxEeXm54RharRaFhYVGfWg0GhQWFtZ6DCIiIiIia2KxUB8aGopr166hpKTEqP3s2bOG92uSlZUFvV6P8ePHo3fv3ob/ACA5ORm9e/fGqVOnAABt2rQBAJw/f96oj/Pnz0Ov1xveJyIiIiKyZhYrv4mNjcWGDRuQlJSEV155BUDVDHpycjKioqLg6ekJoCrEl5WVISgoCADQq1cv+Pj4mPQ3depU9OzZE/Hx8QgLCwMAdO3aFW5ubti0aROio6MN227evBkymQwvvPBCI58lEREREVHjs1ioj4iIQGxsLJYuXQqlUgk/Pz/s3LkTWVlZWLRokWG72bNn49SpU7h06RIAwM/PD35+fjX26evriz59+hheS6VSzJgxAwsWLMCbb76J6OhonD59GikpKZg1axZcXFwa9ySJiIiIiB4Di4V6AFi8eDFWrFiB3bt3Q6VSISQkBOvWrUOHDh0a7Bhjx46FRCLBhg0bkJqaCm9vb8ybNw8JCQkNdgwiIiIiIksSCYLQsEu5POG4+g2RdeH1RdR4eH0RNQ6rWv2GiIiIiIgaBkM9EREREZGVY6gnIiIiIrJyDPVERERERFaOoZ6IiIiIyMox1BMRERERWTmGeiIiIiIiK8dQT0RERERk5RjqiYiIiIisHEM9EREREZGVY6gnIiIiIrJyDPVERERERFaOoZ6IiIiIyMox1BMRERERWTmGeiIiIiIiK8dQT0RERERk5RjqiYiIiIisHEM9EREREZGVY6gnIiIiIrJyDPVERERERFaOoZ6IiIiIyMox1BMRERERWTmGeiIiIiIiK8dQT0RERERk5RjqiYiIiIisHEM9EREREZGVs7XkwTUaDVauXIndu3dDrVYjNDQUM2fORLdu3R64X0pKCrZv344rV65ApVJBoVCgS5cumDZtGlq2bGm0bUhISI19zJ8/Hy+//HKDnQsRERERkaVYNNTPmTMHBw8eREJCAvz9/bFz505MmjQJiYmJiIyMrHW/ixcvwtPTEz169ICrqyuysrKwbds2HDt2DCkpKZDL5UbbR0dHY/DgwUZtERERjXJORERERESPm8VC/blz57B3717MnTsXr7zyCgBg6NChiIuLw9KlS7Fx48Za93377bdN2nr37o3hw4cjJSUFEydONHqvVatWGDJkSIOOn4iIiIioqWiQmnqdTocDBw5g27ZtUCqV9dpn//79kEgkGDFihKHN3t4e8fHx+Pnnn5Gbm2vWGFq0aAEAUKvVNb5fXl6OiooKs/okIiIiIrIGZs/UL168GCdPnsSOHTsAAIIgYMKECTh9+jQEQYCbmxu2bdsGPz+/B/aTlpaGwMBAODo6GrW3a9cOgiAgLS0NCoXigX0UFhaisrISWVlZ+OSTTwCgxnr87du3IzExEYIgIDg4GDNmzEDfvn3NOW0iIiIioibL7Jn6//znP+jYsaPh9ZEjR/Df//4XEydOxLJlywAA69atq7MfpVJZY2ivroevz0x9v3798NxzzyE+Ph6//PIL3nvvPXTt2tVom8jISMycORNr1qzBe++9B41Gg2nTpuGbb76ps38iIiIiImtg9kz9rVu34O/vb3h99OhR+Pj4YNasWQCA33//HXv27Kmzn/LyckgkEpN2e3t7AKhXqczq1atRWlqKa9euISUlBSUlJSbbbNmyxej1sGHDEBcXhyVLlmDgwIEQiUR1Hude7u5OZm1fX3K5c6P0S0S8vogaE68voqbB7FCv1Wpha3t3t5MnT+K5554zvPb19a1XXb1UKoVWqzVprw7z1eH+QTp16gQA6NGjB3r37o1BgwZBJpPhT3/6U637yGQyjB49GsuWLcPVq1cRFBRU53HulZdXDL1eMGuf2pz47RaSv7uCfHUFmrvYY3iPIHQL82qQvomoilzuDKWyyNLDIHoi8foiahxiscjsiWSzy2+8vLzwyy+/AKialc/IyDCEawDIy8uDTCarsx+5XF5jiU31F4K66unv5+vri7CwsHr9lsDb2xsAoFKpzDpGQzrx2y18+e1F5KkrIADIU1fgy28v4sRvtyw2JiIiIiKyTmbP1A8cOBBr1qxBfn4+fv/9dzg5OaFHjx6G99PS0uq8SRYAQkNDkZiYiJKSEqObZc+ePWt431zl5eUoKyurc7uMjAwAQPPmzc0+RkNJ/u4KNDq9UZtGp0fyd1c4W09EREREZjF7pn7y5MkYNmwY/ve//0EkEuGf//wnXFxcAABFRUU4cuRInU+EBYDY2FhotVokJSUZ2jQaDZKTkxEVFQVPT08AQFZWFq5cuWK0b35+vkl/58+fx8WLFxEWFvbA7QoKCrBp0yb4+PggICCgXufcGPLUNd8zkKeugF5omPIeIiIiIno6mD1Tb2dnh3/84x81vufo6Ijjx49DKpXW2U9ERARiY2OxdOlSKJVK+Pn5YefOncjKysKiRYsM282ePRunTp3CpUuXDG09e/ZE//79ERwcDJlMhj/++AM7duyAo6Mj3njjDcN2GzduRGpqKl588UW0aNECOTk52Lp1K/Lz8w1LYFqKu4t9rcF+/ob/YnD3AESFyCE280ZeIiIiInr6NOgTZXU6HZyd638X/OLFi7FixQrs3r0bKpUKISEhWLduHTp06PDA/caMGYMTJ07g8OHDKC8vh1wuR2xsLN544w34+voatouMjMSZM2eQlJQElUoFmUyG9u3bY/LkyXUeo7EN7xGEL7+9aFSCY2crRve2Xki7UYg1u86jhYcjBj0XgE6hCojFDPdEREREVDORIJhX6/Hdd9/h3LlzmD59uqFt48aNWLZsGcrLy9G/f398+OGHNS5X+SR4HKvf6PUC/nsxF3t+vI6s2yXwai7DoOcC0PlZBWzEDfIQYKKnBlfnIGo8vL6IGsfDrH5jdqhPSEiAu7s7li9fDgC4cuUKBg8eDF9fX/j4+OCHH37A7Nmz8corr5g1EGvRkKG+Wm1/KeoFAWcuKZHyw3VkKouhaOaAuG4B6BrmCVsbhnui+mDoIGo8vL6IGsdjWdLy6tWraNu2reH1vn37o2SgRgAAIABJREFUYG9vj+3bt+PTTz/FgAEDsGvXLnO7pRqIRSJ0DFVg/qudMG14OKR2NtiwLw1/W/cTvj+bBV2lvu5OiIiIiOiJZ3ZNvUqlQrNmzQyvf/zxR3Tt2hVOTlXfJjp37ozvvvuu4UZIEItEiAqWI/IZD5y9koc9P1zDF99exJ4frmFAtwBEh3tDYsuZeyIiIqKnldlJsFmzZsjKygIAFBcX49dff0XHjh0N7+t0OlRWVjbcCMlAJBKhfWsPvJPQETNHRsDNyR6JBy5hzr9PIPXnTGh1/NyJiIiInkZmz9S3b98eW7ZsQevWrfH999+jsrISL7zwguH99PR0s58GS+YRiUQIb+WOtoHNcSG9ACnHr2Hjocv45sR19O/ijx7tW8BeYmPpYRIRERHRY2J2qJ8xYwYSEhLwl7/8BQAwbNgwtG7dGgAgCAIOHz6MLl26NOwoqUYikQhhAc3xrH8zXLpRiJQfrmFL6u/Y91M6Yjv7oWdkS9jbMdwTERERPenMXv0GAAoLC3HmzBk4OzujU6dOhnaVSoVdu3ahS5cuCA0NbdCBNhWPc/Wbh3E5oxB7friG364XwMlBgtguVeHewb5BH0lAZDW4OgdR4+H1RdQ4HsuSlk+7ph7qq/1xU4U9P1zHr1fz4Ci1RUxnP/SO8oFMynBPTxeGDqLGw+uLqHE81lB/48YNpKamIiMjAwDg6+uL3r17w8/P72G6sxrWEuqrXctWI+X4NZy9kgeZvS36dvJFn44+cJQ+mQ8HI7ofQwdR4+H1RdQ4HluoX7FiBdavX2+yyo1YLMbkyZPx5ptvmtul1bC2UF8t/VYRUn64hl9+vw0Hexv07uCLmE6+cHJguKcnG0MHUePh9UXUOB4m1Jtdi7F9+3b861//QmRkJP78/9u78/goy3vv45/JTvaFScieEEIiCUsAgRAEFNSUVVEeqwh1gVrRU5eHHmr7Ou059vSxp8UqtVoVtYoHpYBgWBRRUVHCIotgIGwh2xAgIZAEEpJMknn+CEwbA0gwdyaTfN//5Zr7nvs3vF4/5ps713Xds2eTmJgIwOHDh3n99dd5+eWXiY6OZtq0aW19azFQbC8//u2OARSdPMva7ALWZhfw8Y5ixg2O4pZh0fh7ezi6RBERERG5Rm2+Uz9t2jTc3d1ZsmQJbm4tfydoaGhgxowZWK1WVq5c2a6FdhbOeqf+uyxl51ibXcDXuaW4u7twU1oUtw6PIcBH4V66Ft1JFDGO+kvEGNdyp77ND5/Ky8tjwoQJrQI9gJubGxMmTCAvL6+tbysdLMrsy8+mpvLfc4YzpK+Zj74uYv7fsnn3k8OcOVvn6PJEREREpA3aPP3G3d2dmpqay75eXV2Nu7vmaTuL8BAf5kxOYUpGPGu3FPDpTguf7T7GmIER/GhEDMH+Xo4uUURERES+R5vv1Pfv359//OMfnDp1qtVr5eXlLFu2jIEDB7ZLcdJxwoK9eXBiP/7fQyMYmRrG598c45evbGHxRwc5VXne0eWJiIiIyBW0eU79119/zX333YePjw933HGH/WmyR44cYeXKlVRXV/Pmm28ydOhQQwp2tK4yp/77nKo8zwdbi/hyTwkAGf17MSE9jtDAHg6uTKRtOmN/iXQV6i8RY3TYlpYbN27kd7/7HcePH28xHhERwW9+8xvGjh3b1rd0Gt0l1F90uqqWD7cW8cWeEpqabKSnhjEpPY6wYG9HlyZyVTpzf4k4O/WXiDE69OFTTU1N5OTkYLFYgOaHT6WkpLBs2TIWL17MBx98cC1v2+l1t1B/0ZmzdazfVsTn3xyjobGJEf16MWlkLOEhPo4uTeSKnKG/RJyV+kvEGB2yT/0/L+bCgAEDGDBgQIvxM2fOkJ+ff61vK51UkJ8nd49PZMKIGD7aXszG3Ra27jvBsH5hTBoZR2RPhXsRERERR7nmUC/dU4CvJ//npj5kjohhw/ZiPt1lYfv+kwxJDmXyyDiiQ9v2W6WIiIiI/HAK9XJN/L09uHNsApnDY9jwdRGf7LCw40Apg/uamTwyjthefo4uUURERKTbUKiXH8S3hzvTRidw67AYPv66mI93WNh1qIxBfXoyOSOO+HB/R5coIiIi0uUp1Eu78PFy57YbenPL9TF8urOYDV8X87u3dtC/dwhTMuJIiAxwdIkiIiIiXdZVhfq///3vV/2Gu3btuuZixPl5e7kxOSOe8UOj2bjLwkfbi/n92ztJiQtickY8faMDHV2iiIiISJdzVVtaJicnt+1NTSZyc3OvuajOrLtuaXmtausb+Hx3Ceu3FVJVYyU5JpApGfEkxwY5ujTpJrpyf4k4mvpLxBiG7VO/ffv2NhczbNiwNp/jDBTqr02dtZEvvinhw62FVFbX0zc6kCkZcVwXG4TJZHJ0edKFdYf+EnEU9ZeIMTr04VPtob6+noULF5KVlUVVVRXJyck88cQTpKenX/G81atXs2LFCvLy8qisrCQ0NJThw4fz6KOPEhkZ2er45cuX88Ybb2CxWIiIiGDWrFnMmDHjmmpWqP9h6q2NfLn3OB9sLeTM2Tr6RAYwJSOOlPhghXsxRHfqL5GOpv4SMYbThfonn3ySDRs2MGvWLGJjY1m1ahU5OTm8/fbbpKWlXfa8P/7xj5SVlZGcnExAQAAlJSUsW7aMxsZGVq9ejdlsth+7dOlSfvvb35KZmUlGRgY7duwgKyuL+fPn88ADD7S5ZoX69mFtaOKrb4/zwZYCyqvqiA/3Z0pGHAMSQhTupV11x/4S6SjqLxFjOFWo37t3L9OnT+epp57ivvvuA6Curo5JkyYRGhrKkiVL2vR++/btY9q0afz7v/87Dz74IAC1tbWMGTOGIUOG8NJLL9mPnTdvHhs3buSLL77Az69t+6kr1LevhsYmsnNOsDa7gFOVtcSG+TElI45BiT0V7qVddOf+EjGa+kvEGNcS6l0MquV7rV+/Hnd3d6ZPn24f8/T05M4772Tnzp2Ulpa26f0iIiIAqKqqso9t27aNiooK7rnnnhbHzpgxg+rqajZt2vQDPoG0BzdXF0YPjOD//XQE909I5nxdAy+s/Jb//PvX7DhQSpPj/pAkIiIi4jQctk99bm4u8fHx+Pj4tBgfMGAANpuN3NxcQkNDr/geFRUVNDY2UlJSwosvvgjQYj7+/v37AUhNTW1xXkpKCi4uLuzfv5+JEye2x8eRH8jN1YUbBkQwMrUX2/afZE12IS+9n0NkTx8mZ8QxNCkUFxfduRcRERG5FIeF+rKyMsLCwlqNX5wPfzV36m+99VYqKioACAwM5De/+Q0jRoxocQ0PDw8CA1vujX5xrK1/DRDjubq4MDI1nBH9erH9wEnWbC7g5ax9hIfkM2lkHMOuC8XVxWF/YBIRERHplBwW6mtra3F3d2817unpCTTPr/8+f/3rX6mpqSE/P5/Vq1dTXV19Vde4eJ2rucZ3tXV+09Uym9s2t787mBzmz8Qb+pD9bQlLNxxk0Zr9rNtSyP8Z35exg6NwdVW4l6uj/hIxjvpLpHNwWKj38vLCarW2Gr8YtC+G+yu5/vrrARgzZgzjxo1j8uTJeHt7c++999qvUV9ff8lz6+rqruoa36WFsh0vKcKf//jJUHYfOsWazfk8v3Q3S9bnMik9jvTUXrgp3MsVqL9EjKP+EjGGUy2UNZvNl5z+UlZWBvC98+m/Kzo6mpSUFNasWdPiGlar1T5F56L6+noqKirafA1xHBeTiSFJZn57//X8/I4B+Hi58/cPD/DUK1v5/JtjWBuaHF2iiIiIiMM4LNQnJyeTn5/fasrMnj177K+3VW1tLWfP/vOOwXXXXQdATk5Oi+NycnJoamqyvy7Ow2QyMSixJ//xk6E8Pn0gAb4eLF5/kKde3cLGXRasDY2OLlFERESkwzks1GdmZmK1Wlm+fLl9rL6+npUrVzJ48GD7ItqSkhLy8vJanHv69OlW75eTk8OBAwdISUmxj40YMYLAwEDeeeedFse+++67eHt7M3r06Pb8SNKBTCYTAxJC+PXMIfzfuwYR7O/F/244xPyXt/DxjmLqrQr3IiIi0n04bE79wIEDyczMZMGCBZSVlRETE8OqVasoKSnhmWeesR83f/58tm/fzsGDB+1jN954Iz/60Y/o27cv3t7eHDlyhPfeew8fHx/mzp1rP87Ly4uf//znPP300zz22GOMGjWKHTt2sHr1aubNm4e/v3+HfmZpfyaTiZT4YPrFBXGgqILVX+Xz7ieHWbelkB8Nj2HsoEg8PVwdXaaIiIiIoRz2RFloXqz6/PPPs2bNGiorK0lKSuLJJ59k5MiR9mNmzpzZKtT/z//8D1u2bMFisVBbW4vZbGbEiBHMnTuX6OjoVtdZtmwZb7zxBhaLhfDwcGbOnMmsWbOuqWYtlO38DhadYfXmAnILz+Dn7U7msBhuHByJl4fDfocVB1J/iRhH/SVijGtZKOvQUO+MFOqdx2FLBWs2F5CTfxrfHu7ccn0044ZE0cNT4b47UX+JGEf9JWIMhfoOoFDvfPJKKlmzuYC9eeV4e7pxy/XRjB8ahbfXpZ9hIF2L+kvEOOovEWMo1HcAhXrnVXCiijWbC9h9+BQ9PF0ZPySam6+PxreHwn1Xpv4SMY76S8QYCvUdQKHe+RWdPMua7AJ2HizD08OV8UOiuOX6aPy8PRxdmhhA/SViHPWXiDEU6juAQn3XYSk7x9rsAr7OLcXD3ZWbBkdy67AY/H0U7rsS9ZeIcdRfIsZQqO8ACvVdT8mpatZuKWDb/pO4u7owNi2SzOExBPp6Oro0aQfqLxHjqL9EjKFQ3wEU6ruuE6drWJtdwNZ9J3F1NTFmYAQ/GhFLkJ/CvTNTf4kYR/0lYgyF+g6gUN/1nTxTw7othWzJOYHJBDcMjGDC8FhCArwcXZpcA/WXiHHUXyLGUKjvAAr13UdZxXk+2FrIV3uPAzBqQDgTR8TSM7CHgyuTtlB/iRhH/SViDIX6DqBQ3/2UV9bywbZCvtxTgs0G6am9mJQeS2iQt6NLk6ug/hIxjvpLxBgK9R1Aob77OnO2jg+3FvLFnhIaG22MSAlj0sg4egUr3Hdm6i8R46i/RIyhUN8BFOql4lwd67cV8fnuY1gbmxh+XRgTR8YR2dPH0aXJJai/RIyj/hIxhkJ9B1Col4uqquv5aHsRG3cdo97ayNDkUCaPjCMqtG1NKMZSf4kYR/0lYoxrCfVuBtUi0uX5+3gw/cY+ZA6PYcPXxXy608LXB0oZ0tfM5Iw4YsL8HF2iiIiIdBMK9SI/kJ+3B3eMSeDWYTF8sqOYj3dY2HmojEF9ejI5I474cH9HlygiIiJdnKbftJGm38j3qam18slOCx9/XUx1bQMDEkKYnBFHQkSAo0vrltRfIsZRf4kYQ3PqO4BCvVyt83UNbNxl4aPtxZw7byU1PpgpGfH0iVK470jqLxHjqL9EjKFQ3wEU6qWtausb+Gz3MdZvK+JsjZXrYoOYkhFHUkyQo0vrFtRfIsZRf4kYQ6G+AyjUy7Wqq2/k82+O8eG2Iqqq60mKDmRKRhzJsUGYTCZHl9dlqb9EjKP+EjGGQn0HUKiXH6re2sgXe0r4cGshFefq6RMVwJSMOFLighXuDaD+EjGO+kvEGAr1HUChXtqLtaGRL/ceZ92WQs6craN3hD9TMuLo3ztE4b4dqb9EjKP+EjGGQn0HUKiX9mZtaGJzznHWZRdSXlVLbC8/pmTEMahPT4X7dqD+EjGO+kvEGAr1HUChXozS0NjElpwTrN1SQFlFLTGhvkzOiCOtrxkXhftrpv4SMY76S8QYCvUdQKFejNbY1MTWfSdZm13AyTPniTT7MHlkHEOTQnFxUbhvK/WXiHHUXyLGUKjvAAr10lGammxszz3JmuwCjpfXEB7izeSRcQy7Lkzhvg3UXyLGUX+JGEOhvgMo1EtHa2qyseNgKWuyCzhWVk1YsDeT0mMZkRKGq4uLo8vr9NRfIsZRf4kYQ6G+AyjUi6M02WzsPlTG6s0FFJeeIzSwBxNHxpKe0gs3V4X7y1F/iRhH/SViDKcL9fX19SxcuJCsrCyqqqpITk7miSeeID09/YrnbdiwgQ8++IC9e/dSXl5OeHg4N954I3PnzsXPz6/FsUlJSZd8j//8z//k7rvvbnPNCvXiaDabjW+OnGL15gIKT5ylZ4AXE9NjyegfrnB/CeovEeOov0SM4XSh/sknn2TDhg3MmjWL2NhYVq1aRU5ODm+//TZpaWmXPW/48OGEhoYyfvx4IiIiOHjwIEuXLiUuLo733nsPT09P+7FJSUmMGjWKKVOmtHiPgQMHEhcX1+aaFeqls7DZbHx7tJysrwrIP15FsL8nE0fEMmpABO5uCvcXqb9EjKP+EjHGtYR6N4Nq+V579+5l3bp1PPXUU9x3330A3HbbbUyaNIkFCxawZMmSy577l7/8heHDh7cYS01NZf78+axbt45p06a1eK13795MnTq13T+DiCOZTCYGJPSkf+8Q9hWcZvVXBby94RBrtxTyo+ExjB4YgYe7q6PLFBERkQ7gsNt569evx93dnenTp9vHPD09ufPOO9m5cyelpaWXPfe7gR5g/PjxAOTl5V3ynNraWurq6n5g1SKdj8lkIjU+hKfuHcy8Hw/CHODFO58cZv7LW9iwvYg6a6OjSxQRERGDOSzU5+bmEh8fj4+PT4vxAQMGYLPZyM3NbdP7nTp1CoCgoKBWr61YsYJBgwYxYMAAJk+ezMcff3zthYt0UiaTiX5xwfzy3iHMvyeNiJ4+LN14hPl/y+bDbYXU1jc4ukQRERExiMOm35SVlREWFtZq3Gw2A1zxTv2lLFq0CFdXV2655ZYW42lpaUyYMIGoqCiOHz/O4sWLefTRR3n22WeZNGnStX8AkU4sKSaIX8QEcai4gjXZBSz/LI8PtxZx67BobhocRQ9Ph7W+iIiIGMBh3+y1tbW4u7u3Gr+4yLUtU2XWrFnDihUreOihh4iJiWnx2tKlS1v8fPvttzNp0iT+9Kc/MXHiREymtj3Ep62LFq6W2ez3/QeJtJHZ7EfG4GgOFJ7mHx8f4r0vjvLR9mKmjklg0qje+PZo3YNdkfpLxDjqL5HOwWGh3svLC6vV2mr8Ypj/1x1srmTHjh38+te/ZuzYsTz22GPfe7y3tzc//vGPefbZZzl69CgJCQltqlu734gzCvF2Z+7UFPKHRbNmcwFL1h9g5WdHuHloFDdfH42PV9cN9+ovEeOov0SM4VS735jN5ktOsSkrKwMgNDT0e9/jwIEDPPzwwyQlJfHcc8/h6np1O32Eh4cDUFlZ2YaKRZxffLg/P79zAIUnzrI2u4DVmwvY8HUx44dGccv1Md3mzr2IiEhX47CFssnJyeTn51NdXd1ifM+ePfbXr6SoqIjZs2cTHBzMK6+8gre391Vfu7i4GIDg4OA2Vi3SNcT28uORaf15+oFh9O8dwrrsQn7xUjbLPz9CVXW9o8sTERGRNnJYqM/MzMRqtbJ8+XL7WH19PStXrmTw4MH2RbQlJSWttqksKyvjgQcewGQy8frrr182nJ8+fbrV2JkzZ3jnnXeIioq6podPiXQlUaG+PHxbKk/PHk5aYk/Wbyvi31/O5h8bD1N5TlvAioiIOAuHTb8ZOHAgmZmZLFiwgLKyMmJiYli1ahUlJSU888wz9uPmz5/P9u3bOXjwoH1s9uzZFBcXM3v2bHbu3MnOnTvtr8XExNifRrtkyRI+/fRTxo4dS0REBCdPnuQf//gHp0+f5sUXX+y4DyvSyUX29OGnU1KYnBHHui2FbPi6mI27jjFmUAQ/Gh5LkN/VrXERERERx3DovnZ//OMfef7558nKyqKyspKkpCReffVVhgwZcsXzDhw4AMBrr73W6rXbb7/dHurT0tLYtWsXy5cvp7KyEm9vbwYNGsRDDz30vdcQ6Y7CQ3yYPalfc7jPLmTjzmN8vruE0QPDmTAilmB/L0eXKCIiIpdgstls7buVSxen3W+kOymrOM+6LYVs/vY4ADcMaA73PQN7OLiyq6f+EjGO+kvEGNey+41CfRsp1Et3dKryPB9uLeLLvSXYbDAytRcTR8YR6gThXv0lYhz1l4gxFOo7gEK9dGenq2r5cFsRX3xTQlOTjfSUMCaNjCMs+Op3n+po6i8R46i/RIyhUN8BFOpFoOJcHeu3FfH57mNYG5sY0a853IeH+Di6tFbUXyLGUX+JGEOhvgMo1Iv8U2V1PR9tL2LjLgtWaxPXXxfK5JFxRJrb9h+RkdRfIsZRf4kYQ6G+AyjUi7RWVVPPx18X88lOC3X1jQxNMjNpZBwxYX6OLk39JWIg9ZeIMa4l1Dt0S0sR6Rr8vT24Y0wCtw6LuRDui9lxsIy0xJ5MyYgntpfjw72IiEhXpjv1baQ79SLfr7rWyic7LHz8dTE1dQ0MTAhhckY8vSP8O7wW9ZeIcdRfIsbQ9JsOoFAvcvVqahv4dJeFDduLqK5tILV3MFMy4ukTGdBhNai/RIyj/hIxhkJ9B1CoF2m783UNfLb7GOu3FXHuvJV+cUFMyYinb3Sg4ddWf4kYR/0lYgyF+g6gUC9y7erqG5vD/fYiqqrrSY4JZHJGPMkxgZhMJkOuqf4SMY76S8QYCvUdQKFe5Ierszay6ZsSPthWSOW5ehKjApiSEU+/uKB2D/fqLxHjqL9EjKFQ3wEU6kXaj7WhkU17jvPB1kLOnK0jIcKfyRnx9O8d3G7hXv0lYhz1l4gxFOo7gEK9SPuzNjSx+dvjrNtSQHlVHXG9/JiSEc/APiE/ONyrv0SMo/4SMYZCfQdQqBcxTkNjE9k5J1ibXcCpylpiwnyZkhHPoMSeuFxjuFd/iRhH/SViDIX6DqBQL2K8hsYmtu47ydotBZSeOU+U2ZcpGXEMTjK3Odyrv0SMo/4SMYaeKCsiXYKbqwujBoSTnhrG9v2lrMku4KX3c4jo6cPkkXFcnxyKi4sxu+WIiIg4I92pbyPdqRfpeE1NNr4+0BzuS05V0yvYm8kj4xjWLxRXF5crnqv+EjGO+kvEGJp+0wEU6kUcp8lmY9fBMlZvLsBSdo7QoB5MSo9jREoYbq6XDvfqLxHjqL9EjKFQ3wEU6kUcr8lm45vDp1i9OZ+ik+foGeDFpJFxjEzt1Srcq79EjKP+EjGGQn0HUKgX6TxsNht78spZszmf/ONnCfH3ZEJ6HKP6h7PjYCkrv8jjdFUdwf6eTBuTQHpKL0eXLNKl6PtLxBgK9R1AoV6k87HZbOTkn2b1V/nklVTh7elKnbWJxn/pVQ83F37yo2QFe5F2pO8vEWNcS6i/8gozEREnYDKZ6N87hF/NHML//fEg6htaBnqA+oYmVn6R56AKRUREjKVQLyJdhslkIiUumIbGS/81rbyqjmWfHeGbw6c4d97awdWJiIgYR/vUi0iXE+LvSXlVXatxN1cTn+woZv22IgAizT4kRgXSNyqAvtGBBPt7dXSpIiIi7UKhXkS6nGljEnjrwwPUNzTZxy7OqR/S10z+8SoOWSo5XFzB1n0n+Hz3MQBC/L3oGx1AYlQgidGBRIR4Y2rjE2xFREQcQaFeRLqci4thL7f7TVJMEEkxQUDzg62KS89xyFLB4eIK9hWcYcu+kwD49nAnMepiyA8gNszvsvvhi4iIOJJDd7+pr69n4cKFZGVlUVVVRXJyMk888QTp6elXPG/Dhg188MEH7N27l/LycsLDw7nxxhuZO3cufn5+rY5fvnw5b7zxBhaLhYiICGbNmsWMGTOuqWbtfiPiXNraXzabjdKK8xwqruBwcSWHLBWUnjkPgIe7CwkRASRemK6TEBGAp4erUaWLdHr6/hIxhtNtafnkk0+yYcMGZs2aRWxsLKtWrSInJ4e3336btLS0y543fPhwQkNDGT9+PBERERw8eJClS5cSFxfHe++9h6enp/3YpUuX8tvf/pbMzEwyMjLYsWMHWVlZzJ8/nwceeKDNNSvUiziX9uivinN1HLFUcqi4gkOWCopLz2GzgYvJRGwv3+Z5+dGBJEYF4Oft0U6Vi3R++v4SMYZThfq9e/cyffp0nnrqKe677z4A6urqmDRpEqGhoSxZsuSy527bto3hw4e3GHv//feZP38+zzzzDNOmTQOgtraWMWPGMGTIEF566SX7sfPmzWPjxo188cUXl7yzfyUK9SLOxYj+Ol/XwJFjlRy2VHCouJKjJVU0NDbP3w8P8b4Q8gPoGxVISICX5uVLl6XvLxFjXEuod9ic+vXr1+Pu7s706dPtY56entx5550899xzlJaWEhoaeslzvxvoAcaPHw9AXt4/96Hetm0bFRUV3HPPPS2OnTFjBmvWrGHTpk1MnDixPT6OiHQjPTzd6N87hP69QwCwNjRRcKKqecqOpZIdB0rZtKcEgCA/T/t0nb5RgUSYfXBRyBcRkXbmsFCfm5tLfHw8Pj4+LcYHDBiAzWYjNzf3sqH+Uk6dOgVAUFCQfWz//v0ApKamtjg2JSUFFxcX9u/fr1AvIj+Yu5tL82LaqEAAmmw2jpVVXwj5FRwqrmB7bikAPl5u9IkMIPFCyI8L1+JbERH54RwW6svKyggLC2s1bjabASgtLW3T+y1atAhXV1duueWWFtfw8PAgMDCwxbEXx9p6DaDNfwq5WmZz26YBicjVc0R/hYX6MzglHGhefHvydA3788vZd/Q0+46Ws+fz5r8qeri50Dc2iH7xIaTEh5AcF4S3l3uH1ytyrfT9JdI5OCzU19bW4u7e+ovr4iLXurrWD465nDVr1rBixQoeeughYmJivvcaF6/TlmtcpDn1Is6ls/SXK9A/Noj+sUFwYwJVNfUcLq6038lf8elhltkOYTJBTKgfiRfm5CdGBxLgo8W30jl1lv4S6Wqcak69l5cXVmvrx7RfDNr/uoPNlezYsYNf//rXjB07lscee6zVNerr6y95Xl1d3VVfQ0Skvfl7ezAkycyQpObq+i9KAAAX8klEQVS/TtbWN5BXUsXh4uaQv+mbEj7ZYQEgLKiHfbpOYnQAoYE9tPhWRERacFioN5vNl5z+UlZWBnBV8+kPHDjAww8/TFJSEs899xyuri33izabzVitVioqKlpMwamvr6eioqJNc/ZFRIzk5eFGSlwwKXHBADQ0NlF48mzzXvnFFew+VMZXe48DEODr0bzDzoUFuFFmX1xcFPJFRLozh4X65ORk3n77baqrq1sslt2zZ4/99SspKipi9uzZBAcH88orr+Dt7d3qmOuuuw6AnJwcRo0aZR/PycmhqanJ/rqISGfj5tr8oKuEiAAyh8fQZLNx/FQ1hy2V9qff7jjQfGOkh6crCZHN03X6RgcSH+6Hu5seiiUi0p04LNRnZmbyxhtvsHz5cvs+9fX19axcuZLBgwfbF9GWlJRw/vx5EhIS7OeWlZXxwAMPYDKZeP311wkODr7kNUaMGEFgYCDvvPNOi1D/7rvv4u3tzejRo437gCIi7cjFZCLS7Euk2ZexaZEAlFfWNgd8SyWHiytYuekoAG6uJuLC/S+E/AD6RAZo8a2ISBfn0CfKPvbYY3z66af85Cc/ISYmxv5E2bfeeoshQ4YAMHPmTLZv387Bgwft502dOpUDBw4we/Zs+vbt2+I9Y2JiWjyNdsmSJTz99NNkZmYyatQoduzYwfvvv8+8efOYM2dOm2vWQlkR59Kd+uvceSuHLRX2BbgFJ87S2GTDBESF+tr3y0+MCiTIT2uK5IfrTv0l0pGc6omy0LxY9fnnn2fNmjVUVlaSlJTEk08+yciRI+3HXCrUJyUlXfY9b7/9dv7whz+0GFu2bBlvvPEGFouF8PBwZs6cyaxZs66pZoV6EefSnfurztrI0YuLby0V5B2ros7aCIA50Mu+u05iVAC9gr21+FbarDv3l4iRnC7UOyOFehHnov76p8amJopOnrsQ8pvv5p+tad6FzN/bvfkBWhdCfkyYL64ueiiWXJn6S8QYTrWlpYiIdCxXFxfiw/2JD/fnlmHND8U6cbqmefHtha00dx5q3oHM08OVPhH+9q004yP88XTX4lsRkc5KoV5EpJsymUyEh/gQHuLD6IERAJw5W2d/INah4kqyvszHBri6mIjr5WcP+X2iAvDtocW3IiKdhabftJGm34g4F/XXD1NTa+XIsUoOFTdvpVlwvIqGxub/AyN7+lwI+c0LcIP9vRxcrXQ09ZeIMTT9RkRE2pW3lzsDEnoyIKEnANaGC4tvL+yXv3XfCT7ffQyAEH8vEqMD7AtwI0K0+FZEpKMo1IuIyFVzd3MlKSaIpJggAJqabBSXnrPvl59bcIat+04C4NvDncSogAsLcAOIDfPDzVWLb0VEjKBQLyIi18zFxURsLz9ie/lx89BobDYbpRXnOVT8z/3ydx8+BYCHe/NTchOjAkiMDiQhwh8vD30NiYi0B/1vKiIi7cZkMhEW5E1YkDc3DGhefFt5ru6fO+xYKliTXYDN1vyU3NheviRGBdI3unnxrb+3h4M/gYiIc9JC2TbSQlkR56L+6nzO1zWQd6x5Tv6h4kqOllTR0NgEQHiI94WQ3zw3PyTAS/PyOzH1l4gxtFBWREQ6vR6ebqT2DiG1dwgA1oYmCk+cvRDyK9hxoJRNe0oACPLzJPHC7jp9owKJMPvgopAvItKKQr2IiDiUu5sLfaIC6BMVwIQRsTTZbBwrq7bvl3/YUsn23FIAvD3d6PMvIT8uXItvRURAoV5ERDoZF5OJ6FBfokN9uWlwFDabjVOVtfaAf9hSwd68cqD5F4L4cH/7dJ2EyAB6eOqrTUS6H/3PJyIinZrJZMIc2ANzYA8y+ocDUFVTb99d57Clgg+2FLHWVojJBDGhfvYpO4nRgQT4aPGtiHR9WijbRlooK+Jc1F/dQ219A3klVRwubp6yc7SkivqG5sW3YUE9SIwOtAf90MAeWnzbTtRfIsbQQlkREemWvDzcSIkLJiUuGICGxiYKT57lcHHzVpq7D5Xx1d7jAAT4eJAYHUjfCw/Gig71xcVFIV9EnJtCvYiIdDlurs0PukqICCBzeAxNNhvHy2ua7+RbKjh8YZcdgB6eriRENs/J7xsdSHy4H+5urg7+BCIibaNQLyIiXZ6LyURkTx8ie/owNi0SgPLK2uYddiyVHC6uYOWmowC4uZqIC/e/EPID6BMZgLeXuyPLFxH5Xgr1IiLSLYUEeBES0IsRKb0AOHfeemHhbXPI/2h7ER9stWECIs2+zTvsRAeSGBVIkJ+nY4sXEfkOhXoRERHAt4c7aYlm0hLNANRZGzl6YfHtYUsFm789wcZdxwDoGeDVvFf+hQW4vYK9tfhWRBxKoV5EROQSPN1duS42iOtigwBobGqi6OS5CyG/km+PlpOdcwIAP293EqMuLL6NDiQmzBdXFz0US0Q6jkK9iIjIVXB1aX7QVXy4P7cMA5vNxonTNRy2VF54MFYFuw6VAeDp4UqfCH8So5r3yu8d4Y+nuxbfiohxFOpFRESugclkIjzEh/AQH0YPjADgzNm65sW3xRUcKq4k66t8bICri4m4Xn72/fITowLx7aHFtyLSfvTwqTbSw6dEnIv6SxypptbKkWOVHCqu5JClgoLjVTQ0Nn+HRPb0abFffkiAl4OrbTv1l4gx9PApERGRTsTby50BCT0ZkNATAGtDI/nHzzbfybdUsG3/CT7f3bz4NsTf80LIb56yEx7ijYsW34rIVVKoFxER6SDubq72XXMAmppsFJees++Xn1twhq37TgLNu/H0ibywjWZ0ALFhfri5avGtiFyaQr2IiIiDuLiYiO3lR2wvP8YPjcZms1Facf7Cwtvm/fK/OXIKAA83F3pH+F8I+YEkRPjj5aGvcRFp5tD/Derr61m4cCFZWVlUVVWRnJzME088QXp6+hXP27t3LytXrmTv3r0cOnQIq9XKwYMHWx1nsVgYN27cJd9j0aJFjB49ul0+h4iISHswmUyEBXkTFuTNDQOaF99Wnqv7lx12KlmTXYDN1vyU3Nhevs077EQ138339/Zw8CcQEUdxaKj/5S9/yYYNG5g1axaxsbGsWrWKOXPm8Pbbb5OWlnbZ87744guWL19OUlIS0dHRHD169IrXmTJlCqNGjWoxlpyc3C6fQURExEgBvp4MTQ5laHIoAOfrGsg71rzw9nBxJZ/tPsaGr4sBCA/xvhDym6ft9Azw0kOxRLoJh4X6vXv3sm7dOp566inuu+8+AG677TYmTZrEggULWLJkyWXPvfvuu5kzZw5eXl78/ve//95Qn5KSwtSpU9uzfBEREYfo4elGau8QUnuHAGBtaKLwxFkOXdhKc8eBUjbtKQEgyM/THvATowKJNPto8a1IF+WwUL9+/Xrc3d2ZPn26fczT05M777yT5557jtLSUkJDQy95bs+ePdt8vZqaGtzc3PDw0J8mRUSk63B3c6FPVAB9ogKYMCKWJpuNkrJqe8g/bKlke24pAN6ebvS5EPL7RgUS28sPdzctvhXpChwW6nNzc4mPj8fHx6fF+IABA7DZbOTm5l421LfVwoULeeaZZzCZTAwcOJB58+Zx/fXXt8t7i4iIdCYuJhNRob5Ehfpy0+AobDYb5ZW1F0J+JYctFezNKweafyGID/enb3QAfaMCSYgMoIenFt+KOCOHdW5ZWRlhYWGtxs1mMwClpaU/+BouLi6MGjWKm2++mdDQUAoLC3n99de5//77efPNNxk6dOgPvoaIiEhnZjKZ6BnYg56BPRiZGg5AVU09R+yLbyv4YEsRa22FmEwQHepL36hA+y47AT6t/8K9Zd8JVn6Rx+mqOoL9PZk2JoH0lF4d/dFE5F84LNTX1tbi7t76Edmenp4A1NXV/eBrRERE8Prrr7cYmzBhAhMnTmTBggUsXbq0ze/Z1qd7XS2z2c+Q9xUR9ZfId5mBhNgQbr3w8/m6Bg4WnmZ//mn2HS3ny2+P88lOCwARPX3oFx9CSu9g+vUO4WDBGRavP0idtRGA8qo6Fq8/iL+fF2OHRDvmA4mI40K9l5cXVqu11fjFMH8x3Le3sLAwJk6cyLJlyzh//jw9evRo0/nl5edoarK1a016zLaIcdRfIlcnMqgHkUGR3Dw4kobGJgpPnuXwhek6W3OO88nXRQCYTGD7ztdgnbWRN9fuIyUm0AGVi3Q9Li6mNt9IdlioN5vNl5xiU1ZWBtBu8+kvJTw8nKamJqqqqtoc6kVERLo6N1cXEiICSIgIIHN4DE02G8fLazhcXMHij1o/Fwaa79iLiOM4bMl7cnIy+fn5VFdXtxjfs2eP/XWjFBcX4+rqSkBAgGHXEBER6SpcTCYie/owNi2SEP9L/yX9cuMi0jEcFuozMzOxWq0sX77cPlZfX8/KlSsZPHiwfRFtSUkJeXl513SN06dPtxorLCxk3bp1DB06FC8vr2srXkREpJuaNiYBj+9sg+nh5sK0MQkOqkhEwIHTbwYOHEhmZiYLFiygrKyMmJgYVq1aRUlJCc8884z9uPnz57N9+3YOHvznn/uOHTtGVlYWAN9++y0AL730EtB8h/+mm24C4E9/+hPFxcWMGDGC0NBQioqK7Itj58+f3yGfU0REpCu5uMuNdr8R6VwcuhntH//4R55//nmysrKorKwkKSmJV199lSFDhlzxPIvFwsKFC1uMXfz59ttvt4f6jIwMli5dyv/+7/9y9uxZ/P39ycjI4NFHHyUxMdGYDyUiItLFpaf0Ij2llxaii3QiJpvtu2vY5Uq0+42Ic1F/iRhH/SVijGvZ/UbPhhYRERERcXIK9SIiIiIiTk6hXkRERETEySnUi4iIiIg4OYV6EREREREnp1AvIiIiIuLkFOpFRERERJycQr2IiIiIiJNz6BNlnZGLi8mp3ldE1F8iRlJ/ibS/a+krPVFWRERERMTJafqNiIiIiIiTU6gXEREREXFyCvUiIiIiIk5OoV5ERERExMkp1IuIiIiIODmFehERERERJ6dQLyIiIiLi5BTqRUREREScnEK9iIiIiIiTU6gXEREREXFybo4uoLsqLS1l8eLF7Nmzh5ycHGpqali8eDHDhw93dGkiTm3v3r2sWrWKbdu2UVJSQmBgIGlpaTz++OPExsY6ujwRp/btt9/y8ssvs3//fsrLy/Hz8yM5OZlHHnmEwYMHO7o8kS5n0aJFLFiwgOTkZLKysq54rEK9g+Tn57No0SJiY2NJSkpi9+7dji5JpEt47bXX2LVrF5mZmSQlJVFWVsaSJUu47bbbWLFiBQkJCY4uUcRpFRcX09jYyPTp0zGbzZw9e5Y1a9Zw7733smjRIjIyMhxdokiXUVZWxt/+9je8vb2v6niTzWazGVyTXMK5c+ewWq0EBQXxySef8Mgjj+hOvUg72LVrF6mpqXh4eNjHCgoKmDx5MhMnTuQPf/iDA6sT6XrOnz/P+PHjSU1N5ZVXXnF0OSJdxi9/+UtKSkqw2WxUVVV97516zal3EF9fX4KCghxdhkiXM3jw4BaBHiAuLo7ExETy8vIcVJVI19WjRw+Cg4OpqqpydCkiXcbevXtZvXo1Tz311FWfo1AvIl2ezWbj1KlT+kVapJ2cO3eO06dPc/ToUf785z9z6NAh0tPTHV2WSJdgs9n43e9+x2233cZ111131edpTr2IdHmrV6/m5MmTPPHEE44uRaRL+NWvfsVHH30EgLu7Oz/+8Y/52c9+5uCqRLqG999/nyNHjvDiiy+26TyFehHp0vLy8nj66acZMmQIU6dOdXQ5Il3CI488wl133cWJEyfIysqivr4eq9XaauqbiLTNuXPnePbZZ/npT39KaGhom87V9BsR6bLKysp46KGHCAgIYOHChbi46L88kfaQlJRERkYGd9xxB6+//jr79u1r09xfEbm0v/3tb7i7u3P//fe3+Vx9w4lIl3T27FnmzJnD2bNnee211zCbzY4uSaRLcnd3Z9y4cWzYsIHa2lpHlyPitEpLS3nrrbe45557OHXqFBaLBYvFQl1dHVarFYvFQmVl5WXP1/QbEely6urq+NnPfkZBQQFvvvkmvXv3dnRJIl1abW0tNpuN6upqvLy8HF2OiFMqLy/HarWyYMECFixY0Or1cePGMWfOHObNm3fJ8xXqRaRLaWxs5PHHH+ebb77hpZdeYtCgQY4uSaTLOH36NMHBwS3Gzp07x0cffUR4eDghISEOqkzE+UVFRV1ycezzzz9PTU0Nv/rVr4iLi7vs+Qr1DvTSSy8B2PfOzsrKYufOnfj7+3Pvvfc6sjQRp/WHP/yBjRs3cuONN1JRUdHiYR0+Pj6MHz/egdWJOLfHH38cT09P0tLSMJvNHD9+nJUrV3LixAn+/Oc/O7o8Eafm5+d3ye+ot956C1dX1+/9/tITZR0oKSnpkuORkZFs3Lixg6sR6RpmzpzJ9u3bL/maekvkh1mxYgVZWVkcOXKEqqoq/Pz8GDRoEA888ADDhg1zdHkiXdLMmTOv6omyCvUiIiIiIk5Ou9+IiIiIiDg5hXoRERERESenUC8iIiIi4uQU6kVEREREnJxCvYiIiIiIk1OoFxERERFxcgr1IiIiIiJOTqFeREQ6vZkzZ3LTTTc5ugwRkU7LzdEFiIiIY2zbto1Zs2Zd9nVXV1f279/fgRWJiMi1UqgXEenmJk2axOjRo1uNu7joj7kiIs5CoV5EpJvr168fU6dOdXQZIiLyA+g2jIiIXJHFYiEpKYkXXniBtWvXMnnyZPr378/YsWN54YUXaGhoaHXOgQMHeOSRRxg+fDj9+/dnwoQJLFq0iMbGxlbHlpWV8d///d+MGzeO1NRU0tPTuf/++9m8eXOrY0+ePMmTTz7J9ddfz8CBA3nwwQfJz8835HOLiDgT3akXEenmzp8/z+nTp1uNe3h44Ovra/9548aNFBcXM2PGDHr27MnGjRv561//SklJCc8884z9uG+//ZaZM2fi5uZmP/azzz5jwYIFHDhwgGeffdZ+rMVi4e6776a8vJypU6eSmprK+fPn2bNnD9nZ2WRkZNiPramp4d5772XgwIE88cQTWCwWFi9ezNy5c1m7di2urq4G/QuJiHR+CvUiIt3cCy+8wAsvvNBqfOzYsbzyyiv2nw8cOMCKFStISUkB4N577+XRRx9l5cqV3HXXXQwaNAiA3//+99TX17N06VKSk5Ptxz7++OOsXbuWO++8k/T0dAD+67/+i9LSUl577TVuuOGGFtdvampq8fOZM2d48MEHmTNnjn0sODiYP/3pT2RnZ7c6X0SkO1GoFxHp5u666y4yMzNbjQcHB7f4eeTIkfZAD2AymZg9ezaffPIJH3/8MYMGDaK8vJzdu3dz88032wP9xWMffvhh1q9fz8cff0x6ejoVFRV8+eWX3HDDDZcM5N9dqOvi4tJqt54RI0YAUFhYqFAvIt2aQr2ISDcXGxvLyJEjv/e4hISEVmN9+vQBoLi4GGieTvOv4/+qd+/euLi42I8tKirCZrPRr1+/q6ozNDQUT0/PFmOBgYEAVFRUXNV7iIh0VVooKyIiTuFKc+ZtNlsHViIi0vko1IuIyFXJy8trNXbkyBEAoqOjAYiKimox/q+OHj1KU1OT/diYmBhMJhO5ublGlSwi0m0o1IuIyFXJzs5m37599p9tNhuvvfYaAOPHjwcgJCSEtLQ0PvvsMw4dOtTi2FdffRWAm2++GWieOjN69Gg2bdpEdnZ2q+vp7ruIyNXTnHoRkW5u//79ZGVlXfK1i2EdIDk5mZ/85CfMmDEDs9nMp59+SnZ2NlOnTiUtLc1+3K9//WtmzpzJjBkzuOeeezCbzXz22Wd89dVXTJo0yb7zDcB//Md/sH//fubMmcNtt91GSkoKdXV17Nmzh8jISH7xi18Y98FFRLoQhXoRkW5u7dq1rF279pKvbdiwwT6X/aabbiI+Pp5XXnmF/Px8QkJCmDt3LnPnzm1xTv/+/Vm6dCl/+ctfePfdd6mpqSE6Opp58+bxwAMPtDg2Ojqa9957jxdffJFNmzaRlZWFv78/ycnJ3HXXXcZ8YBGRLshk0983RUTkCiwWC+PGjePRRx/l3/7t3xxdjoiIXILm1IuIiIiIODmFehERERERJ6dQLyIiIiLi5DSnXkRERETEyelOvYiIiIiIk1OoFxERERFxcgr1IiIiIiJOTqFeRERERMTJKdSLiIiIiDg5hXoRERERESf3/wGzTNFzljXXdAAAAABJRU5ErkJggg==\n"},"metadata":{}}]},{"cell_type":"code","source":["# Tokenize all of the sentences and map the tokens to thier word IDs.\n","input_ids = []\n","attention_masks = []\n","\n","# For every sentence...\n","for pair in pairs_test:\n","\n","    encoded_dict = tokenizer.encode_plus(\n","                        pair,         # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = 73,           # Pad & truncate all sentences.\n","                        pad_to_max_length = True,\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","    \n","    # Add the encoded sentence to the list.    \n","    input_ids.append(encoded_dict['input_ids'])\n","    \n","    # And its attention mask (simply differentiates padding from non-padding).\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","# Convert the lists into tensors.\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(labels_test)\n","\n","# Set the batch size.  \n","batch_size = 32  \n","\n","# Create the DataLoader.\n","prediction_data = TensorDataset(input_ids, attention_masks, labels)\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652636386828,"user_tz":-120,"elapsed":5115,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"74a389f9-9505-4c07-80a3-544ab13ad556","id":"uG8opOxcfuUD"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]}]},{"cell_type":"code","source":["# Prediction on test set\n","\n","print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n","\n","# Put model in evaluation mode\n","model.eval()\n","\n","# Tracking variables \n","predictions , true_labels = [], []\n","\n","# Predict \n","for batch in prediction_dataloader:\n","  # Add batch to GPU\n","  batch = tuple(t.to(device) for t in batch)\n","  \n","  # Unpack the inputs from our dataloader\n","  b_input_ids, b_input_mask, b_labels = batch\n","  \n","\n","  with torch.no_grad():\n","      # Forward pass, calculate logit predictions.\n","      result = model(b_input_ids, \n","                     token_type_ids=None, \n","                     attention_mask=b_input_mask,\n","                     return_dict=True)\n","\n","  logits = result.logits\n","\n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","  \n","  # Store predictions and true labels\n","  predictions.append(logits)\n","  true_labels.append(label_ids)\n","\n","print('    DONE.')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652643036490,"user_tz":-120,"elapsed":21521,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"0099091b-eb67-4e46-f1b8-84e25f81cb06","id":"Lt5hgkKbfuUD"},"execution_count":91,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicting labels for 4,171 test sentences...\n","    DONE.\n"]}]},{"cell_type":"code","source":["print('Positive samples: %d of %d (%.2f%%)' % (labels_test.sum(), len(labels_test), (labels_test.sum() / len(labels_test) * 100.0)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652643039224,"user_tz":-120,"elapsed":258,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"c857c433-9fa3-4767-fe67-17b126599308","id":"5L7BYrhvfuUE"},"execution_count":92,"outputs":[{"output_type":"stream","name":"stdout","text":["Positive samples: 760 of 4171 (18.22%)\n"]}]},{"cell_type":"code","source":["from sklearn.metrics import matthews_corrcoef,confusion_matrix,accuracy_score,f1_score,classification_report\n","\n","# Combine the results across all batches. \n","flat_predictions = np.concatenate(predictions, axis=0)\n","\n","# For each sample, pick the label (0 or 1) with the higher score.\n","flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n","\n","# Combine the correct labels for each batch into a single list.\n","flat_true_labels = np.concatenate(true_labels, axis=0)\n","\n","# Calculate the MCC\n","mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n","acc = accuracy_score(flat_true_labels, flat_predictions)\n","\n","\n","print('Total MCC: %.3f' % mcc)\n","print('Total Acc: %.3f' % acc)\n","print(classification_report(flat_true_labels,flat_predictions))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652643043891,"user_tz":-120,"elapsed":218,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"7fc7c67f-aeaa-4d88-bf34-53f508390852","id":"qQ7VHkd5fuUE"},"execution_count":93,"outputs":[{"output_type":"stream","name":"stdout","text":["Total MCC: 0.424\n","Total Acc: 0.808\n","              precision    recall  f1-score   support\n","\n","           0       0.91      0.85      0.88      3411\n","           1       0.48      0.61      0.54       760\n","\n","    accuracy                           0.81      4171\n","   macro avg       0.69      0.73      0.71      4171\n","weighted avg       0.83      0.81      0.82      4171\n","\n"]}]},{"cell_type":"code","source":["import os\n","\n","# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n","\n","output_dir = './bert_base_bl/'\n","\n","# Create output directory if needed\n","if not os.path.exists(output_dir):\n","    os.makedirs(output_dir)\n","\n","print(\"Saving model to %s\" % output_dir)\n","\n","# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n","# They can then be reloaded using `from_pretrained()`\n","model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n","model_to_save.save_pretrained(output_dir)\n","tokenizer.save_pretrained(output_dir)\n","\n","# Good practice: save your training arguments together with the trained model\n","# torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5nOMA7jTyEaX","executionInfo":{"status":"ok","timestamp":1652636893960,"user_tz":-120,"elapsed":1668,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"1123fce0-d656-4184-fe2d-a2a4538d5a27"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["Saving model to ./bert_base_bl/\n"]},{"output_type":"execute_result","data":{"text/plain":["('./bert_base_bl/tokenizer_config.json',\n"," './bert_base_bl/special_tokens_map.json',\n"," './bert_base_bl/vocab.txt',\n"," './bert_base_bl/added_tokens.json')"]},"metadata":{},"execution_count":35}]},{"cell_type":"code","source":["!ls -l --block-size=K ./bert_base_bl/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mSaqFw600Rd7","executionInfo":{"status":"ok","timestamp":1652642997578,"user_tz":-120,"elapsed":569,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"3aa058f8-0506-4151-9702-c690a1580a78"},"execution_count":89,"outputs":[{"output_type":"stream","name":"stdout","text":["total 427984K\n","-rw-r--r-- 1 root root      1K May 15 17:48 config.json\n","-rw-r--r-- 1 root root 427741K May 15 17:48 pytorch_model.bin\n","-rw-r--r-- 1 root root      1K May 15 17:48 special_tokens_map.json\n","-rw-r--r-- 1 root root      1K May 15 17:48 tokenizer_config.json\n","-rw-r--r-- 1 root root    227K May 15 17:48 vocab.txt\n"]}]},{"cell_type":"code","source":["# Copy the model files to a directory in your Google Drive.\n","!cp -r ./bert_base_bl/ \"/content/drive/MyDrive/Keypoints\""],"metadata":{"id":"BIDQyY5v0Shc","executionInfo":{"status":"ok","timestamp":1652636971413,"user_tz":-120,"elapsed":1932,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["# Load a trained model and vocabulary that you have fine-tuned\n","model = model.from_pretrained('./bert_base_bl/')\n","tokenizer = tokenizer.from_pretrained('./bert_base_bl/')\n","\n","# Copy the model to the GPU.\n","model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KWoK4dds0xMu","executionInfo":{"status":"ok","timestamp":1652643006979,"user_tz":-120,"elapsed":1867,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"4a558963-7e5a-4cb1-def8-c085d6a8c51c"},"execution_count":90,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"metadata":{},"execution_count":90}]},{"cell_type":"markdown","source":["## large(baseline)"],"metadata":{"id":"0F7zXizRbBM2"}},{"cell_type":"code","source":["import tensorflow as tf\n","\n","# Get the GPU device name.\n","device_name = tf.test.gpu_device_name()\n","\n","# The device name should look like the following:\n","if device_name == '/device:GPU:0':\n","    print('Found GPU at: {}'.format(device_name))\n","else:\n","    raise SystemError('GPU device not found')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652637622856,"user_tz":-120,"elapsed":218,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"3429980a-850a-486e-fc90-e994b2ba0179","id":"wBDrzYpCbBM2"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["Found GPU at: /device:GPU:0\n"]}]},{"cell_type":"code","source":["import torch\n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652637624506,"user_tz":-120,"elapsed":248,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"d71754a7-bd4b-4e5a-cabc-a7765887c014","id":"2H4ngFUwbBM2"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla T4\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","train = pd.read_csv(\"/content/drive/MyDrive/Keypoints/train.csv\")\n","dev = pd.read_csv(\"/content/drive/MyDrive/Keypoints/dev.csv\")\n","test = pd.read_csv(\"/content/drive/MyDrive/Keypoints/test.csv\")"],"metadata":{"id":"kYzihhNNbBM3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for split in [train,dev,test]:\n","  for i in split.index:\n","    arg = split['argument'][i]\n","    key = split['key_point'][i]\n","    if arg[-1] != '.':\n","      pair = arg + '. ' + key + '.'\n","      split.at[i, 'pair'] = pair\n","    else:\n","      pair = arg + ' ' + key + '.'\n","      split.at[i, 'pair'] = pair"],"metadata":{"id":"0xBnE7nvbBM3","executionInfo":{"status":"ok","timestamp":1652637643004,"user_tz":-120,"elapsed":640,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["print(train['pair'][0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652637644097,"user_tz":-120,"elapsed":219,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"27a514c9-7f63-4ffb-c7f8-756bd785f650","id":"w1qE3-IibBM3"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["a person created through cloning could potentially have developmental problems caused by imperfections in the cloning process. Cloning is not understood enough yet.\n"]}]},{"cell_type":"code","source":["pairs_train = train.pair.values\n","labels_train = train.label.values\n","\n","pairs_dev = dev.pair.values\n","labels_dev = dev.label.values\n","\n","pairs_test = test.pair.values\n","labels_test = test.label.values"],"metadata":{"id":"8LRIq3HXbBM3","executionInfo":{"status":"ok","timestamp":1652637646513,"user_tz":-120,"elapsed":197,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}}},"execution_count":47,"outputs":[]},{"cell_type":"code","source":["from transformers import BertTokenizer\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-large-uncased', do_lower_case=True)"],"metadata":{"executionInfo":{"status":"ok","timestamp":1652637649933,"user_tz":-120,"elapsed":1467,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"id":"qGSeHcf0bBM3","colab":{"base_uri":"https://localhost:8080/","height":112,"referenced_widgets":["a0d79ca33b8940cea6748e0757032691","9d8b45faeffe4956862fbb528b761de4","544c55993fb24bdd8e2d0029def933a3","71cf8329b3d442b08537e1a4cdff6b04","e369a531e7f54fa2bdf6b983d6d85127","bc3d53040b024f96907ace59fc7234e7","a44cdbc37d2345a7b52c10006a2a0cb6","cd400c9cf96d4cb9952172503add70fb","7e16e2444d6a4d4f84f4297769a7ed0d","312a81607ac04adb8ecb12a0abe3ff98","cd8b95bd05374e4d9862142d15a50c4e","4ff7415fb5b24e56b66d49a419626590","6cd4bad8b6114f5d83937b2196250e58","35909b2e50b7415383c2898a8564f33a","c4b72b41d4e246b9b06392a7042d0674","2b519cab4d7e41a09b9b318302ebb6a5","0b866b28b8494938bcd24352de0b83a7","cc259b588fb4486a87e6d8b8bdc40d15","114dbc4ecfd14b77870ce95c79676c62","15b891cf237341cd9ccf997fec3c2cbb","7745a628d33942af9022a395be069e04","d780337e432348979ce7bcd6312af3ec","495188cfe29843d08d5b0f3251a4e04f","8ddf098f9fdc419fb8b269ac3444fe2a","bd6fd48a6a43471e9331ea2ddf057459","d57d873a14614ff392bac18317c5222d","de08b3c7a0ea4553b08a068ea6efca6f","b782544f70964ae783d994dfe7f84747","0f9e47d90410437a9e1f519916646d34","1a1e363d1d8e4962bb9d89bc8c81f279","149d06df74bf417eba33d0386db0eb93","076cc52243e84940a310a831b4be55e9","364254a6517d48898d7453ad127dc119"]},"outputId":"820245dc-595b-4159-cab4-feadd8273660"},"execution_count":48,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0d79ca33b8940cea6748e0757032691"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ff7415fb5b24e56b66d49a419626590"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/571 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"495188cfe29843d08d5b0f3251a4e04f"}},"metadata":{}}]},{"cell_type":"code","source":["max_len = 0\n","\n","for split in [pairs_train,pairs_dev,pairs_test]:\n","  for pair in split:\n","\n","      # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n","      input_ids = tokenizer.encode(pair, add_special_tokens=True)\n","\n","      # Update the maximum sentence length.\n","      max_len = max(max_len, len(input_ids))\n","\n","print('Max sentence length: ', max_len)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652637673771,"user_tz":-120,"elapsed":22385,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"4e0aca2b-1e93-41dd-b35a-f6d1ffe476fa","id":"rA_xjYL3bBM3"},"execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":["Max sentence length:  73\n"]}]},{"cell_type":"code","source":["input_ids_train = []\n","attention_masks_train = []\n","\n","\n","for pair in pairs_train:\n","    encoded_dict = tokenizer.encode_plus(\n","                        pair,     # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = 73,   # Pad & truncate all sentences.\n","                        pad_to_max_length = True,\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","    \n","    # Add the encoded sentence to the list.    \n","    input_ids_train.append(encoded_dict['input_ids'])\n","    \n","    # And its attention mask (simply differentiates padding from non-padding).\n","    attention_masks_train.append(encoded_dict['attention_mask'])\n","\n","# Convert the lists into tensors.\n","input_ids_train = torch.cat(input_ids_train, dim=0)\n","attention_masks_train = torch.cat(attention_masks_train, dim=0)\n","labels_train = torch.tensor(labels_train)\n","\n","# Print sentence 0, now as a list of IDs.\n","print('Original: ', pairs_train[0])\n","print('Token IDs:', input_ids_train[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652637704257,"user_tz":-120,"elapsed":26884,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"d5af7fb0-7c44-4200-f117-f6e5e9d2bcc2","id":"R1z4xbgubBM3"},"execution_count":50,"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Original:  a person created through cloning could potentially have developmental problems caused by imperfections in the cloning process. Cloning is not understood enough yet.\n","Token IDs: tensor([  101,  1037,  2711,  2580,  2083, 18856, 13369,  2071,  9280,  2031,\n","        13908,  3471,  3303,  2011, 29238,  8496,  1999,  1996, 18856, 13369,\n","         2832,  1012, 18856, 13369,  2003,  2025,  5319,  2438,  2664,  1012,\n","          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0])\n"]}]},{"cell_type":"code","source":["# Tokenize all of the sentences and map the tokens to thier word IDs.\n","input_ids_dev = []\n","attention_masks_dev = []\n","\n","for pair in pairs_dev:\n","    encoded_dict = tokenizer.encode_plus(\n","                        pair,     # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = 73,   # Pad & truncate all sentences.\n","                        pad_to_max_length = True,\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","        \n","    input_ids_dev.append(encoded_dict['input_ids'])\n","    \n","\n","    attention_masks_dev.append(encoded_dict['attention_mask'])\n","\n","\n","input_ids_dev = torch.cat(input_ids_dev, dim=0)\n","attention_masks_dev = torch.cat(attention_masks_dev, dim=0)\n","labels_dev = torch.tensor(labels_dev)\n","\n","\n","print('Original: ', pairs_dev[0])\n","print('Token IDs:', input_ids_dev[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652637716097,"user_tz":-120,"elapsed":3396,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"5805a4b4-4c49-47f5-ecfb-c6d646e628d5","id":"J8rcuB5hbBM4"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Original:  a man or woman has the right to do what they wish with their body, and if they choose to sell it for sex, the government should not interfere. Legalizing sex work boosts the economy.\n","Token IDs: tensor([  101,  1037,  2158,  2030,  2450,  2038,  1996,  2157,  2000,  2079,\n","         2054,  2027,  4299,  2007,  2037,  2303,  1010,  1998,  2065,  2027,\n","         5454,  2000,  5271,  2009,  2005,  3348,  1010,  1996,  2231,  2323,\n","         2025, 15115,  1012,  3423,  6026,  3348,  2147, 12992,  2015,  1996,\n","         4610,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0])\n"]}]},{"cell_type":"code","source":["from torch.utils.data import TensorDataset\n","train_dataset = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n","dev_dataset = TensorDataset(input_ids_dev, attention_masks_dev, labels_dev)"],"metadata":{"id":"JNzoLXq2bBM4","executionInfo":{"status":"ok","timestamp":1652637718871,"user_tz":-120,"elapsed":227,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}}},"execution_count":52,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","batch_size = 32\n","\n","\n","train_dataloader = DataLoader(\n","            train_dataset,  # The training samples.\n","            sampler = RandomSampler(train_dataset), # Select batches randomly\n","            batch_size = batch_size # Trains with this batch size.\n","        )\n","\n","# For validation the order doesn't matter, so we'll just read them sequentially.\n","dev_dataloader = DataLoader(\n","            dev_dataset, # The validation samples.\n","            sampler = SequentialSampler(dev_dataset), # Pull out batches sequentially.\n","            batch_size = batch_size # Evaluate with this batch size.\n","        )"],"metadata":{"id":"iZwijTEbbBM4","executionInfo":{"status":"ok","timestamp":1652637721243,"user_tz":-120,"elapsed":261,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}}},"execution_count":53,"outputs":[]},{"cell_type":"code","source":["from transformers import BertForSequenceClassification, AdamW, BertConfig\n","\n","\n","model = BertForSequenceClassification.from_pretrained(\n","    \"bert-large-uncased\", # Use the -layer BERT model, with an uncased vocab.\n","    num_labels = 2, # The number of output labels--2 for binary classification.\n","                    # You can increase this for multi-class tasks.   \n","    output_attentions = False, # Whether the model returns attentions weights.\n","    output_hidden_states = False, # Whether the model returns all hidden-states.\n",")\n","\n","# Tell pytorch to run this model on the GPU.\n","model.cuda()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["3fe739e0666a484bbc2a1a22c5c00f89","a6a21e7213c946a8bf1ca5be2df02a52","a5c12f8555224ec28582783de97debf4","d79eea5cdec9408493e7f5189a7aaa78","8ac31770e97d455488374e8ae1f07706","671fbad3bfdf48c9a4535f910f701eaf","68aeea6e038c4e1aba083bdc3221292f","088a856a699944abbb5a83f141c2a414","41f1058b54854c1eaff1e767dcdec694","9402fa4a8fbc44d7a310fb00ef23f030","033f4457d5394ee9bcbc4c4c4b8563aa"]},"executionInfo":{"status":"ok","timestamp":1652637765194,"user_tz":-120,"elapsed":38067,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"cab7f92b-a322-40d7-8184-7a5af91f8236","id":"ekU_VJVlbBM4"},"execution_count":55,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/1.25G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fe739e0666a484bbc2a1a22c5c00f89"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n","      (position_embeddings): Embedding(512, 1024)\n","      (token_type_embeddings): Embedding(2, 1024)\n","      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (12): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (13): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (14): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (15): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (16): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (17): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (18): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (19): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (20): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (21): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (22): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (23): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=1024, out_features=2, bias=True)\n",")"]},"metadata":{},"execution_count":55}]},{"cell_type":"code","source":["optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n","                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n","                )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652637805374,"user_tz":-120,"elapsed":224,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"719765b7-14b6-432d-d09c-931c45ef4413","id":"AK_z-eiRbBM4"},"execution_count":56,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n"]}]},{"cell_type":"code","source":["from transformers import get_linear_schedule_with_warmup\n","\n","# Number of training epochs. The BERT authors recommend between 2 and 4. \n","\n","epochs = 3\n","\n","# Total number of training steps is [number of batches] x [number of epochs]. \n","# (Note that this is not the same as the number of training samples).\n","total_steps = len(train_dataloader) * epochs\n","\n","# Create the learning rate scheduler.\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0, # Default value in run_glue.py\n","                                            num_training_steps = total_steps)"],"metadata":{"id":"XsrByJnWbBM5","executionInfo":{"status":"ok","timestamp":1652637806531,"user_tz":-120,"elapsed":252,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}}},"execution_count":57,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"],"metadata":{"id":"VTsBhFeObBM5","executionInfo":{"status":"ok","timestamp":1652637808849,"user_tz":-120,"elapsed":321,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}}},"execution_count":58,"outputs":[]},{"cell_type":"code","source":["import time\n","import datetime\n","\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"],"metadata":{"id":"7xbiOqi2bBM5","executionInfo":{"status":"ok","timestamp":1652637809462,"user_tz":-120,"elapsed":2,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}}},"execution_count":59,"outputs":[]},{"cell_type":"code","source":["import random\n","import numpy as np\n","\n","# This training code is based on the `run_glue.py` script here:\n","# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n","\n","# Set the seed value all over the place to make this reproducible.\n","seed_val = 42\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# We'll store a number of quantities such as training and validation loss, \n","# validation accuracy, and timings.\n","training_stats = []\n","\n","# Measure the total training time for the whole run.\n","total_t0 = time.time()\n","\n","# For each epoch...\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    # Perform one full pass over the training set.\n","\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    # Measure how long the training epoch takes.\n","    t0 = time.time()\n","\n","    # Reset the total loss for this epoch.\n","    total_train_loss = 0\n","\n","\n","    model.train()\n","\n","    # For each batch of training data...\n","    for step, batch in enumerate(train_dataloader):\n","\n","        # Progress update every 40 batches.\n","        if step % 40 == 0 and not step == 0:\n","            # Calculate elapsed time in minutes.\n","            elapsed = format_time(time.time() - t0)\n","            \n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n"," \n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","\n","\n","        model.zero_grad()        \n","\n","        result = model(b_input_ids, \n","                       token_type_ids=None, \n","                       attention_mask=b_input_mask, \n","                       labels=b_labels,\n","                       return_dict=True)\n","\n","        loss = result.loss\n","        logits = result.logits\n","\n","\n","        total_train_loss += loss.item()\n","\n","        # Perform a backward pass to calculate the gradients.\n","        loss.backward()\n","\n","\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        optimizer.step()\n","\n","        # Update the learning rate.\n","        scheduler.step()\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_train_loss = total_train_loss / len(train_dataloader)            \n","    \n","    # Measure how long this epoch took.\n","    training_time = format_time(time.time() - t0)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epcoh took: {:}\".format(training_time))\n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    # After the completion of each training epoch, measure our performance on\n","    # our validation set.\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    t0 = time.time()\n","\n","    # Put the model in evaluation mode--the dropout layers behave differently\n","    # during evaluation.\n","    model.eval()\n","\n","    # Tracking variables \n","    total_eval_accuracy = 0\n","    total_eval_loss = 0\n","    nb_eval_steps = 0\n","\n","    # Evaluate data for one epoch\n","    for batch in dev_dataloader:\n","        \n","\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","        \n","\n","        with torch.no_grad():        \n","\n","            # Forward pass, calculate logit predictions.\n","            # token_type_ids is the same as the \"segment ids\", which \n","            # differentiates sentence 1 and 2 in 2-sentence tasks.\n","            result = model(b_input_ids, \n","                           token_type_ids=None, \n","                           attention_mask=b_input_mask,\n","                           labels=b_labels,\n","                           return_dict=True)\n","\n","        # Get the loss and \"logits\" output by the model. The \"logits\" are the \n","        # output values prior to applying an activation function like the \n","        # softmax.\n","        loss = result.loss\n","        logits = result.logits\n","            \n","        # Accumulate the validation loss.\n","        total_eval_loss += loss.item()\n","\n","        # Move logits and labels to CPU\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","\n","        # Calculate the accuracy for this batch of test sentences, and\n","        # accumulate it over all batches.\n","        total_eval_accuracy += flat_accuracy(logits, label_ids)\n","        \n","\n","    # Report the final accuracy for this validation run.\n","    avg_val_accuracy = total_eval_accuracy / len(dev_dataloader)\n","    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_val_loss = total_eval_loss / len(dev_dataloader)\n","    \n","    # Measure how long the validation run took.\n","    validation_time = format_time(time.time() - t0)\n","    \n","    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n","    print(\"  Validation took: {:}\".format(validation_time))\n","\n","    # Record all statistics from this epoch.\n","    training_stats.append(\n","        {\n","            'epoch': epoch_i + 1,\n","            'Training Loss': avg_train_loss,\n","            'Valid. Loss': avg_val_loss,\n","            'Valid. Accur.': avg_val_accuracy,\n","            'Training Time': training_time,\n","            'Validation Time': validation_time\n","        }\n","    )\n","\n","print(\"\")\n","print(\"Training complete!\")\n","\n","print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652640193776,"user_tz":-120,"elapsed":2382753,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"4b84ad2d-282d-4b3a-e1c2-3a868ab2a714","id":"AOIQH7iObBM5"},"execution_count":60,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======== Epoch 1 / 3 ========\n","Training...\n","  Batch    40  of    532.    Elapsed: 0:00:58.\n","  Batch    80  of    532.    Elapsed: 0:01:54.\n","  Batch   120  of    532.    Elapsed: 0:02:50.\n","  Batch   160  of    532.    Elapsed: 0:03:47.\n","  Batch   200  of    532.    Elapsed: 0:04:43.\n","  Batch   240  of    532.    Elapsed: 0:05:39.\n","  Batch   280  of    532.    Elapsed: 0:06:35.\n","  Batch   320  of    532.    Elapsed: 0:07:31.\n","  Batch   360  of    532.    Elapsed: 0:08:27.\n","  Batch   400  of    532.    Elapsed: 0:09:24.\n","  Batch   440  of    532.    Elapsed: 0:10:20.\n","  Batch   480  of    532.    Elapsed: 0:11:16.\n","  Batch   520  of    532.    Elapsed: 0:12:12.\n","\n","  Average training loss: 0.33\n","  Training epcoh took: 0:12:29\n","\n","Running Validation...\n","  Accuracy: 0.82\n","  Validation Loss: 0.43\n","  Validation took: 0:00:47\n","\n","======== Epoch 2 / 3 ========\n","Training...\n","  Batch    40  of    532.    Elapsed: 0:00:56.\n","  Batch    80  of    532.    Elapsed: 0:01:52.\n","  Batch   120  of    532.    Elapsed: 0:02:48.\n","  Batch   160  of    532.    Elapsed: 0:03:45.\n","  Batch   200  of    532.    Elapsed: 0:04:41.\n","  Batch   240  of    532.    Elapsed: 0:05:37.\n","  Batch   280  of    532.    Elapsed: 0:06:33.\n","  Batch   320  of    532.    Elapsed: 0:07:29.\n","  Batch   360  of    532.    Elapsed: 0:08:25.\n","  Batch   400  of    532.    Elapsed: 0:09:21.\n","  Batch   440  of    532.    Elapsed: 0:10:18.\n","  Batch   480  of    532.    Elapsed: 0:11:14.\n","  Batch   520  of    532.    Elapsed: 0:12:10.\n","\n","  Average training loss: 0.19\n","  Training epcoh took: 0:12:26\n","\n","Running Validation...\n","  Accuracy: 0.81\n","  Validation Loss: 0.47\n","  Validation took: 0:00:47\n","\n","======== Epoch 3 / 3 ========\n","Training...\n","  Batch    40  of    532.    Elapsed: 0:00:56.\n","  Batch    80  of    532.    Elapsed: 0:01:52.\n","  Batch   120  of    532.    Elapsed: 0:02:48.\n","  Batch   160  of    532.    Elapsed: 0:03:45.\n","  Batch   200  of    532.    Elapsed: 0:04:41.\n","  Batch   240  of    532.    Elapsed: 0:05:37.\n","  Batch   280  of    532.    Elapsed: 0:06:33.\n","  Batch   320  of    532.    Elapsed: 0:07:29.\n","  Batch   360  of    532.    Elapsed: 0:08:25.\n","  Batch   400  of    532.    Elapsed: 0:09:21.\n","  Batch   440  of    532.    Elapsed: 0:10:17.\n","  Batch   480  of    532.    Elapsed: 0:11:13.\n","  Batch   520  of    532.    Elapsed: 0:12:09.\n","\n","  Average training loss: 0.13\n","  Training epcoh took: 0:12:26\n","\n","Running Validation...\n","  Accuracy: 0.82\n","  Validation Loss: 0.58\n","  Validation took: 0:00:47\n","\n","Training complete!\n","Total training took 0:39:42 (h:mm:ss)\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","\n","# Display floats with two decimal places.\n","pd.set_option('precision', 2)\n","\n","# Create a DataFrame from our training statistics.\n","df_stats = pd.DataFrame(data=training_stats)\n","\n","# Use the 'epoch' as the row index.\n","df_stats = df_stats.set_index('epoch')\n","\n","# A hack to force the column headers to wrap.\n","#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n","\n","# Display the table.\n","df_stats"],"metadata":{"id":"SpkiLLdkbBM6","colab":{"base_uri":"https://localhost:8080/","height":175},"executionInfo":{"status":"ok","timestamp":1652640817502,"user_tz":-120,"elapsed":261,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"b66b2dd9-0e35-4954-f6ae-d09bc3545456"},"execution_count":61,"outputs":[{"output_type":"execute_result","data":{"text/plain":["       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n","epoch                                                                         \n","1               0.33         0.43           0.82       0:12:29         0:00:47\n","2               0.19         0.47           0.81       0:12:26         0:00:47\n","3               0.13         0.58           0.82       0:12:26         0:00:47"],"text/html":["\n","  <div id=\"df-26b64aed-13e2-4fed-8068-dc4f80d8b020\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Training Loss</th>\n","      <th>Valid. Loss</th>\n","      <th>Valid. Accur.</th>\n","      <th>Training Time</th>\n","      <th>Validation Time</th>\n","    </tr>\n","    <tr>\n","      <th>epoch</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>0.33</td>\n","      <td>0.43</td>\n","      <td>0.82</td>\n","      <td>0:12:29</td>\n","      <td>0:00:47</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.19</td>\n","      <td>0.47</td>\n","      <td>0.81</td>\n","      <td>0:12:26</td>\n","      <td>0:00:47</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.13</td>\n","      <td>0.58</td>\n","      <td>0.82</td>\n","      <td>0:12:26</td>\n","      <td>0:00:47</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-26b64aed-13e2-4fed-8068-dc4f80d8b020')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-26b64aed-13e2-4fed-8068-dc4f80d8b020 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-26b64aed-13e2-4fed-8068-dc4f80d8b020');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":61}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","% matplotlib inline\n","\n","import seaborn as sns\n","\n","# Use plot styling from seaborn.\n","sns.set(style='darkgrid')\n","\n","# Increase the plot size and font size.\n","sns.set(font_scale=1.5)\n","plt.rcParams[\"figure.figsize\"] = (12,6)\n","\n","# Plot the learning curve.\n","plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n","plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n","\n","# Label the plot.\n","plt.title(\"Training & Validation Loss\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.xticks([1, 2, 3, 4])\n","\n","plt.show()"],"metadata":{"id":"G5fM1MqJbBM6","colab":{"base_uri":"https://localhost:8080/","height":427},"executionInfo":{"status":"ok","timestamp":1652640824975,"user_tz":-120,"elapsed":649,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"8d740c0c-3586-4cfa-fa40-a88c53eb6eaa"},"execution_count":62,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 864x432 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":["# Tokenize all of the sentences and map the tokens to thier word IDs.\n","input_ids = []\n","attention_masks = []\n","\n","# For every sentence...\n","for pair in pairs_test:\n","    # `encode_plus` will:\n","    #   (1) Tokenize the sentence.\n","    #   (2) Prepend the `[CLS]` token to the start.\n","    #   (3) Append the `[SEP]` token to the end.\n","    #   (4) Map tokens to their IDs.\n","    #   (5) Pad or truncate the sentence to `max_length`\n","    #   (6) Create attention masks for [PAD] tokens.\n","    encoded_dict = tokenizer.encode_plus(\n","                        pair,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = 90,           # Pad & truncate all sentences.\n","                        pad_to_max_length = True,\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","    \n","    # Add the encoded sentence to the list.    \n","    input_ids.append(encoded_dict['input_ids'])\n","    \n","    # And its attention mask (simply differentiates padding from non-padding).\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","# Convert the lists into tensors.\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(labels_test)\n","\n","# Set the batch size.  \n","batch_size = 32  \n","\n","# Create the DataLoader.\n","prediction_data = TensorDataset(input_ids, attention_masks, labels)\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"],"metadata":{"id":"6io1UX0RnJfE","colab":{"base_uri":"https://localhost:8080/","height":72},"executionInfo":{"status":"ok","timestamp":1652640833937,"user_tz":-120,"elapsed":4898,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"5c8e7123-809c-4b3e-f9e4-b2f31f85ba5e"},"execution_count":63,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]}]},{"cell_type":"markdown","source":["### Prediction"],"metadata":{"id":"6FxanW0SQLT5"}},{"cell_type":"code","source":["# Prediction on test set\n","\n","print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n","\n","# Put model in evaluation mode\n","model.eval()\n","\n","# Tracking variables \n","predictions , true_labels = [], []\n","\n","# Predict \n","for batch in prediction_dataloader:\n","  # Add batch to GPU\n","  batch = tuple(t.to(device) for t in batch)\n","  \n","  # Unpack the inputs from our dataloader\n","  b_input_ids, b_input_mask, b_labels = batch\n","  \n","  # Telling the model not to compute or store gradients, saving memory and \n","  # speeding up prediction\n","  with torch.no_grad():\n","      # Forward pass, calculate logit predictions.\n","      result = model(b_input_ids, \n","                     token_type_ids=None, \n","                     attention_mask=b_input_mask,\n","                     return_dict=True)\n","\n","  logits = result.logits\n","\n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","  \n","  # Store predictions and true labels\n","  predictions.append(logits)\n","  true_labels.append(label_ids)\n","\n","print('    DONE.')"],"metadata":{"id":"jPefKKssnVIx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652640920403,"user_tz":-120,"elapsed":84821,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"204acfb6-285b-459b-c12c-1efbcc899757"},"execution_count":64,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicting labels for 4,171 test sentences...\n","    DONE.\n"]}]},{"cell_type":"code","source":["print('Positive samples: %d of %d (%.2f%%)' % (labels_test.sum(), len(labels_test), (labels_test.sum() / len(labels_test) * 100.0)))"],"metadata":{"id":"DJu8KOOlbBM6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652640969472,"user_tz":-120,"elapsed":232,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"b76867dc-3c16-4f3a-8e52-97d700b3aef7"},"execution_count":65,"outputs":[{"output_type":"stream","name":"stdout","text":["Positive samples: 760 of 4171 (18.22%)\n"]}]},{"cell_type":"code","source":["from sklearn.metrics import matthews_corrcoef,confusion_matrix,accuracy_score,f1_score,classification_report\n","\n","# Combine the results across all batches. \n","flat_predictions = np.concatenate(predictions, axis=0)\n","\n","# For each sample, pick the label (0 or 1) with the higher score.\n","flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n","\n","# Combine the correct labels for each batch into a single list.\n","flat_true_labels = np.concatenate(true_labels, axis=0)\n","\n","# Calculate the MCC\n","mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n","acc = accuracy_score(flat_true_labels, flat_predictions)\n","\n","\n","print('Total MCC: %.3f' % mcc)\n","print('Total Acc: %.3f' % acc)\n","print(classification_report(flat_true_labels,flat_predictions))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yLaCas_tGb-L","executionInfo":{"status":"ok","timestamp":1652641852170,"user_tz":-120,"elapsed":235,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"9f724ddd-36c1-4f17-af5e-f7a56e115ce1"},"execution_count":83,"outputs":[{"output_type":"stream","name":"stdout","text":["Total MCC: 0.409\n","Total Acc: 0.794\n","              precision    recall  f1-score   support\n","\n","           0       0.91      0.83      0.87      3411\n","           1       0.45      0.63      0.53       760\n","\n","    accuracy                           0.79      4171\n","   macro avg       0.68      0.73      0.70      4171\n","weighted avg       0.83      0.79      0.81      4171\n","\n"]}]},{"cell_type":"markdown","source":["### save and load"],"metadata":{"id":"bwttor08QPQt"}},{"cell_type":"code","source":["import os\n","\n","# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n","\n","output_dir = './bert_large_bl/'\n","\n","# Create output directory if needed\n","if not os.path.exists(output_dir):\n","    os.makedirs(output_dir)\n","\n","print(\"Saving model to %s\" % output_dir)\n","\n","# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n","# They can then be reloaded using `from_pretrained()`\n","model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n","model_to_save.save_pretrained(output_dir)\n","tokenizer.save_pretrained(output_dir)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wtQCOrHIJypA","executionInfo":{"status":"ok","timestamp":1652642242599,"user_tz":-120,"elapsed":5524,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"7ffb4224-7af4-4e9f-b916-2eacff4de3a7"},"execution_count":84,"outputs":[{"output_type":"stream","name":"stdout","text":["Saving model to ./bert_large_bl/\n"]},{"output_type":"execute_result","data":{"text/plain":["('./bert_large_bl/tokenizer_config.json',\n"," './bert_large_bl/special_tokens_map.json',\n"," './bert_large_bl/vocab.txt',\n"," './bert_large_bl/added_tokens.json')"]},"metadata":{},"execution_count":84}]},{"cell_type":"code","source":["!cp -r ./bert_large_bl/ \"/content/drive/MyDrive/Keypoints\""],"metadata":{"id":"_oS3xMnuJ-zk","executionInfo":{"status":"ok","timestamp":1652642735460,"user_tz":-120,"elapsed":7461,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}}},"execution_count":86,"outputs":[]},{"cell_type":"code","source":["# Load a trained model and vocabulary that you have fine-tuned\n","model = model.from_pretrained('./bert_large_bl/')\n","tokenizer = tokenizer.from_pretrained('./bert_large_bl/')\n","\n","# Copy the model to the GPU.\n","model.to(device)"],"metadata":{"id":"VQEsgFqaKBli"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## base(prompt)"],"metadata":{"id":"LqrkCU2rwa43"}},{"cell_type":"code","source":["import tensorflow as tf\n","\n","# Get the GPU device name.\n","device_name = tf.test.gpu_device_name()\n","\n","# The device name should look like the following:\n","if device_name == '/device:GPU:0':\n","    print('Found GPU at: {}'.format(device_name))\n","else:\n","    raise SystemError('GPU device not found')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rpPUiQFHwcZg","executionInfo":{"status":"ok","timestamp":1652643128727,"user_tz":-120,"elapsed":204,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"3a973b9c-d4be-43ec-e94e-c5f0e7c410d9"},"execution_count":94,"outputs":[{"output_type":"stream","name":"stdout","text":["Found GPU at: /device:GPU:0\n"]}]},{"cell_type":"code","source":["import torch\n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aZiHr5vKwnRw","executionInfo":{"status":"ok","timestamp":1652643130270,"user_tz":-120,"elapsed":275,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"fa7189c9-6e4a-40cc-a3e0-b9defcb36b2a"},"execution_count":95,"outputs":[{"output_type":"stream","name":"stdout","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla T4\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","train = pd.read_csv(\"/content/drive/MyDrive/Keypoints/train.csv\")\n","dev = pd.read_csv(\"/content/drive/MyDrive/Keypoints/dev.csv\")\n","test = pd.read_csv(\"/content/drive/MyDrive/Keypoints/test.csv\")"],"metadata":{"id":"P3BAHhxAw4YN","executionInfo":{"status":"ok","timestamp":1652643142288,"user_tz":-120,"elapsed":242,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}}},"execution_count":97,"outputs":[]},{"cell_type":"code","source":["for split in [train,dev,test]:\n","  for i in split.index:\n","    arg = split['argument'][i]\n","    key = split['key_point'][i]\n","    if arg[-1] != '.':\n","      prompt = f\"The argument is \\\"{arg}.\\\" The keypoint is \\\"{key}.\\\" The argument and the keypoint are\"\n","      split.at[i, 'pair'] = prompt\n","    else:\n","      prompt = f\"The argument is \\\"{arg}\\\" The keypoint is \\\"{key}.\\\" The argument and the keypoint are\"\n","      split.at[i, 'pair'] = prompt"],"metadata":{"id":"tiWyl--DYhv7","executionInfo":{"status":"ok","timestamp":1652643146690,"user_tz":-120,"elapsed":928,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}}},"execution_count":98,"outputs":[]},{"cell_type":"code","source":["print(train['pair'][0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bjqvoqDtZIPo","executionInfo":{"status":"ok","timestamp":1652643148287,"user_tz":-120,"elapsed":209,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"aafc2944-76a3-444e-dba1-35964492f412"},"execution_count":99,"outputs":[{"output_type":"stream","name":"stdout","text":["The argument is \"a person created through cloning could potentially have developmental problems caused by imperfections in the cloning process.\" The keypoint is \"Cloning is not understood enough yet.\" The argument and the keypoint are\n"]}]},{"cell_type":"code","source":["pairs_train = train.pair.values\n","labels_train = train.label.values\n","\n","pairs_dev = dev.pair.values\n","labels_dev = dev.label.values\n","\n","pairs_test = test.pair.values\n","labels_test = test.label.values"],"metadata":{"id":"6X_tt0FaZa-V","executionInfo":{"status":"ok","timestamp":1652643156416,"user_tz":-120,"elapsed":227,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}}},"execution_count":100,"outputs":[]},{"cell_type":"code","source":["from transformers import BertTokenizer\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"],"metadata":{"id":"eitKclBqx09l","executionInfo":{"status":"ok","timestamp":1652643158753,"user_tz":-120,"elapsed":822,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}}},"execution_count":101,"outputs":[]},{"cell_type":"code","source":["max_len = 0\n","\n","for split in [pairs_train,pairs_dev,pairs_test]:\n","  for pair in split:\n","\n","      # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n","      input_ids = tokenizer.encode(pair, add_special_tokens=True)\n","\n","      # Update the maximum sentence length.\n","      max_len = max(max_len, len(input_ids))\n","\n","print('Max sentence length: ', max_len)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VRi2iOBMzeuK","executionInfo":{"status":"ok","timestamp":1652643191432,"user_tz":-120,"elapsed":29951,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"37c1d8b7-4023-4011-ed4d-7e264bd679b5"},"execution_count":102,"outputs":[{"output_type":"stream","name":"stdout","text":["Max sentence length:  91\n"]}]},{"cell_type":"code","source":["# Tokenize all of the sentences and map the tokens to thier word IDs.\n","input_ids_train = []\n","attention_masks_train = []\n","\n","# For every sentence...\n","for pair in pairs_train:\n","    # `encode_plus` will:\n","    #   (1) Tokenize the sentence.\n","    #   (2) Prepend the `[CLS]` token to the start.\n","    #   (3) Append the `[SEP]` token to the end.\n","    #   (4) Map tokens to their IDs.\n","    #   (5) Pad or truncate the sentence to `max_length`\n","    #   (6) Create attention masks for [PAD] tokens.\n","    encoded_dict = tokenizer.encode_plus(\n","                        pair,     # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = 91,   # Pad & truncate all sentences.\n","                        pad_to_max_length = True,\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","    \n","    # Add the encoded sentence to the list.    \n","    input_ids_train.append(encoded_dict['input_ids'])\n","    \n","    # And its attention mask (simply differentiates padding from non-padding).\n","    attention_masks_train.append(encoded_dict['attention_mask'])\n","\n","# Convert the lists into tensors.\n","input_ids_train = torch.cat(input_ids_train, dim=0)\n","attention_masks_train = torch.cat(attention_masks_train, dim=0)\n","labels_train = torch.tensor(labels_train)\n","\n","# Print sentence 0, now as a list of IDs.\n","print('Original: ', pairs_train[0])\n","print('Token IDs:', input_ids_train[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UwDmDoXY2B-5","executionInfo":{"status":"ok","timestamp":1652643265853,"user_tz":-120,"elapsed":23739,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"1fb620a4-e30c-497d-9a2e-3e3add418cca"},"execution_count":103,"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Original:  The argument is \"a person created through cloning could potentially have developmental problems caused by imperfections in the cloning process.\" The keypoint is \"Cloning is not understood enough yet.\" The argument and the keypoint are\n","Token IDs: tensor([  101,  1996,  6685,  2003,  1000,  1037,  2711,  2580,  2083, 18856,\n","        13369,  2071,  9280,  2031, 13908,  3471,  3303,  2011, 29238,  8496,\n","         1999,  1996, 18856, 13369,  2832,  1012,  1000,  1996,  3145,  8400,\n","         2003,  1000, 18856, 13369,  2003,  2025,  5319,  2438,  2664,  1012,\n","         1000,  1996,  6685,  1998,  1996,  3145,  8400,  2024,   102,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0])\n"]}]},{"cell_type":"code","source":["# Tokenize all of the sentences and map the tokens to thier word IDs.\n","input_ids_dev = []\n","attention_masks_dev = []\n","\n","for pair in pairs_dev:\n","    encoded_dict = tokenizer.encode_plus(\n","                        pair,     # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = 91,   # Pad & truncate all sentences.\n","                        pad_to_max_length = True,\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","        \n","    input_ids_dev.append(encoded_dict['input_ids'])\n","    \n","\n","    attention_masks_dev.append(encoded_dict['attention_mask'])\n","\n","\n","input_ids_dev = torch.cat(input_ids_dev, dim=0)\n","attention_masks_dev = torch.cat(attention_masks_dev, dim=0)\n","labels_dev = torch.tensor(labels_dev)\n","\n","\n","print('Original: ', pairs_dev[0])\n","print('Token IDs:', input_ids_dev[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wLrH9Yn-32q4","executionInfo":{"status":"ok","timestamp":1652643282171,"user_tz":-120,"elapsed":4932,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"c4e1c13c-fb4e-4c1d-8ee1-86bec5416885"},"execution_count":104,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Original:  The argument is \"a man or woman has the right to do what they wish with their body, and if they choose to sell it for sex, the government should not interfere.\" The keypoint is \"Legalizing sex work boosts the economy.\" The argument and the keypoint are\n","Token IDs: tensor([  101,  1996,  6685,  2003,  1000,  1037,  2158,  2030,  2450,  2038,\n","         1996,  2157,  2000,  2079,  2054,  2027,  4299,  2007,  2037,  2303,\n","         1010,  1998,  2065,  2027,  5454,  2000,  5271,  2009,  2005,  3348,\n","         1010,  1996,  2231,  2323,  2025, 15115,  1012,  1000,  1996,  3145,\n","         8400,  2003,  1000,  3423,  6026,  3348,  2147, 12992,  2015,  1996,\n","         4610,  1012,  1000,  1996,  6685,  1998,  1996,  3145,  8400,  2024,\n","          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0])\n"]}]},{"cell_type":"code","source":["from torch.utils.data import TensorDataset\n","train_dataset = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n","dev_dataset = TensorDataset(input_ids_dev, attention_masks_dev, labels_dev)"],"metadata":{"id":"jB1OtJKa3O4p","executionInfo":{"status":"ok","timestamp":1652643319771,"user_tz":-120,"elapsed":213,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}}},"execution_count":105,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","# The DataLoader needs to know our batch size for training, so we specify it \n","# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n","# size of 16 or 32.\n","batch_size = 32\n","\n","# Create the DataLoaders for our training and validation sets.\n","# We'll take training samples in random order. \n","train_dataloader = DataLoader(\n","            train_dataset,  # The training samples.\n","            sampler = RandomSampler(train_dataset), # Select batches randomly\n","            batch_size = batch_size # Trains with this batch size.\n","        )\n","\n","# For validation the order doesn't matter, so we'll just read them sequentially.\n","dev_dataloader = DataLoader(\n","            dev_dataset, # The validation samples.\n","            sampler = SequentialSampler(dev_dataset), # Pull out batches sequentially.\n","            batch_size = batch_size # Evaluate with this batch size.\n","        )"],"metadata":{"id":"bhdbFbEn4B6t","executionInfo":{"status":"ok","timestamp":1652643320848,"user_tz":-120,"elapsed":2,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}}},"execution_count":106,"outputs":[]},{"cell_type":"code","source":["from transformers import BertForSequenceClassification, AdamW, BertConfig\n","\n","# Load BertForSequenceClassification, the pretrained BERT model with a single \n","# linear classification layer on top. \n","model = BertForSequenceClassification.from_pretrained(\n","    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n","    num_labels = 2, # The number of output labels--2 for binary classification.\n","                    # You can increase this for multi-class tasks.   \n","    output_attentions = False, # Whether the model returns attentions weights.\n","    output_hidden_states = False, # Whether the model returns all hidden-states.\n",")\n","\n","# Tell pytorch to run this model on the GPU.\n","model.cuda()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wXdethBv4j5_","executionInfo":{"status":"ok","timestamp":1652643331582,"user_tz":-120,"elapsed":4236,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"9f6a62d9-fa0c-417c-a4e4-52304fc73433"},"execution_count":108,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"metadata":{},"execution_count":108}]},{"cell_type":"code","source":["# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n","# I believe the 'W' stands for 'Weight Decay fix\"\n","optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n","                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n","                )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8H6CHuu35vmF","executionInfo":{"status":"ok","timestamp":1652643334007,"user_tz":-120,"elapsed":225,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"4d073ebc-db10-4f6e-a430-732f63d6e466"},"execution_count":109,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n"]}]},{"cell_type":"code","source":["from transformers import get_linear_schedule_with_warmup\n","\n","# Number of training epochs. The BERT authors recommend between 2 and 4. \n","\n","epochs = 3\n","\n","# Total number of training steps is [number of batches] x [number of epochs]. \n","# (Note that this is not the same as the number of training samples).\n","total_steps = len(train_dataloader) * epochs\n","\n","# Create the learning rate scheduler.\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0, # Default value in run_glue.py\n","                                            num_training_steps = total_steps)"],"metadata":{"id":"gUxPcuNo6Gs4","executionInfo":{"status":"ok","timestamp":1652643335415,"user_tz":-120,"elapsed":212,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}}},"execution_count":110,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"],"metadata":{"id":"FpGr7KTP6TZD","executionInfo":{"status":"ok","timestamp":1652643336989,"user_tz":-120,"elapsed":223,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}}},"execution_count":111,"outputs":[]},{"cell_type":"code","source":["import time\n","import datetime\n","\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"],"metadata":{"id":"QBYfkPM86xNh","executionInfo":{"status":"ok","timestamp":1652643338075,"user_tz":-120,"elapsed":211,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}}},"execution_count":112,"outputs":[]},{"cell_type":"code","source":["import random\n","import numpy as np\n","\n","# This training code is based on the `run_glue.py` script here:\n","# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n","\n","# Set the seed value all over the place to make this reproducible.\n","seed_val = 42\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# We'll store a number of quantities such as training and validation loss, \n","# validation accuracy, and timings.\n","training_stats = []\n","\n","# Measure the total training time for the whole run.\n","total_t0 = time.time()\n","\n","# For each epoch...\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    # Perform one full pass over the training set.\n","\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    # Measure how long the training epoch takes.\n","    t0 = time.time()\n","\n","    # Reset the total loss for this epoch.\n","    total_train_loss = 0\n","\n","    # Put the model into training mode. Don't be mislead--the call to \n","    # `train` just changes the *mode*, it doesn't *perform* the training.\n","    # `dropout` and `batchnorm` layers behave differently during training\n","    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n","    model.train()\n","\n","    # For each batch of training data...\n","    for step, batch in enumerate(train_dataloader):\n","\n","        # Progress update every 40 batches.\n","        if step % 40 == 0 and not step == 0:\n","            # Calculate elapsed time in minutes.\n","            elapsed = format_time(time.time() - t0)\n","            \n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n","        # `to` method.\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: attention masks\n","        #   [2]: labels \n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","\n","        # Always clear any previously calculated gradients before performing a\n","        # backward pass. PyTorch doesn't do this automatically because \n","        # accumulating the gradients is \"convenient while training RNNs\". \n","        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n","        model.zero_grad()        \n","\n","        # Perform a forward pass (evaluate the model on this training batch).\n","        # In PyTorch, calling `model` will in turn call the model's `forward` \n","        # function and pass down the arguments. The `forward` function is \n","        # documented here: \n","        # https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification\n","        # The results are returned in a results object, documented here:\n","        # https://huggingface.co/transformers/main_classes/output.html#transformers.modeling_outputs.SequenceClassifierOutput\n","        # Specifically, we'll get the loss (because we provided labels) and the\n","        # \"logits\"--the model outputs prior to activation.\n","        result = model(b_input_ids, \n","                       token_type_ids=None, \n","                       attention_mask=b_input_mask, \n","                       labels=b_labels,\n","                       return_dict=True)\n","\n","        loss = result.loss\n","        logits = result.logits\n","\n","        # Accumulate the training loss over all of the batches so that we can\n","        # calculate the average loss at the end. `loss` is a Tensor containing a\n","        # single value; the `.item()` function just returns the Python value \n","        # from the tensor.\n","        total_train_loss += loss.item()\n","\n","        # Perform a backward pass to calculate the gradients.\n","        loss.backward()\n","\n","        # Clip the norm of the gradients to 1.0.\n","        # This is to help prevent the \"exploding gradients\" problem.\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # Update parameters and take a step using the computed gradient.\n","        # The optimizer dictates the \"update rule\"--how the parameters are\n","        # modified based on their gradients, the learning rate, etc.\n","        optimizer.step()\n","\n","        # Update the learning rate.\n","        scheduler.step()\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_train_loss = total_train_loss / len(train_dataloader)            \n","    \n","    # Measure how long this epoch took.\n","    training_time = format_time(time.time() - t0)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epcoh took: {:}\".format(training_time))\n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    # After the completion of each training epoch, measure our performance on\n","    # our validation set.\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    t0 = time.time()\n","\n","    # Put the model in evaluation mode--the dropout layers behave differently\n","    # during evaluation.\n","    model.eval()\n","\n","    # Tracking variables \n","    total_eval_accuracy = 0\n","    total_eval_loss = 0\n","    nb_eval_steps = 0\n","\n","    # Evaluate data for one epoch\n","    for batch in dev_dataloader:\n","        \n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using \n","        # the `to` method.\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: attention masks\n","        #   [2]: labels \n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","        \n","        # Tell pytorch not to bother with constructing the compute graph during\n","        # the forward pass, since this is only needed for backprop (training).\n","        with torch.no_grad():        \n","\n","            # Forward pass, calculate logit predictions.\n","            # token_type_ids is the same as the \"segment ids\", which \n","            # differentiates sentence 1 and 2 in 2-sentence tasks.\n","            result = model(b_input_ids, \n","                           token_type_ids=None, \n","                           attention_mask=b_input_mask,\n","                           labels=b_labels,\n","                           return_dict=True)\n","\n","        # Get the loss and \"logits\" output by the model. The \"logits\" are the \n","        # output values prior to applying an activation function like the \n","        # softmax.\n","        loss = result.loss\n","        logits = result.logits\n","            \n","        # Accumulate the validation loss.\n","        total_eval_loss += loss.item()\n","\n","        # Move logits and labels to CPU\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","\n","        # Calculate the accuracy for this batch of test sentences, and\n","        # accumulate it over all batches.\n","        total_eval_accuracy += flat_accuracy(logits, label_ids)\n","        \n","\n","    # Report the final accuracy for this validation run.\n","    avg_val_accuracy = total_eval_accuracy / len(dev_dataloader)\n","    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_val_loss = total_eval_loss / len(dev_dataloader)\n","    \n","    # Measure how long the validation run took.\n","    validation_time = format_time(time.time() - t0)\n","    \n","    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n","    print(\"  Validation took: {:}\".format(validation_time))\n","\n","    # Record all statistics from this epoch.\n","    training_stats.append(\n","        {\n","            'epoch': epoch_i + 1,\n","            'Training Loss': avg_train_loss,\n","            'Valid. Loss': avg_val_loss,\n","            'Valid. Accur.': avg_val_accuracy,\n","            'Training Time': training_time,\n","            'Validation Time': validation_time\n","        }\n","    )\n","\n","print(\"\")\n","print(\"Training complete!\")\n","\n","print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HWPdoD9o67oI","executionInfo":{"status":"ok","timestamp":1652644219959,"user_tz":-120,"elapsed":880259,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"ebc1d576-d4da-4c1b-cc53-110799527ee4"},"execution_count":113,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======== Epoch 1 / 3 ========\n","Training...\n","  Batch    40  of    532.    Elapsed: 0:00:18.\n","  Batch    80  of    532.    Elapsed: 0:00:37.\n","  Batch   120  of    532.    Elapsed: 0:00:56.\n","  Batch   160  of    532.    Elapsed: 0:01:16.\n","  Batch   200  of    532.    Elapsed: 0:01:35.\n","  Batch   240  of    532.    Elapsed: 0:01:55.\n","  Batch   280  of    532.    Elapsed: 0:02:15.\n","  Batch   320  of    532.    Elapsed: 0:02:35.\n","  Batch   360  of    532.    Elapsed: 0:02:56.\n","  Batch   400  of    532.    Elapsed: 0:03:16.\n","  Batch   440  of    532.    Elapsed: 0:03:37.\n","  Batch   480  of    532.    Elapsed: 0:03:57.\n","  Batch   520  of    532.    Elapsed: 0:04:18.\n","\n","  Average training loss: 0.36\n","  Training epcoh took: 0:04:24\n","\n","Running Validation...\n","  Accuracy: 0.80\n","  Validation Loss: 0.43\n","  Validation took: 0:00:18\n","\n","======== Epoch 2 / 3 ========\n","Training...\n","  Batch    40  of    532.    Elapsed: 0:00:21.\n","  Batch    80  of    532.    Elapsed: 0:00:42.\n","  Batch   120  of    532.    Elapsed: 0:01:03.\n","  Batch   160  of    532.    Elapsed: 0:01:24.\n","  Batch   200  of    532.    Elapsed: 0:01:45.\n","  Batch   240  of    532.    Elapsed: 0:02:07.\n","  Batch   280  of    532.    Elapsed: 0:02:28.\n","  Batch   320  of    532.    Elapsed: 0:02:49.\n","  Batch   360  of    532.    Elapsed: 0:03:10.\n","  Batch   400  of    532.    Elapsed: 0:03:31.\n","  Batch   440  of    532.    Elapsed: 0:03:52.\n","  Batch   480  of    532.    Elapsed: 0:04:14.\n","  Batch   520  of    532.    Elapsed: 0:04:35.\n","\n","  Average training loss: 0.20\n","  Training epcoh took: 0:04:41\n","\n","Running Validation...\n","  Accuracy: 0.81\n","  Validation Loss: 0.43\n","  Validation took: 0:00:18\n","\n","======== Epoch 3 / 3 ========\n","Training...\n","  Batch    40  of    532.    Elapsed: 0:00:21.\n","  Batch    80  of    532.    Elapsed: 0:00:42.\n","  Batch   120  of    532.    Elapsed: 0:01:04.\n","  Batch   160  of    532.    Elapsed: 0:01:25.\n","  Batch   200  of    532.    Elapsed: 0:01:46.\n","  Batch   240  of    532.    Elapsed: 0:02:07.\n","  Batch   280  of    532.    Elapsed: 0:02:28.\n","  Batch   320  of    532.    Elapsed: 0:02:50.\n","  Batch   360  of    532.    Elapsed: 0:03:11.\n","  Batch   400  of    532.    Elapsed: 0:03:32.\n","  Batch   440  of    532.    Elapsed: 0:03:53.\n","  Batch   480  of    532.    Elapsed: 0:04:14.\n","  Batch   520  of    532.    Elapsed: 0:04:35.\n","\n","  Average training loss: 0.13\n","  Training epcoh took: 0:04:41\n","\n","Running Validation...\n","  Accuracy: 0.83\n","  Validation Loss: 0.50\n","  Validation took: 0:00:18\n","\n","Training complete!\n","Total training took 0:14:40 (h:mm:ss)\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","\n","# Display floats with two decimal places.\n","pd.set_option('precision', 2)\n","\n","# Create a DataFrame from our training statistics.\n","df_stats = pd.DataFrame(data=training_stats)\n","\n","# Use the 'epoch' as the row index.\n","df_stats = df_stats.set_index('epoch')\n","\n","# A hack to force the column headers to wrap.\n","#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n","\n","# Display the table.\n","df_stats"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":175},"id":"jGChEQKGPZDr","executionInfo":{"status":"ok","timestamp":1652644221980,"user_tz":-120,"elapsed":219,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"6c8d8867-734c-43ee-f5a3-a5b76209acc8"},"execution_count":114,"outputs":[{"output_type":"execute_result","data":{"text/plain":["       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n","epoch                                                                         \n","1               0.36         0.43           0.80       0:04:24         0:00:18\n","2               0.20         0.43           0.81       0:04:41         0:00:18\n","3               0.13         0.50           0.83       0:04:41         0:00:18"],"text/html":["\n","  <div id=\"df-2ba47184-6aa2-4c65-81d1-fb0fc6274e78\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Training Loss</th>\n","      <th>Valid. Loss</th>\n","      <th>Valid. Accur.</th>\n","      <th>Training Time</th>\n","      <th>Validation Time</th>\n","    </tr>\n","    <tr>\n","      <th>epoch</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>0.36</td>\n","      <td>0.43</td>\n","      <td>0.80</td>\n","      <td>0:04:24</td>\n","      <td>0:00:18</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.20</td>\n","      <td>0.43</td>\n","      <td>0.81</td>\n","      <td>0:04:41</td>\n","      <td>0:00:18</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.13</td>\n","      <td>0.50</td>\n","      <td>0.83</td>\n","      <td>0:04:41</td>\n","      <td>0:00:18</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2ba47184-6aa2-4c65-81d1-fb0fc6274e78')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-2ba47184-6aa2-4c65-81d1-fb0fc6274e78 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-2ba47184-6aa2-4c65-81d1-fb0fc6274e78');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":114}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","% matplotlib inline\n","\n","import seaborn as sns\n","\n","# Use plot styling from seaborn.\n","sns.set(style='darkgrid')\n","\n","# Increase the plot size and font size.\n","sns.set(font_scale=1.5)\n","plt.rcParams[\"figure.figsize\"] = (12,6)\n","\n","# Plot the learning curve.\n","plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n","plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n","\n","# Label the plot.\n","plt.title(\"Training & Validation Loss\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.xticks([1, 2, 3, 4])\n","\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":427},"id":"7caIUQRfPjMm","executionInfo":{"status":"ok","timestamp":1652644227129,"user_tz":-120,"elapsed":646,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"4f137bdc-56a3-4548-b7d7-509db2165bf4"},"execution_count":115,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 864x432 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":["# Tokenize all of the sentences and map the tokens to thier word IDs.\n","input_ids = []\n","attention_masks = []\n","\n","# For every sentence...\n","for pair in pairs_test:\n","    # `encode_plus` will:\n","    #   (1) Tokenize the sentence.\n","    #   (2) Prepend the `[CLS]` token to the start.\n","    #   (3) Append the `[SEP]` token to the end.\n","    #   (4) Map tokens to their IDs.\n","    #   (5) Pad or truncate the sentence to `max_length`\n","    #   (6) Create attention masks for [PAD] tokens.\n","    encoded_dict = tokenizer.encode_plus(\n","                        pair,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = 91,           # Pad & truncate all sentences.\n","                        pad_to_max_length = True,\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","    \n","    # Add the encoded sentence to the list.    \n","    input_ids.append(encoded_dict['input_ids'])\n","    \n","    # And its attention mask (simply differentiates padding from non-padding).\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","# Convert the lists into tensors.\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(labels_test)\n","\n","# Set the batch size.  \n","batch_size = 32  \n","\n","# Create the DataLoader.\n","prediction_data = TensorDataset(input_ids, attention_masks, labels)\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gGA3nly8PnLx","executionInfo":{"status":"ok","timestamp":1652644254535,"user_tz":-120,"elapsed":16515,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"b258a4d3-ea70-4832-bbfc-5708a15a2940"},"execution_count":117,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]}]},{"cell_type":"code","source":["# Prediction on test set\n","\n","print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n","\n","# Put model in evaluation mode\n","model.eval()\n","\n","# Tracking variables \n","predictions , true_labels = [], []\n","\n","# Predict \n","for batch in prediction_dataloader:\n","  # Add batch to GPU\n","  batch = tuple(t.to(device) for t in batch)\n","  \n","  # Unpack the inputs from our dataloader\n","  b_input_ids, b_input_mask, b_labels = batch\n","  \n","  # Telling the model not to compute or store gradients, saving memory and \n","  # speeding up prediction\n","  with torch.no_grad():\n","      # Forward pass, calculate logit predictions.\n","      result = model(b_input_ids, \n","                     token_type_ids=None, \n","                     attention_mask=b_input_mask,\n","                     return_dict=True)\n","\n","  logits = result.logits\n","\n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","  \n","  # Store predictions and true labels\n","  predictions.append(logits)\n","  true_labels.append(label_ids)\n","\n","print('    DONE.')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R7mRSGcoPsUD","executionInfo":{"status":"ok","timestamp":1652644286133,"user_tz":-120,"elapsed":25873,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"bd3f1f39-f191-4d83-8a1d-cf0067402714"},"execution_count":118,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicting labels for 4,171 test sentences...\n","    DONE.\n"]}]},{"cell_type":"code","source":["from sklearn.metrics import matthews_corrcoef,confusion_matrix,accuracy_score,f1_score,classification_report\n","\n","# Combine the results across all batches. \n","flat_predictions = np.concatenate(predictions, axis=0)\n","\n","# For each sample, pick the label (0 or 1) with the higher score.\n","flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n","\n","# Combine the correct labels for each batch into a single list.\n","flat_true_labels = np.concatenate(true_labels, axis=0)\n","\n","# Calculate the MCC\n","mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n","acc = accuracy_score(flat_true_labels, flat_predictions)\n","\n","\n","print('Total MCC: %.3f' % mcc)\n","print('Total Acc: %.3f' % acc)\n","print(classification_report(flat_true_labels,flat_predictions))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ji22yj0VP8-O","executionInfo":{"status":"ok","timestamp":1652644297226,"user_tz":-120,"elapsed":322,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"58cdca30-3d04-4773-c7ad-69adccd62b4e"},"execution_count":119,"outputs":[{"output_type":"stream","name":"stdout","text":["Total MCC: 0.422\n","Total Acc: 0.815\n","              precision    recall  f1-score   support\n","\n","           0       0.90      0.87      0.88      3411\n","           1       0.49      0.58      0.53       760\n","\n","    accuracy                           0.81      4171\n","   macro avg       0.70      0.73      0.71      4171\n","weighted avg       0.83      0.81      0.82      4171\n","\n"]}]},{"cell_type":"markdown","source":["### save and load"],"metadata":{"id":"lP8WGte_QaHh"}},{"cell_type":"code","source":["import os\n","\n","# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n","\n","output_dir = './bert_base_pt/'\n","\n","# Create output directory if needed\n","if not os.path.exists(output_dir):\n","    os.makedirs(output_dir)\n","\n","print(\"Saving model to %s\" % output_dir)\n","\n","# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n","# They can then be reloaded using `from_pretrained()`\n","model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n","model_to_save.save_pretrained(output_dir)\n","tokenizer.save_pretrained(output_dir)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652644317697,"user_tz":-120,"elapsed":1863,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"d742312b-e268-4947-bf58-e171fda057d0","id":"-m1TD52kQaHh"},"execution_count":120,"outputs":[{"output_type":"stream","name":"stdout","text":["Saving model to ./bert_base_pt/\n"]},{"output_type":"execute_result","data":{"text/plain":["('./bert_base_pt/tokenizer_config.json',\n"," './bert_base_pt/special_tokens_map.json',\n"," './bert_base_pt/vocab.txt',\n"," './bert_base_pt/added_tokens.json')"]},"metadata":{},"execution_count":120}]},{"cell_type":"code","source":["!cp -r ./bert_base_pt/ \"/content/drive/MyDrive/Keypoints\""],"metadata":{"executionInfo":{"status":"ok","timestamp":1652644322913,"user_tz":-120,"elapsed":2546,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"id":"45HhpTS7QaHh"},"execution_count":121,"outputs":[]},{"cell_type":"code","source":["# Load a trained model and vocabulary that you have fine-tuned\n","model = model.from_pretrained('./bert_base_pt/')\n","tokenizer = tokenizer.from_pretrained('./bert_base_pt/')\n","\n","# Copy the model to the GPU.\n","model.to(device)"],"metadata":{"id":"sMdzkMnJQaHi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## large(prompt)"],"metadata":{"id":"wWTAbqbpZ7tq"}},{"cell_type":"code","source":["import tensorflow as tf\n","\n","# Get the GPU device name.\n","device_name = tf.test.gpu_device_name()\n","\n","# The device name should look like the following:\n","if device_name == '/device:GPU:0':\n","    print('Found GPU at: {}'.format(device_name))\n","else:\n","    raise SystemError('GPU device not found')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652644333551,"user_tz":-120,"elapsed":242,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"4d80a564-5b60-4278-c3a7-2282e0fb14bd","id":"6TMtWXdSZ7tq"},"execution_count":122,"outputs":[{"output_type":"stream","name":"stdout","text":["Found GPU at: /device:GPU:0\n"]}]},{"cell_type":"code","source":["import torch\n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652644335869,"user_tz":-120,"elapsed":418,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"da65faa5-5978-452b-d61b-9ec88c352770","id":"2PKCN_wDZ7tq"},"execution_count":123,"outputs":[{"output_type":"stream","name":"stdout","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla T4\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","train = pd.read_csv(\"/content/drive/MyDrive/LTP-keypoints/train.csv\")\n","dev = pd.read_csv(\"/content/drive/MyDrive/LTP-keypoints/dev.csv\")\n","test = pd.read_csv(\"/content/drive/MyDrive/LTP-keypoints/test.csv\")"],"metadata":{"id":"mYeli20lZ7tr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for split in [train,dev,test]:\n","  for i in split.index:\n","    arg = split['argument'][i]\n","    key = split['key_point'][i]\n","    if arg[-1] != '.':\n","      prompt = f\"The argument is \\\"{arg}.\\\" The keypoint is \\\"{key}.\\\" The argument and the keypoint are\"\n","      split.at[i, 'pair'] = prompt\n","    else:\n","      prompt = f\"The argument is \\\"{arg}\\\" The keypoint is \\\"{key}.\\\" The argument and the keypoint are\"\n","      split.at[i, 'pair'] = prompt"],"metadata":{"id":"3NIRYb3VZ7tr","executionInfo":{"status":"ok","timestamp":1652644342712,"user_tz":-120,"elapsed":545,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}}},"execution_count":124,"outputs":[]},{"cell_type":"code","source":["print(train['pair'][0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652644344008,"user_tz":-120,"elapsed":225,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"dce6f533-4f1b-420e-decf-f4e42f2cd95a","id":"cAA6pyLKZ7tr"},"execution_count":125,"outputs":[{"output_type":"stream","name":"stdout","text":["The argument is \"a person created through cloning could potentially have developmental problems caused by imperfections in the cloning process.\" The keypoint is \"Cloning is not understood enough yet.\" The argument and the keypoint are\n"]}]},{"cell_type":"code","source":["pairs_train = train.pair.values\n","labels_train = train.label.values\n","\n","pairs_dev = dev.pair.values\n","labels_dev = dev.label.values\n","\n","pairs_test = test.pair.values\n","labels_test = test.label.values"],"metadata":{"id":"LYgZ8-CLZ7tr","executionInfo":{"status":"ok","timestamp":1652644351112,"user_tz":-120,"elapsed":222,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}}},"execution_count":126,"outputs":[]},{"cell_type":"code","source":["from transformers import BertTokenizer\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-large-uncased', do_lower_case=True)"],"metadata":{"executionInfo":{"status":"ok","timestamp":1652644353305,"user_tz":-120,"elapsed":646,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"id":"p7dqLs0zZ7ts"},"execution_count":127,"outputs":[]},{"cell_type":"code","source":["max_len = 0\n","\n","for split in [pairs_train,pairs_dev,pairs_test]:\n","  for pair in split:\n","\n","      # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n","      input_ids = tokenizer.encode(pair, add_special_tokens=True)\n","\n","      # Update the maximum sentence length.\n","      max_len = max(max_len, len(input_ids))\n","\n","print('Max sentence length: ', max_len)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652644402957,"user_tz":-120,"elapsed":41552,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"a5b1b01c-4932-45c7-ecb4-f3aba4865120","id":"_typkMQ1Z7ts"},"execution_count":128,"outputs":[{"output_type":"stream","name":"stdout","text":["Max sentence length:  91\n"]}]},{"cell_type":"code","source":["# Tokenize all of the sentences and map the tokens to thier word IDs.\n","input_ids_train = []\n","attention_masks_train = []\n","\n","# For every sentence...\n","for pair in pairs_train:\n","    # `encode_plus` will:\n","    #   (1) Tokenize the sentence.\n","    #   (2) Prepend the `[CLS]` token to the start.\n","    #   (3) Append the `[SEP]` token to the end.\n","    #   (4) Map tokens to their IDs.\n","    #   (5) Pad or truncate the sentence to `max_length`\n","    #   (6) Create attention masks for [PAD] tokens.\n","    encoded_dict = tokenizer.encode_plus(\n","                        pair,     # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = 91,   # Pad & truncate all sentences.\n","                        pad_to_max_length = True,\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","    \n","    # Add the encoded sentence to the list.    \n","    input_ids_train.append(encoded_dict['input_ids'])\n","    \n","    # And its attention mask (simply differentiates padding from non-padding).\n","    attention_masks_train.append(encoded_dict['attention_mask'])\n","\n","# Convert the lists into tensors.\n","input_ids_train = torch.cat(input_ids_train, dim=0)\n","attention_masks_train = torch.cat(attention_masks_train, dim=0)\n","labels_train = torch.tensor(labels_train)\n","\n","# Print sentence 0, now as a list of IDs.\n","print('Original: ', pairs_train[0])\n","print('Token IDs:', input_ids_train[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652644513381,"user_tz":-120,"elapsed":26770,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"779e97ab-105c-454d-e821-9d334041bad7","id":"29oqpmP0Z7ts"},"execution_count":129,"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Original:  The argument is \"a person created through cloning could potentially have developmental problems caused by imperfections in the cloning process.\" The keypoint is \"Cloning is not understood enough yet.\" The argument and the keypoint are\n","Token IDs: tensor([  101,  1996,  6685,  2003,  1000,  1037,  2711,  2580,  2083, 18856,\n","        13369,  2071,  9280,  2031, 13908,  3471,  3303,  2011, 29238,  8496,\n","         1999,  1996, 18856, 13369,  2832,  1012,  1000,  1996,  3145,  8400,\n","         2003,  1000, 18856, 13369,  2003,  2025,  5319,  2438,  2664,  1012,\n","         1000,  1996,  6685,  1998,  1996,  3145,  8400,  2024,   102,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0])\n"]}]},{"cell_type":"code","source":["# Tokenize all of the sentences and map the tokens to thier word IDs.\n","input_ids_dev = []\n","attention_masks_dev = []\n","\n","for pair in pairs_dev:\n","    encoded_dict = tokenizer.encode_plus(\n","                        pair,     # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = 91,   # Pad & truncate all sentences.\n","                        pad_to_max_length = True,\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","        \n","    input_ids_dev.append(encoded_dict['input_ids'])\n","    \n","\n","    attention_masks_dev.append(encoded_dict['attention_mask'])\n","\n","\n","input_ids_dev = torch.cat(input_ids_dev, dim=0)\n","attention_masks_dev = torch.cat(attention_masks_dev, dim=0)\n","labels_dev = torch.tensor(labels_dev)\n","\n","\n","print('Original: ', pairs_dev[0])\n","print('Token IDs:', input_ids_dev[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652644613550,"user_tz":-120,"elapsed":4324,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"6a9e8aa3-aa4f-44b7-9939-36b33cc12c8b","id":"gu5GfWMAZ7ts"},"execution_count":130,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Original:  The argument is \"a man or woman has the right to do what they wish with their body, and if they choose to sell it for sex, the government should not interfere.\" The keypoint is \"Legalizing sex work boosts the economy.\" The argument and the keypoint are\n","Token IDs: tensor([  101,  1996,  6685,  2003,  1000,  1037,  2158,  2030,  2450,  2038,\n","         1996,  2157,  2000,  2079,  2054,  2027,  4299,  2007,  2037,  2303,\n","         1010,  1998,  2065,  2027,  5454,  2000,  5271,  2009,  2005,  3348,\n","         1010,  1996,  2231,  2323,  2025, 15115,  1012,  1000,  1996,  3145,\n","         8400,  2003,  1000,  3423,  6026,  3348,  2147, 12992,  2015,  1996,\n","         4610,  1012,  1000,  1996,  6685,  1998,  1996,  3145,  8400,  2024,\n","          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0])\n"]}]},{"cell_type":"code","source":["from torch.utils.data import TensorDataset\n","train_dataset = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n","dev_dataset = TensorDataset(input_ids_dev, attention_masks_dev, labels_dev)"],"metadata":{"id":"9-mmXtP-Z7ts","executionInfo":{"status":"ok","timestamp":1652644616166,"user_tz":-120,"elapsed":211,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}}},"execution_count":131,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","# The DataLoader needs to know our batch size for training, so we specify it \n","# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n","# size of 16 or 32.\n","batch_size = 32\n","\n","# Create the DataLoaders for our training and validation sets.\n","# We'll take training samples in random order. \n","train_dataloader = DataLoader(\n","            train_dataset,  # The training samples.\n","            sampler = RandomSampler(train_dataset), # Select batches randomly\n","            batch_size = batch_size # Trains with this batch size.\n","        )\n","\n","# For validation the order doesn't matter, so we'll just read them sequentially.\n","dev_dataloader = DataLoader(\n","            dev_dataset, # The validation samples.\n","            sampler = SequentialSampler(dev_dataset), # Pull out batches sequentially.\n","            batch_size = batch_size # Evaluate with this batch size.\n","        )"],"metadata":{"id":"p6sl_2KRZ7tt","executionInfo":{"status":"ok","timestamp":1652644617067,"user_tz":-120,"elapsed":4,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}}},"execution_count":132,"outputs":[]},{"cell_type":"code","source":["from transformers import BertForSequenceClassification, AdamW, BertConfig\n","\n","# Load BertForSequenceClassification, the pretrained BERT model with a single \n","# linear classification layer on top. \n","model = BertForSequenceClassification.from_pretrained(\n","    \"bert-large-uncased\", # Use the -layer BERT model, with an uncased vocab.\n","    num_labels = 2, # The number of output labels--2 for binary classification.\n","                    # You can increase this for multi-class tasks.   \n","    output_attentions = False, # Whether the model returns attentions weights.\n","    output_hidden_states = False, # Whether the model returns all hidden-states.\n",")\n","\n","# Tell pytorch to run this model on the GPU.\n","model.cuda()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652644629763,"user_tz":-120,"elapsed":11132,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"ad95a403-d6c7-4a7e-a3b0-9ffb3b65d747","id":"JWKwvy5DZ7tt"},"execution_count":133,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n","      (position_embeddings): Embedding(512, 1024)\n","      (token_type_embeddings): Embedding(2, 1024)\n","      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (12): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (13): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (14): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (15): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (16): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (17): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (18): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (19): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (20): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (21): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (22): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (23): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=1024, out_features=2, bias=True)\n",")"]},"metadata":{},"execution_count":133}]},{"cell_type":"code","source":["# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n","# I believe the 'W' stands for 'Weight Decay fix\"\n","optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n","                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n","                )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652644633537,"user_tz":-120,"elapsed":209,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}},"outputId":"0032dca3-acd6-4666-c933-a07284bc1423","id":"VK5hv-j2Z7tt"},"execution_count":134,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n"]}]},{"cell_type":"code","source":["from transformers import get_linear_schedule_with_warmup\n","\n","# Number of training epochs. The BERT authors recommend between 2 and 4. \n","\n","epochs = 3\n","\n","# Total number of training steps is [number of batches] x [number of epochs]. \n","# (Note that this is not the same as the number of training samples).\n","total_steps = len(train_dataloader) * epochs\n","\n","# Create the learning rate scheduler.\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0, # Default value in run_glue.py\n","                                            num_training_steps = total_steps)"],"metadata":{"id":"V3VZ_JscZ7tt","executionInfo":{"status":"ok","timestamp":1652644635123,"user_tz":-120,"elapsed":267,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}}},"execution_count":135,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"],"metadata":{"id":"wVja2vpAZ7tt","executionInfo":{"status":"ok","timestamp":1652644637915,"user_tz":-120,"elapsed":282,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}}},"execution_count":136,"outputs":[]},{"cell_type":"code","source":["import time\n","import datetime\n","\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"],"metadata":{"id":"ZSUQPvBWZ7tu","executionInfo":{"status":"ok","timestamp":1652644638902,"user_tz":-120,"elapsed":228,"user":{"displayName":"J. Chen","userId":"13439303419918023814"}}},"execution_count":137,"outputs":[]},{"cell_type":"code","source":["import random\n","import numpy as np\n","\n","# This training code is based on the `run_glue.py` script here:\n","# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n","\n","# Set the seed value all over the place to make this reproducible.\n","seed_val = 42\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# We'll store a number of quantities such as training and validation loss, \n","# validation accuracy, and timings.\n","training_stats = []\n","\n","# Measure the total training time for the whole run.\n","total_t0 = time.time()\n","\n","# For each epoch...\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    # Perform one full pass over the training set.\n","\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    # Measure how long the training epoch takes.\n","    t0 = time.time()\n","\n","    # Reset the total loss for this epoch.\n","    total_train_loss = 0\n","\n","    # Put the model into training mode. Don't be mislead--the call to \n","    # `train` just changes the *mode*, it doesn't *perform* the training.\n","    # `dropout` and `batchnorm` layers behave differently during training\n","    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n","    model.train()\n","\n","    # For each batch of training data...\n","    for step, batch in enumerate(train_dataloader):\n","\n","        # Progress update every 40 batches.\n","        if step % 40 == 0 and not step == 0:\n","            # Calculate elapsed time in minutes.\n","            elapsed = format_time(time.time() - t0)\n","            \n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n","        # `to` method.\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: attention masks\n","        #   [2]: labels \n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","\n","        # Always clear any previously calculated gradients before performing a\n","        # backward pass. PyTorch doesn't do this automatically because \n","        # accumulating the gradients is \"convenient while training RNNs\". \n","        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n","        model.zero_grad()        \n","\n","        # Perform a forward pass (evaluate the model on this training batch).\n","        # In PyTorch, calling `model` will in turn call the model's `forward` \n","        # function and pass down the arguments. The `forward` function is \n","        # documented here: \n","        # https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification\n","        # The results are returned in a results object, documented here:\n","        # https://huggingface.co/transformers/main_classes/output.html#transformers.modeling_outputs.SequenceClassifierOutput\n","        # Specifically, we'll get the loss (because we provided labels) and the\n","        # \"logits\"--the model outputs prior to activation.\n","        result = model(b_input_ids, \n","                       token_type_ids=None, \n","                       attention_mask=b_input_mask, \n","                       labels=b_labels,\n","                       return_dict=True)\n","\n","        loss = result.loss\n","        logits = result.logits\n","\n","        # Accumulate the training loss over all of the batches so that we can\n","        # calculate the average loss at the end. `loss` is a Tensor containing a\n","        # single value; the `.item()` function just returns the Python value \n","        # from the tensor.\n","        total_train_loss += loss.item()\n","\n","        # Perform a backward pass to calculate the gradients.\n","        loss.backward()\n","\n","        # Clip the norm of the gradients to 1.0.\n","        # This is to help prevent the \"exploding gradients\" problem.\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # Update parameters and take a step using the computed gradient.\n","        # The optimizer dictates the \"update rule\"--how the parameters are\n","        # modified based on their gradients, the learning rate, etc.\n","        optimizer.step()\n","\n","        # Update the learning rate.\n","        scheduler.step()\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_train_loss = total_train_loss / len(train_dataloader)            \n","    \n","    # Measure how long this epoch took.\n","    training_time = format_time(time.time() - t0)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epcoh took: {:}\".format(training_time))\n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    # After the completion of each training epoch, measure our performance on\n","    # our validation set.\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    t0 = time.time()\n","\n","    # Put the model in evaluation mode--the dropout layers behave differently\n","    # during evaluation.\n","    model.eval()\n","\n","    # Tracking variables \n","    total_eval_accuracy = 0\n","    total_eval_loss = 0\n","    nb_eval_steps = 0\n","\n","    # Evaluate data for one epoch\n","    for batch in dev_dataloader:\n","        \n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using \n","        # the `to` method.\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: attention masks\n","        #   [2]: labels \n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","        \n","        # Tell pytorch not to bother with constructing the compute graph during\n","        # the forward pass, since this is only needed for backprop (training).\n","        with torch.no_grad():        \n","\n","            # Forward pass, calculate logit predictions.\n","            # token_type_ids is the same as the \"segment ids\", which \n","            # differentiates sentence 1 and 2 in 2-sentence tasks.\n","            result = model(b_input_ids, \n","                           token_type_ids=None, \n","                           attention_mask=b_input_mask,\n","                           labels=b_labels,\n","                           return_dict=True)\n","\n","        # Get the loss and \"logits\" output by the model. The \"logits\" are the \n","        # output values prior to applying an activation function like the \n","        # softmax.\n","        loss = result.loss\n","        logits = result.logits\n","            \n","        # Accumulate the validation loss.\n","        total_eval_loss += loss.item()\n","\n","        # Move logits and labels to CPU\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","\n","        # Calculate the accuracy for this batch of test sentences, and\n","        # accumulate it over all batches.\n","        total_eval_accuracy += flat_accuracy(logits, label_ids)\n","        \n","\n","    # Report the final accuracy for this validation run.\n","    avg_val_accuracy = total_eval_accuracy / len(dev_dataloader)\n","    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_val_loss = total_eval_loss / len(dev_dataloader)\n","    \n","    # Measure how long the validation run took.\n","    validation_time = format_time(time.time() - t0)\n","    \n","    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n","    print(\"  Validation took: {:}\".format(validation_time))\n","\n","    # Record all statistics from this epoch.\n","    training_stats.append(\n","        {\n","            'epoch': epoch_i + 1,\n","            'Training Loss': avg_train_loss,\n","            'Valid. Loss': avg_val_loss,\n","            'Valid. Accur.': avg_val_accuracy,\n","            'Training Time': training_time,\n","            'Validation Time': validation_time\n","        }\n","    )\n","\n","print(\"\")\n","print(\"Training complete!\")\n","\n","print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"],"metadata":{"id":"U_kPaQRqNI36"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Positive samples: %d of %d (%.2f%%)' % (labels_test.sum(), len(labels_test), (labels_test.sum() / len(labels_test) * 100.0)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652565258884,"user_tz":-120,"elapsed":185,"user":{"displayName":"Liza Chan","userId":"08159604947673382978"}},"outputId":"5e8e4f34-4f26-49d3-b306-197bf22b387e","id":"eho1TSN6Z7tw"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Positive samples: 760 of 4171 (18.22%)\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","\n","# Display floats with two decimal places.\n","pd.set_option('precision', 2)\n","\n","# Create a DataFrame from our training statistics.\n","df_stats = pd.DataFrame(data=training_stats)\n","\n","# Use the 'epoch' as the row index.\n","df_stats = df_stats.set_index('epoch')\n","\n","# A hack to force the column headers to wrap.\n","#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n","\n","# Display the table.\n","df_stats"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":175},"id":"buiFp1iAdG8D","executionInfo":{"status":"ok","timestamp":1652565016881,"user_tz":-120,"elapsed":222,"user":{"displayName":"Liza Chan","userId":"08159604947673382978"}},"outputId":"31760845-da97-41b7-d2f1-f96327f0724e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n","epoch                                                                         \n","1               0.33         0.43           0.83       0:04:22         0:00:17\n","2               0.16         0.49           0.82       0:04:26         0:00:17\n","3               0.09         0.57           0.84       0:04:26         0:00:17"],"text/html":["\n","  <div id=\"df-09123f44-fe69-4bfb-9d65-ab27e2f2770c\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Training Loss</th>\n","      <th>Valid. Loss</th>\n","      <th>Valid. Accur.</th>\n","      <th>Training Time</th>\n","      <th>Validation Time</th>\n","    </tr>\n","    <tr>\n","      <th>epoch</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>0.33</td>\n","      <td>0.43</td>\n","      <td>0.83</td>\n","      <td>0:04:22</td>\n","      <td>0:00:17</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.16</td>\n","      <td>0.49</td>\n","      <td>0.82</td>\n","      <td>0:04:26</td>\n","      <td>0:00:17</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.09</td>\n","      <td>0.57</td>\n","      <td>0.84</td>\n","      <td>0:04:26</td>\n","      <td>0:00:17</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-09123f44-fe69-4bfb-9d65-ab27e2f2770c')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-09123f44-fe69-4bfb-9d65-ab27e2f2770c button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-09123f44-fe69-4bfb-9d65-ab27e2f2770c');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":59}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","% matplotlib inline\n","\n","import seaborn as sns\n","\n","# Use plot styling from seaborn.\n","sns.set(style='darkgrid')\n","\n","# Increase the plot size and font size.\n","sns.set(font_scale=1.5)\n","plt.rcParams[\"figure.figsize\"] = (12,6)\n","\n","# Plot the learning curve.\n","plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n","plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n","\n","# Label the plot.\n","plt.title(\"Training & Validation Loss\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.xticks([1, 2, 3, 4])\n","\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":427},"id":"of4YlLjLdJpE","executionInfo":{"status":"ok","timestamp":1652565026868,"user_tz":-120,"elapsed":635,"user":{"displayName":"Liza Chan","userId":"08159604947673382978"}},"outputId":"3e316a31-8c28-481d-c132-15fba9050a5b"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 864x432 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":["# Tokenize all of the sentences and map the tokens to thier word IDs.\n","input_ids = []\n","attention_masks = []\n","\n","# For every sentence...\n","for pair in pairs_test:\n","    # `encode_plus` will:\n","    #   (1) Tokenize the sentence.\n","    #   (2) Prepend the `[CLS]` token to the start.\n","    #   (3) Append the `[SEP]` token to the end.\n","    #   (4) Map tokens to their IDs.\n","    #   (5) Pad or truncate the sentence to `max_length`\n","    #   (6) Create attention masks for [PAD] tokens.\n","    encoded_dict = tokenizer.encode_plus(\n","                        pair,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = 90,           # Pad & truncate all sentences.\n","                        pad_to_max_length = True,\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","    \n","    # Add the encoded sentence to the list.    \n","    input_ids.append(encoded_dict['input_ids'])\n","    \n","    # And its attention mask (simply differentiates padding from non-padding).\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","# Convert the lists into tensors.\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(labels_test)\n","\n","# Set the batch size.  \n","batch_size = 32  \n","\n","# Create the DataLoader.\n","prediction_data = TensorDataset(input_ids, attention_masks, labels)\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"79gjMFQRdOcx","executionInfo":{"status":"ok","timestamp":1652565010608,"user_tz":-120,"elapsed":5057,"user":{"displayName":"Liza Chan","userId":"08159604947673382978"}},"outputId":"360bbd90-b2c4-4664-e534-8912fa36efd5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]}]},{"cell_type":"code","source":["# Prediction on test set\n","\n","print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n","\n","# Put model in evaluation mode\n","model.eval()\n","\n","# Tracking variables \n","predictions , true_labels = [], []\n","\n","# Predict \n","for batch in prediction_dataloader:\n","  # Add batch to GPU\n","  batch = tuple(t.to(device) for t in batch)\n","  \n","  # Unpack the inputs from our dataloader\n","  b_input_ids, b_input_mask, b_labels = batch\n","  \n","  # Telling the model not to compute or store gradients, saving memory and \n","  # speeding up prediction\n","  with torch.no_grad():\n","      # Forward pass, calculate logit predictions.\n","      result = model(b_input_ids, \n","                     token_type_ids=None, \n","                     attention_mask=b_input_mask,\n","                     return_dict=True)\n","\n","  logits = result.logits\n","\n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","  \n","  # Store predictions and true labels\n","  predictions.append(logits)\n","  true_labels.append(label_ids)\n","\n","print('    DONE.')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"weJ-OpLqjUfJ","executionInfo":{"status":"ok","timestamp":1652565182052,"user_tz":-120,"elapsed":22048,"user":{"displayName":"Liza Chan","userId":"08159604947673382978"}},"outputId":"dfc3d08d-4e8d-4898-bc9f-1e7efcde5a1f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicting labels for 4,171 test sentences...\n","    DONE.\n"]}]},{"cell_type":"code","source":["print('Positive samples: %d of %d (%.2f%%)' % (labels_test.sum(), len(labels_test), (labels_test.sum() / len(labels_test) * 100.0)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_eSVJw6qj0QT","executionInfo":{"status":"ok","timestamp":1652565258884,"user_tz":-120,"elapsed":185,"user":{"displayName":"Liza Chan","userId":"08159604947673382978"}},"outputId":"5e8e4f34-4f26-49d3-b306-197bf22b387e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Positive samples: 760 of 4171 (18.22%)\n"]}]},{"cell_type":"code","source":["from sklearn.metrics import matthews_corrcoef\n","\n","matthews_set = []\n","\n","# Evaluate each test batch using Matthew's correlation coefficient\n","print('Calculating Matthews Corr. Coef. for each batch...')\n","\n","# For each input batch...\n","for i in range(len(true_labels)):\n","  \n","  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n","  # and one column for \"1\"). Pick the label with the highest value and turn this\n","  # in to a list of 0s and 1s.\n","  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n","  \n","  # Calculate and store the coef for this batch.  \n","  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n","  matthews_set.append(matthews)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3FefG_7xkLvl","executionInfo":{"status":"ok","timestamp":1652565307252,"user_tz":-120,"elapsed":485,"user":{"displayName":"Liza Chan","userId":"08159604947673382978"}},"outputId":"2c2ddd97-cdcf-41e4-bb6c-b76414599e5c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Calculating Matthews Corr. Coef. for each batch...\n"]}]},{"cell_type":"code","source":["# Combine the results across all batches. \n","flat_predictions = np.concatenate(predictions, axis=0)\n","\n","# For each sample, pick the label (0 or 1) with the higher score.\n","flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n","\n","# Combine the correct labels for each batch into a single list.\n","flat_true_labels = np.concatenate(true_labels, axis=0)\n","\n","# Calculate the MCC\n","mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n","\n","print('Total MCC: %.3f' % mcc)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OzK-F7AtkXdS","executionInfo":{"status":"ok","timestamp":1652565386236,"user_tz":-120,"elapsed":204,"user":{"displayName":"Liza Chan","userId":"08159604947673382978"}},"outputId":"05ace687-7114-4f01-ff45-55886c94c9b2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total MCC: 0.503\n"]}]}]}