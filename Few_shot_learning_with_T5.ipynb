{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Few_shot_learning_with_T5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samin9796/arg2keypoint/blob/main/Few_shot_learning_with_T5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpJqElfMrd3s"
      },
      "source": [
        " **Few shot text generation with T5 Transformer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7DFNSwyfFuu"
      },
      "source": [
        "Author: Ramsri Goutham Golla\n",
        "\n",
        "Linkedin : https://www.linkedin.com/in/ramsrig/\n",
        "\n",
        "Twitter: https://twitter.com/ramsri_goutham"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4D0OZzQaB3I"
      },
      "source": [
        "## 1. Install libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhRIV6wXSFbH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d14a9859-3340-4c84-df48-e9afa3b38703"
      },
      "source": [
        "!pip install transformers==2.9.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==2.9.0\n",
            "  Downloading transformers-2.9.0-py3-none-any.whl (635 kB)\n",
            "\u001b[K     |████████████████████████████████| 635 kB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==2.9.0) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==2.9.0) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==2.9.0) (2019.12.20)\n",
            "Collecting tokenizers==0.7.0\n",
            "  Downloading tokenizers-0.7.0-cp37-cp37m-manylinux1_x86_64.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 23.0 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 16.6 MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 33.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==2.9.0) (4.64.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==2.9.0) (3.6.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.9.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.9.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.9.0) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.9.0) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.9.0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.9.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.9.0) (1.1.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=7482abd8eb22c6d6eae6f035dce5f336dcd8818854279879d5cbab0227b27464\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.53 sentencepiece-0.1.96 tokenizers-0.7.0 transformers-2.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojc3j-S6VS_t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a40ef52a-1788-41a0-c5f7-b941e01e163a"
      },
      "source": [
        "# Check we have a GPU and check the memory size of the GUP\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon May  9 15:01:35 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   66C    P8    31W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Hpcn7EOWHII"
      },
      "source": [
        "## 2. Prepare Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6neDi6_VxU7"
      },
      "source": [
        "\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    T5ForConditionalGeneration,\n",
        "    T5Tokenizer,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "\n",
        "def set_seed(seed):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "\n",
        "set_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTzBRXGNvaF9"
      },
      "source": [
        "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
        "t5_model1 = T5ForConditionalGeneration.from_pretrained('t5-base')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KlFP1jpzizh"
      },
      "source": [
        "# optimizer\n",
        "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "optimizer_grouped_parameters = [\n",
        "    {\n",
        "        \"params\": [p for n, p in t5_model1.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "        \"weight_decay\": 0.0,\n",
        "    },\n",
        "    {\n",
        "        \"params\": [p for n, p in t5_model1.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "        \"weight_decay\": 0.0,\n",
        "    },\n",
        "]\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=3e-4, eps=1e-8)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82WbxhWMFKWe"
      },
      "source": [
        "# dataset preparation\n",
        "\n",
        "arg_keypoint_tuples = [\n",
        "                               (\"a person should have the right to be able to choose if they want to live or die.\",\"Assisted suicide reduces suffering\", \"unmatched\"),\n",
        "                               (\"assisted suicide allows one to end terminal pain and suffering and should be allowed to continue.\",\"Assisted suicide gives dignity to the person that wants to commit it\", \"unmatched\"),\n",
        "                               (\"assisted suicide allows people who have terrible health conditions to die with dignity. if assisted suicide is a crime they may have to endure many more years living in agony.\",\"People should have the freedom to choose to end their life\", \"unmatched\"),\n",
        "                               (\"assisted suicide allows terminally ill people to die with dignity and should not be criminalized.\",\"People should have the freedom to choose to end their life\", \"unmatched\"),\n",
        "                               (\"assisted suicide allows terminally ill people to die with dignity and should not be criminalized.\",\"The terminally ill would benefit from assisted suicide\", \"unmatched\"),\n",
        "                               (\"assisted suicide helps terminally ill people end their suffering.\",\"Assisted suicide gives dignity to the person that wants to commit it\", \"unmatched\"),\n",
        "                               (\"Assisted suicide helps those who are in pain due to a devastating disease end their own lives on their own terms.\",\"Assisted suicide gives dignity to the person that wants to commit it\", \"unmatched\"),\n",
        "                               (\"assisted suicide is a human right and would spare the terminally i'll pain.\",\"Assisted suicide gives dignity to the person that wants to commit it\", \"unmatched\"),\n",
        "                               (\"helping someone commit suicide is like killing them yourself.\",\"Assisted suicide is akin to killing someone\",\"matched\"),\n",
        "                               (\"helping someone kill themself is murder\", \"Assisted suicide is akin to killing someone\", \"matched\"),\n",
        "                               (\"the vow of celibacy is unnatural and causes harm.\",\"Celibacy is unhealthy/unnatural\",\"matched\"),\n",
        "                               (\"marriage is a sacred bond that is highly regarded in various religions.\",\"Marriage is important for people, either generally or because of religious/traditional reasons\",\"matched\"),\n",
        "                               (\"abolishing intellectual property rights would damage the economy since there would be less incentive to bring new and innovative products to market\",\"Intellectual property rights incentivize investment in developing new products\",\"matched\")\n",
        "\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAkjy-dsENoc"
      },
      "source": [
        "## 3. Train Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQHqVp9UGzro",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a062788-eb63-4416-995a-20810f43f2b1"
      },
      "source": [
        "t5_model1.train()\n",
        "\n",
        "epochs = 5\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  print (\"epoch \",epoch)\n",
        "  for input1, input2, output in arg_keypoint_tuples:\n",
        "    input_sent = \"The argument: \"+input1+ \" and the keypoint: \"+input2+ \"are </s>\"\n",
        "    ouput_sent = output+\" </s>\"\n",
        "\n",
        "    tokenized_inp = tokenizer.encode_plus(input_sent,  max_length=96, pad_to_max_length=True,return_tensors=\"pt\")\n",
        "    tokenized_output = tokenizer.encode_plus(ouput_sent, max_length=96, pad_to_max_length=True,return_tensors=\"pt\")\n",
        "\n",
        "\n",
        "    input_ids  = tokenized_inp[\"input_ids\"]\n",
        "    attention_mask = tokenized_inp[\"attention_mask\"]\n",
        "\n",
        "    lm_labels= tokenized_output[\"input_ids\"]\n",
        "    decoder_attention_mask=  tokenized_output[\"attention_mask\"]\n",
        "\n",
        "\n",
        "    # the forward function automatically creates the correct decoder_input_ids\n",
        "    output = t5_model1(input_ids=input_ids, lm_labels=lm_labels,decoder_attention_mask=decoder_attention_mask,attention_mask=attention_mask)\n",
        "    loss = output[0]\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch  0\n",
            "epoch  1\n",
            "epoch  2\n",
            "epoch  3\n",
            "epoch  4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHnSgB7e5pdp"
      },
      "source": [
        "## 4. Test model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpUH01FvMuus",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb1bba31-c977-4b1e-857b-d1ec4ab59a4a"
      },
      "source": [
        "test_sent = 'The argument: It can be counterproductive to subject a child to the side effects of vaccines. and the keypoint: The parents and not the state should decide are </s>'\n",
        "test_tokenized = tokenizer.encode_plus(test_sent, return_tensors=\"pt\")\n",
        "\n",
        "test_input_ids  = test_tokenized[\"input_ids\"]\n",
        "test_attention_mask = test_tokenized[\"attention_mask\"]\n",
        "\n",
        "t5_model1.eval()\n",
        "beam_outputs = t5_model1.generate(\n",
        "    input_ids=test_input_ids,attention_mask=test_attention_mask,\n",
        "    max_length=64,\n",
        "    early_stopping=True,\n",
        "    num_beams=10,\n",
        "    num_return_sequences=3,\n",
        "    no_repeat_ngram_size=2\n",
        ")\n",
        "\n",
        "for beam_output in beam_outputs:\n",
        "    sent = tokenizer.decode(beam_output, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
        "    print (sent)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py:1432: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  beam_id = beam_token_id // vocab_size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unmatched\n",
            "matched\n",
            "a child's innate ability to learn is matched\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ea-svE2qjgSH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7da5bb96-d9f5-4332-f647-dc9fdaae6785"
      },
      "source": [
        "test_sent = 'The argument: Death with dignity is having the option to decide your own fate. You are able to take control of your own fate. It allows you to leave the world with your true self intact. means: </s>'\n",
        "test_tokenized = tokenizer.encode_plus(test_sent, return_tensors=\"pt\")\n",
        "\n",
        "test_input_ids  = test_tokenized[\"input_ids\"]\n",
        "test_attention_mask = test_tokenized[\"attention_mask\"]\n",
        "\n",
        "t5_model1.eval()\n",
        "beam_outputs = t5_model1.generate(\n",
        "    input_ids=test_input_ids,attention_mask=test_attention_mask,\n",
        "    max_length=64,\n",
        "    early_stopping=True,\n",
        "    num_beams=10,\n",
        "    num_return_sequences=3,\n",
        "    no_repeat_ngram_size=2\n",
        ")\n",
        "\n",
        "for beam_output in beam_outputs:\n",
        "    sent = tokenizer.decode(beam_output, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
        "    print (sent)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py:1432: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  beam_id = beam_token_id // vocab_size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "True\n",
            "Argument: Death with dignity is having the option to decide your own fate\n"
          ]
        }
      ]
    }
  ]
}