{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"bert(2 ver).ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPSoYo9Lc3SWVHyvbdUByVI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# base(base)(in-texts)(BART-cross-t3)"],"metadata":{"id":"QDKx4xc9eWcT"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653839192688,"user_tz":-120,"elapsed":3189,"user":{"displayName":"Chen Jingyan","userId":"08436694167263442771"}},"outputId":"58989729-9472-4335-ded3-43e113e4636c","id":"yuVIQtSdeWcT"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pytorch_pretrained_bert in /usr/local/lib/python3.7/dist-packages (0.6.2)\n","Requirement already satisfied: pytorch-nlp in /usr/local/lib/python3.7/dist-packages (0.5.0)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.23.10)\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n","Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.11.0+cu113)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.21.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (4.64.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (4.2.0)\n","Requirement already satisfied: botocore<1.27.0,>=1.26.10 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_pretrained_bert) (1.26.10)\n","Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_pretrained_bert) (0.5.2)\n","Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_pretrained_bert) (1.0.0)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.27.0,>=1.26.10->boto3->pytorch_pretrained_bert) (2.8.2)\n","Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.27.0,>=1.26.10->boto3->pytorch_pretrained_bert) (1.25.11)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.27.0,>=1.26.10->boto3->pytorch_pretrained_bert) (1.15.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2022.5.18.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n"]}],"source":["!pip install pytorch_pretrained_bert pytorch-nlp"]},{"cell_type":"code","source":["import sys\n","import numpy as np\n","import random as rn\n","import torch\n","from pytorch_pretrained_bert import BertModel\n","from torch import nn\n","from pytorch_pretrained_bert import BertTokenizer\n","\n","from keras.preprocessing.sequence import pad_sequences\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from torch.optim import Adam\n","from torch.nn.utils import clip_grad_norm_\n","from IPython.display import clear_output"],"metadata":{"id":"YiQVDc6DeWcT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["seed_val = 42\n","rn.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed(seed_val)"],"metadata":{"id":"dw2SxGh2eWcT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"],"metadata":{"id":"iGO5dTqqeWcU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653839199498,"user_tz":-120,"elapsed":214,"user":{"displayName":"Chen Jingyan","userId":"08436694167263442771"}},"outputId":"b1fe8e5e-7a28-4627-9013-56a42d09a9f9","id":"XU5TexH7eWcU"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<pytorch_pretrained_bert.tokenization.BertTokenizer at 0x7f40681baad0>"]},"metadata":{},"execution_count":65}]},{"cell_type":"code","source":["import pandas as pd\n","train = pd.read_csv(\"/content/drive/MyDrive/Keypoints/inter-text_from_bart_large_temp_3_full_train_cross.csv\")\n","# dev = pd.read_csv(\"/content/drive/MyDrive/Keypoints/inter-text_from_T5-small_temp_1_full_dev.csv\")\n","test = pd.read_csv(\"/content/drive/MyDrive/Keypoints/inter-text_from_bart_large_temp_3_full_test_cross.csv\")\n","for split in [train,test]:\n","  for i in split.index:\n","    arg = split['argument'][i]\n","    key = split['key_point'][i]\n","    # topic = split['topic'][i]\n","    in_text = split['intermediary_text'][i]\n","    if arg[-1] != '.':\n","      pair =  arg + '. ' + in_text + '. '+ key + '.'\n","      split.at[i, 'pair'] = pair\n","    else:\n","      pair =  arg + ' ' + in_text + '. '+ key + '.'\n","      split.at[i, 'pair'] = pair"],"metadata":{"id":"8LvszlEzeWcU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train['pair'][0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1653839179626,"user_tz":-120,"elapsed":215,"user":{"displayName":"Chen Jingyan","userId":"08436694167263442771"}},"outputId":"8e321edd-fc9d-4e1a-d466-c42eb3571777","id":"s8t-fFX1eWcU"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'a person created through cloning could potentially have developmental problems caused by imperfections in the cloning process. Cloning is unsafe. Cloning is not understood enough yet.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":60}]},{"cell_type":"code","source":["pairs_train = train.pair.values\n","labels_train = train.label.values\n","\n","# pairs_dev = dev.pair.values\n","# labels_dev = dev.label.values\n","\n","pairs_test = test.pair.values\n","labels_test = test.label.values"],"metadata":{"id":"3RUCPN82eWcU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"id":"4AUB1XjWfo6x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import BertTokenizer\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n","\n","max_len = 0\n","\n","for split in [pairs_train,pairs_test]:\n","  for pair in split:\n","\n","      # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n","      input_ids = tokenizer.encode(pair, add_special_tokens=True)\n","\n","      # Update the maximum sentence length.\n","      max_len = max(max_len, len(input_ids))\n","\n","print('Max sentence length: ', max_len)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653839159060,"user_tz":-120,"elapsed":36215,"user":{"displayName":"Chen Jingyan","userId":"08436694167263442771"}},"outputId":"368b2d6e-ffa7-409a-c17d-a27d137778c4","id":"CjCbO1hXeWcU"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Max sentence length:  78\n"]}]},{"cell_type":"markdown","source":["initiate the tokenizer from pytorch again!"],"metadata":{"id":"jBpwXFfMhPmg"}},{"cell_type":"code","source":["train_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:76] + ['[SEP]'], pairs_train))\n","# dev_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:76] + ['[SEP]'], pairs_dev))\n","test_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:76] + ['[SEP]'], pairs_test))\n","\n","train_tokens_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, train_tokens)), maxlen=78, truncating=\"post\", padding=\"post\", dtype=\"int\")\n","# dev_tokens_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, dev_tokens)), maxlen=78, truncating=\"post\", padding=\"post\", dtype=\"int\")\n","test_tokens_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, test_tokens)), maxlen=78, truncating=\"post\", padding=\"post\", dtype=\"int\")\n","\n"],"metadata":{"id":"yeiXAaZOeWcV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_masks = [[float(i > 0) for i in ii] for ii in train_tokens_ids]\n","# dev_masks = [[float(i > 0) for i in ii] for ii in dev_tokens_ids]\n","test_masks = [[float(i > 0) for i in ii] for ii in test_tokens_ids]"],"metadata":{"id":"MQPfJN4XeWcV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### BERT model"],"metadata":{"id":"LyHjO9KheWcV"}},{"cell_type":"code","source":["class BertBinaryClassifier(nn.Module):\n","    def __init__(self, dropout=0.1):\n","        super(BertBinaryClassifier, self).__init__()\n","\n","        self.bert = BertModel.from_pretrained('bert-base-uncased')\n","\n","        self.dropout = nn.Dropout(dropout)\n","        self.linear = nn.Linear(768, 1)\n","        self.sigmoid = nn.Sigmoid()\n","    \n","    def forward(self, tokens, masks=None):\n","        _, pooled_output = self.bert(tokens, attention_mask=masks, output_all_encoded_layers=False)\n","        dropout_output = self.dropout(pooled_output)\n","        linear_output = self.linear(dropout_output)\n","        proba = self.sigmoid(linear_output)\n","        return proba"],"metadata":{"id":"qYIg60AFeWcV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653839238567,"user_tz":-120,"elapsed":7,"user":{"displayName":"Chen Jingyan","userId":"08436694167263442771"}},"outputId":"7a2db37c-1f09-4ac2-9710-180c2e37c7b3","id":"Vu1o3mdJeWcV"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":69}]},{"cell_type":"code","source":["bert_clf = BertBinaryClassifier()\n","bert_clf = bert_clf.cuda()"],"metadata":{"id":"frgzrDr_eWcV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Finetune"],"metadata":{"id":"81yxxN2deWcV"}},{"cell_type":"code","source":["BATCH_SIZE = 32\n","EPOCHS = 3"],"metadata":{"id":"3qKLgbcneWcV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_tokens_tensor = torch.tensor(train_tokens_ids)\n","train_labels_tensor = torch.tensor(labels_train.reshape(-1, 1)).float()\n","\n","# dev_tokens_tensor = torch.tensor(dev_tokens_ids)\n","# dev_labels_tensor = torch.tensor(labels_dev.reshape(-1, 1)).float()\n","\n","test_tokens_tensor = torch.tensor(test_tokens_ids)\n","test_labels_tensor = torch.tensor(labels_test.reshape(-1, 1)).float()\n","\n","train_masks_tensor = torch.tensor(train_masks)\n","# dev_masks_tensor = torch.tensor(dev_masks)\n","test_masks_tensor = torch.tensor(test_masks)"],"metadata":{"id":"byw5F53deWcV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset = TensorDataset(train_tokens_tensor, train_masks_tensor, train_labels_tensor)\n","train_sampler = RandomSampler(train_dataset)\n","train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)\n","\n","# dev_dataset = TensorDataset(dev_tokens_tensor, dev_masks_tensor, dev_labels_tensor)\n","# dev_sampler = SequentialSampler(dev_dataset)\n","# dev_dataloader = DataLoader(dev_dataset, sampler=dev_sampler, batch_size=BATCH_SIZE)\n","\n","test_dataset = TensorDataset(test_tokens_tensor, test_masks_tensor, test_labels_tensor)\n","test_sampler = SequentialSampler(test_dataset)\n","test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)"],"metadata":{"id":"UoWZMaIGeWcW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["param_optimizer = list(bert_clf.sigmoid.named_parameters()) \n","optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n","\n","optimizer = Adam(bert_clf.parameters(), lr=2e-5)"],"metadata":{"id":"KyJ-AHLUeWcW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import time\n","import datetime\n","\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"],"metadata":{"id":"EBmGJ7NQeWcW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.cuda.empty_cache()"],"metadata":{"id":"3oUOqk-peWcW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["t0 = time.time()\n","for epoch_num in range(EPOCHS):\n","\n","  bert_clf.train()\n","  train_loss = 0\n","  for step_num, batch_data in enumerate(train_dataloader):\n","    token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n","    print(str(torch.cuda.memory_allocated(device)/1000000 ) + 'M')\n","    \n","    logits = bert_clf(token_ids, masks)\n","    loss_func = nn.BCELoss()\n","\n","    batch_loss = loss_func(logits, labels)\n","    train_loss += batch_loss.item()\n","    \n","    bert_clf.zero_grad()\n","    batch_loss.backward()\n","        \n","\n","    clip_grad_norm_(parameters=bert_clf.parameters(), max_norm=1.0)\n","    optimizer.step()\n","        \n","    clear_output(wait=True)\n","    print('Epoch: ', epoch_num + 1)\n","    print(\"\\r\" + \"{0}/{1} train loss: {2} \".format(step_num, len(train) / BATCH_SIZE, train_loss / (step_num + 1)))\n","\n","print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-t0)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"0c819c2d-8e34-4530-f778-1d9cdba78edc","id":"kUwaSQafeWcW"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch:  2\n","\r34/531.84375 train loss: 0.2590363011828491 \n","1777.851904M\n"]}]},{"cell_type":"markdown","source":["## Evaluation"],"metadata":{"id":"9qzyicoXeWcW"}},{"cell_type":"code","source":["bert_clf.eval()\n","\n","all_logits = []\n","\n","with torch.no_grad():\n","    for step_num, batch_data in enumerate(test_dataloader):\n","\n","        token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n","\n","        logits = bert_clf(token_ids, masks)\n","        loss_func = nn.BCELoss()\n","        loss = loss_func(logits, labels)\n","\n","        # Move logits and labels to CPU\n","        numpy_logits = logits.cpu().detach().numpy()\n","\n","        # label_ids = labels.to('cpu').numpy()\n","        # true_labels.append(label_ids) \n","\n","         \n","        all_logits += list(numpy_logits[:, 0]) \n","        # bert_predicted += list(numpy_logits[:, 0] > threshold) \n","\n","        "],"metadata":{"id":"0wr98EIEeWcW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### threshold"],"metadata":{"id":"EizMH4t9eWcX"}},{"cell_type":"code","source":["# f1 of test set\n","test[\"prob\"] = all_logits\n","threshold = 0.5\n","for i in test.index:\n","  if test[\"prob\"][i] > threshold:\n","    test.at[i,\"prediction\"] = 1\n","  else:\n","    test.at[i,\"prediction\"] = 0\n","\n","from sklearn.metrics import classification_report,accuracy_score,f1_score\n","true = test[\"label\"]\n","prediction = test[\"prediction\"]\n","print(\"base-bs-BART-cross-t3\")\n","print(\"f1:\",f1_score(true, prediction))\n","print(\"acc:\",accuracy_score(true, prediction))\n","print(classification_report(true, prediction, digits=3))"],"metadata":{"id":"NuZV6rHgeWcX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Save and load"],"metadata":{"id":"TI7gV6uDeWcX"}},{"cell_type":"code","source":["torch.save({\n","            'epoch': epoch_num,\n","            'model_state_dict': bert_clf.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'loss': batch_loss,\n","            }, \"/content/drive/MyDrive/Keypoints/bert1-base-bs-cross-BART-t3.pth\")"],"metadata":{"id":"4nSVkig9eWcX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bert_clf = BertBinaryClassifier() \n","bert_clf.load_state_dict(torch.load(\"/content/drive/MyDrive/Keypoints/bert1-base-bs-cross-BART-t3.pth\")['model_state_dict'],strict=False) \n","bert_clf.to(device) \n","bert_clf.eval()"],"metadata":{"id":"gWsvw7qyeWcY"},"execution_count":null,"outputs":[]}]}